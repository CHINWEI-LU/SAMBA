{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb5b855",
   "metadata": {},
   "source": [
    "#### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c74ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import argparse\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b7a05",
   "metadata": {},
   "source": [
    "#### class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce79fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    seq_in: int\n",
    "    seq_out: int\n",
    "    d_state: int =128\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_conv: int = 3\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "\n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "       \n",
    "\n",
    "class SAMBA(nn.Module):\n",
    "    def __init__(self, ModelArgs,hidden,inp,out,embed,cheb_k):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "\n",
    "        self.mam1 = Mamba(ModelArgs,hidden)\n",
    "\n",
    "        \n",
    "\n",
    "        self.cheb_k=cheb_k\n",
    "\n",
    "        self.gamma=nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "       \n",
    "\n",
    "  \n",
    "\n",
    "        self.adj=nn.Parameter(torch.randn(ModelArgs.vocab_size,embed), requires_grad=True)\n",
    "\n",
    "        self.embed_w=nn.Parameter(torch.randn(embed,embed), requires_grad=True)\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, out))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed, out))\n",
    "\n",
    "        self.proj=nn.Linear(ModelArgs.vocab_size,1)\n",
    "        self.proj_seq=nn.Linear(ModelArgs.seq_in,1)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def gaussian_kernel_graph(self,E_A,x ,gamma=1.0):\n",
    "    \n",
    "    # Compute pairwise squared Euclidean distance\n",
    "\n",
    "        x_mean=torch.mean(x,dim=0)\n",
    "\n",
    "        x_time=torch.mm(x_mean.permute(1,0),x_mean)\n",
    "\n",
    "        \n",
    "        N = E_A.size(0)\n",
    "    # Expanding the dimensions to compute pairwise differences\n",
    "        E_A_expanded = E_A.unsqueeze(0).expand(N, N, -1)\n",
    "        E_A_T_expanded = E_A.unsqueeze(1).expand(N, N, -1)\n",
    "    # Pairwise squared Euclidean distances\n",
    "        distance_matrix = torch.sum((E_A_expanded - E_A_T_expanded)**2, dim=2)\n",
    "    \n",
    "    # Apply Gaussian kernel\n",
    "        A = torch.exp(-gamma * distance_matrix)\n",
    "\n",
    "        dr=nn.Dropout(0.35)\n",
    "\n",
    "        #A=torch.tanh(torch.mm(self.adj, self.adj.transpose(0, 1))+1)\n",
    "    \n",
    "    # Optional: Normalize the adjacency matrix with softmax (row-wise)\n",
    "        A = F.softmax(A, dim=1)\n",
    "    \n",
    "        return dr(A)\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        #x_mean=torch.mean(input_ids,dim=0)\n",
    "\n",
    "        #ADJ=F.softmax(torch.mm(x_mean.permute(1,0),x_mean),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        xx=self.mam1(input_ids)\n",
    "\n",
    "\n",
    "        \n",
    "        #m = nn.LeakyReLU(0.1)\n",
    "        #ADJ=F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "        #dr=nn.Dropout(0.35)\n",
    "        #ADJ=dr(F.softmax(F.relu(torch.mm(torch.mm(self.adj, self.embed_w),self.adj.transpose(0, 1))),dim=1))\n",
    "        \n",
    "        ADJ=self.gaussian_kernel_graph(self.adj,xx,gamma=self.gamma)\n",
    "\n",
    "        #degree = torch.sum(ADJ, dim=1)\n",
    "        # laplacian is sym or not\n",
    "        #attention = 0.5 * (attention + attention.T)\n",
    "        #degree_l = torch.diag(degree)+1e-5\n",
    "        \n",
    "        #deg=torch.diag(1 / (degree + 1e-5))\n",
    "        \n",
    "        #diagonal_degree_hat = torch.diag(1 / (torch.sqrt(degree) + 1e-5))\n",
    "        #attention = torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))#milan\n",
    "        #A=torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))\n",
    "        #r=torch.rand(1).cuda()\n",
    "        \n",
    "        #d1=torch.diag(1 / (torch.pow(degree,1-r) + 1e-5))\n",
    "        #d2=torch.diag(1 / (torch.pow(degree,r) + 1e-5))\n",
    "        \n",
    "        \n",
    "        #L = torch.eye(input_ids.size(2)).cuda()-torch.matmul(d1,torch.matmul(ADJ,d2))\n",
    "\n",
    "        I=torch.eye(input_ids.size(2)).cuda()\n",
    "\n",
    "        #L=I-ADJ\n",
    "\n",
    "        #out=self.mlp(xx)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        support_set = [I,ADJ]#(math.sqrt(2)/2)*L,(-math.sqrt(2)/2)*L]\n",
    "    \n",
    "        \n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "        weights = torch.einsum('nd,dkio->nkio', self.adj, self.weights_pool)  #N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)                       #N, dim_out\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, xx.permute(0,2,1))      #B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out = torch.einsum('bnki,nkio->bno', x_g, weights) + bias#B,N,D_OUT\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return self.proj(out.permute(0,2,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, args: ModelArgs,hid):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "        self.nl=args.n_layer\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        self.layers2 = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        #self.layers3 = nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "\n",
    "        #self.layers3=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "        \n",
    "        #self.layers4=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),gconv(args.seq_in,hid,2,10,args.d_model),nn.ReLU(),gconv(hid,args.seq_in,2,10,args.d_model)) for _ in range(args.n_layer)])\n",
    "      \n",
    "       \n",
    "\n",
    "        self.lin=nn.ModuleList([nn.Sequential(nn.LayerNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in)) for _ in range(args.n_layer-2)]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))])\n",
    "        \n",
    "        #self.lin2=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in)) for _ in range(args.n_layer-2)]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))])\n",
    "        \n",
    "        \n",
    "        self.norm_f = nn.LayerNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size)\n",
    "\n",
    "\n",
    "        self.proj=nn.Sequential(nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))\n",
    "\n",
    "        self.nnl=nn.LayerNorm(args.vocab_size)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "      \n",
    "        #self.proj=nn.Linear(2*ModelArgs.vocab_size, ModelArgs.vocab_size)\n",
    "        #self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
    "                                                     # See \"Weight Tying\" paper\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        x1=x\n",
    "        x2=x\n",
    "    \n",
    "\n",
    "        for i in range(self.nl):\n",
    "            \n",
    "            x1 = self.layers[i](x1)\n",
    "            x2=self.layers2[i](x2.flip([1]))\n",
    "            \n",
    "            x=x1+x2.flip([1])+x\n",
    "\n",
    "            x=self.lin[i](x.permute(0,2,1)).permute(0,2,1)+x\n",
    "\n",
    "            x1=x\n",
    "            x2=x\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        x = self.norm_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        \n",
    "\n",
    "#        a=logits.shape\n",
    "\n",
    " #       #sq=torch.reshape(logits,(a[0],a[2],a[1]))\n",
    "\n",
    "  #      out=self.out(sq)\n",
    "\n",
    "   #     b=out.shape\n",
    "\n",
    "    #    out=torch.reshape(out,(b[0],b[2],b[1]))\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name: str):\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name: One of\n",
    "                * 'state-spaces/mamba-2.8b-slimpj'\n",
    "                * 'state-spaces/mamba-2.8b'\n",
    "                * 'state-spaces/mamba-1.4b'\n",
    "                * 'state-spaces/mamba-790m'\n",
    "                * 'state-spaces/mamba-370m'\n",
    "                * 'state-spaces/mamba-130m'\n",
    "\n",
    "        Returns:\n",
    "            model: Mamba model with weights loaded\n",
    "\n",
    "        \"\"\"\n",
    "        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "        from transformers.utils.hub import cached_file\n",
    "\n",
    "        def load_config_hf(model_name):\n",
    "            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return json.load(open(resolved_archive_file))\n",
    "\n",
    "\n",
    "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
    "            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n",
    "\n",
    "        config_data = load_config_hf(pretrained_model_name)\n",
    "        args = ModelArgs(\n",
    "            d_model=config_data['d_model'],\n",
    "            n_layer=config_data['n_layer'],\n",
    "            vocab_size=config_data['vocab_size']\n",
    "        )\n",
    "        model = Mamba(args)\n",
    "\n",
    "        state_dict = load_state_dict_hf(pretrained_model_name)\n",
    "        new_state_dict = {}\n",
    "        for key in state_dict:\n",
    "            new_key = key.replace('backbone.', '')\n",
    "            new_state_dict[new_key] = state_dict[key]\n",
    "        model.load_state_dict(new_state_dict)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.norm = nn.LayerNorm(args.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "\n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.mixer(self.norm(x)) \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class gconv(nn.Module):\n",
    "    def __init__(self, inp, hid,embed,cheb_k,n):\n",
    "        super(gconv, self).__init__()\n",
    "\n",
    "        self.node_num=n\n",
    "\n",
    "        self.inp=inp\n",
    "\n",
    "        self.cheb_k=cheb_k\n",
    "\n",
    "        self.adj=nn.Parameter(torch.randn(n,embed), requires_grad=True)\n",
    "       \n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, hid))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed,hid))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        #output shape [B, N, C]\n",
    "        \n",
    "        ADJ=F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        support_set = [torch.eye(self.node_num).cuda(),ADJ]\n",
    "    \n",
    "        \n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "        weights = torch.einsum('nd,dkio->nkio', self.adj, self.weights_pool)  #N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)                       #N, dim_out\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, x)      #B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out_6 = torch.einsum('bnki,nkio->bno', x_g, weights) + bias   #B,N,D_OUT\n",
    "\n",
    "        return out_6\n",
    "\n",
    "class AVWGCN(nn.Module):\n",
    "    def __init__(self, dim_in, hid, cheb_k,n):\n",
    "        super(AVWGCN, self).__init__()\n",
    "\n",
    "        self.node_num=n\n",
    "\n",
    "        self.inp=dim_in\n",
    "\n",
    "        self.cheb_k = cheb_k\n",
    "        self.node_embeddings = nn.Parameter(torch.randn(n,dim_in,dim_in), requires_grad=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(cheb_k,n,dim_in, hid))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(n, hid))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        #output shape [B, N, C]\n",
    "        \n",
    "        supports = F.softmax(F.relu(self.node_embeddings), dim=2)\n",
    "\n",
    "        I=torch.eye(self.inp).cuda()\n",
    "\n",
    "        I2=I[None,:,:].repeat(x.size(1),1,1)\n",
    "        \n",
    "\n",
    "        support_set = [I2, supports]\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "                              #N, dim_out\n",
    "        x_g = torch.einsum(\"bnc,kncm->bknm\", x, supports)      #B, cheb_k, N, dim_in\n",
    "        #x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        x_gconv = torch.einsum('bknm,knmo->bno', x_g, self.weights_pool) + self.bias_pool     #b, N, dim_out\n",
    "        return x_gconv\n",
    "\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.in_proj_r = nn.Linear(args.d_model, args.d_inner, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
    "\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size,bias=False)\n",
    "\n",
    "        #self.x_proj_r = nn.Linear(args.d_inner, args.dt_rank + args.d_state, bias=True)\n",
    "\n",
    "        #self.x_proj = FourierKANLayer(args.d_inner, args.dt_rank + args.d_state * 2, 100)\n",
    "\n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "        #self.dt_proj=FourierKANLayer(args.dt_rank, args.d_inner, 100)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d) = x.shape\n",
    "\n",
    "        #x=self.embedding(x)\n",
    "\n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
    "\n",
    "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
    "    \n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
    "\n",
    "\n",
    "\n",
    "        x = F.silu(x)\n",
    "\n",
    "        gate=x*(1-F.sigmoid(res))\n",
    "\n",
    "        \n",
    "\n",
    "        y = self.ssm(x)\n",
    "        y = y * F.silu(res)\n",
    "\n",
    "        output = self.out_proj(y)\n",
    "\n",
    "        #o1=self.norm_f(output)\n",
    "\n",
    "        #o2=self.lm_head(o1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute ∆ A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
    "        D = self.D.float()\n",
    "\n",
    "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
    "\n",
    "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
    "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
    "\n",
    "        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def selective_scan(self, u, delta, A, B, C, D):\n",
    "        \"\"\"Does selective scan algorithm. See:\n",
    "            - Section 2 State Space Models in the Mamba paper [1]\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        This is the classic discrete state space formula:\n",
    "            x(t + 1) = Ax(t) + Bu(t)\n",
    "            y(t)     = Cx(t) + Du(t)\n",
    "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
    "\n",
    "        Args:\n",
    "            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "            delta: shape (b, l, d_in)\n",
    "            A: shape (d_in, n)\n",
    "            B: shape (b, l, n)\n",
    "            C: shape (b, l, n)\n",
    "            D: shape (d_in,)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
    "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d_in) = u.shape\n",
    "        n = A.shape[1]\n",
    "\n",
    "        # Discretize continuous parameters (A, B)\n",
    "        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
    "        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n",
    "        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
    "        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n",
    "        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
    "\n",
    "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
    "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n",
    "        # is additionally hardware-aware (like FlashAttention).\n",
    "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
    "        ys = []\n",
    "        for i in range(l):\n",
    "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
    "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
    "\n",
    "        y = y + u * D\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4637ba3",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd545717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(root, name=None, debug=True):\n",
    "    #when debug is true, show DEBUG and INFO in screen\n",
    "    #when debug is false, show DEBUG in file and info in both screen&file\n",
    "    #INFO will always be in screen\n",
    "    # create a logger\n",
    "    logger = logging.getLogger(name)\n",
    "    #critical > error > warning > info > debug > notset\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # define the formate\n",
    "    formatter = logging.Formatter('%(asctime)s: %(message)s', \"%Y-%m-%d %H:%M\")\n",
    "    # create another handler for output log to console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    if debug:\n",
    "        console_handler.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        # create a handler for write log to file\n",
    "        logfile = os.path.join(root, 'run.log')\n",
    "        print('Creat Log File in: ', logfile)\n",
    "        file_handler = logging.FileHandler(logfile, mode='w')\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    # add Handler to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    if not debug:\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def init_seed(seed):\n",
    "    '''\n",
    "    Disable cudnn to maximize reproducibility\n",
    "    '''\n",
    "    torch.cuda.cudnn_enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def init_device(opt):\n",
    "    if torch.cuda.is_available():\n",
    "        opt.cuda = True\n",
    "        torch.cuda.set_device(int(opt.device[5]))\n",
    "    else:\n",
    "        opt.cuda = False\n",
    "        opt.device = 'cpu'\n",
    "    return opt\n",
    "\n",
    "def init_optim(model, opt):\n",
    "    '''\n",
    "    Initialize optimizer\n",
    "    '''\n",
    "    return torch.optim.Adam(params=model.parameters(),lr=opt.lr_init)\n",
    "\n",
    "def init_lr_scheduler(optim, opt):\n",
    "    '''\n",
    "    Initialize the learning rate scheduler\n",
    "    '''\n",
    "    #return torch.optim.lr_scheduler.StepLR(optimizer=optim,gamma=opt.lr_scheduler_rate,step_size=opt.lr_scheduler_step)\n",
    "    return torch.optim.lr_scheduler.MultiStepLR(optimizer=optim, milestones=opt.lr_decay_steps,\n",
    "                                                gamma = opt.lr_scheduler_rate)\n",
    "\n",
    "def print_model_parameters(model, only_num = True):\n",
    "    \n",
    "\n",
    "    \n",
    "    print('*****************Model Parameter*****************')\n",
    "    if not only_num:\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape, param.requires_grad)\n",
    "    total_num = sum([param.nelement() for param in model.parameters()])\n",
    "    print('Total params num: {}'.format(total_num))\n",
    "    print('*****************Finish Parameter****************')\n",
    "\n",
    "def get_memory_usage(device):\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / (1024*1024.)\n",
    "    cached_memory = torch.cuda.memory_cached(device) / (1024*1024.)\n",
    "    return allocated_memory, cached_memory\n",
    "    #print('Allocated Memory: {:.2f} MB, Cached Memory: {:.2f} MB'.format(allocated_memory, cached_memory))\n",
    "\n",
    "\n",
    "def MAE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true-pred))\n",
    "\n",
    "def MSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean((pred - true) ** 2)\n",
    "\n",
    "def RMSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.mean((pred - true) ** 2))\n",
    "\n",
    "def RRSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.sum((pred - true) ** 2)) / torch.sqrt(torch.sum((pred - true.mean()) ** 2))\n",
    "\n",
    "\n",
    "\n",
    "def MAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(torch.div((true - pred), true)))\n",
    "\n",
    "def PNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    indicator = torch.gt(pred - true, 0).float()\n",
    "    return indicator.mean()\n",
    "\n",
    "def oPNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    bias = (true+pred) / (2*true)\n",
    "    return bias.mean()\n",
    "\n",
    "def MARE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.div(torch.sum(torch.abs((true - pred))), torch.sum(true))\n",
    "\n",
    "def SMAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true-pred)/(torch.abs(true)+torch.abs(pred)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def All_Metrics(pred, true, mask1, mask2):\n",
    "    #mask1 filter the very small value, mask2 filter the value lower than a defined threshold\n",
    "    assert type(pred) == type(true)\n",
    "    #if type(pred) == np.ndarray:\n",
    "    #    mae  = MAE_np(pred, true, mask1)\n",
    "    #    rmse = RMSE_np(pred, true, mask1)\n",
    "    #    mape = MAPE_np(pred, true, mask2)\n",
    "    #    rrse = RRSE_np(pred, true, mask1)\n",
    "\n",
    "        #corr = CORR_np(pred, true, mask1)\n",
    "        #pnbi = PNBI_np(pred, true, mask1)\n",
    "        #opnbi = oPNBI_np(pred, true, mask2)\n",
    "    if type(pred) == torch.Tensor:\n",
    "        mae  = MAE_torch(pred, true, mask1)\n",
    "        rmse = RMSE_torch(pred, true, mask1)\n",
    "        rrse = RRSE_torch(pred, true, mask1)\n",
    "\n",
    "        #pnbi = PNBI_torch(pred, true, mask1)\n",
    "        #opnbi = oPNBI_torch(pred, true, mask2)\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return mae, rmse, rrse\n",
    "\n",
    "def SIGIR_Metrics(pred, true, mask1, mask2):\n",
    "    rrse = RRSE_torch(pred, true, mask1)\n",
    "    corr = CORR_torch(pred, true, 0)\n",
    "    return rrse, corr\n",
    "\n",
    "def save_model(model, model_dir, epoch=None):\n",
    "    if model_dir is None:\n",
    "        return\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    epoch = str(epoch) if epoch else \"\"\n",
    "    file_name = os.path.join(model_dir, epoch + \"_stemgnn.pt\")\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        torch.save(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7aa212",
   "metadata": {},
   "source": [
    "#### Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce4a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, loss, optimizer, train_loader, val_loader, test_loader,\n",
    "                 args, lr_scheduler=None):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        # self.scaler = scaler\n",
    "        self.args = args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.train_per_epoch = len(train_loader)\n",
    "        if val_loader != None:\n",
    "            self.val_per_epoch = len(val_loader)\n",
    "        self.best_path = os.path.join(self.args.get('log_dir'), 'best_model.pth')\n",
    "        self.loss_figure_path = os.path.join(self.args.get('log_dir'), 'loss.png')\n",
    "        # log\n",
    "        if os.path.isdir(args.get('log_dir')) == False and not args.get('debug'):\n",
    "            os.makedirs(args.get('log_dir'), exist_ok=True)\n",
    "        self.logger = get_logger(args.get('log_dir'), name=args.get('model'), debug=args.get('debug'))\n",
    "        self.logger.info('Experiment log path in: {}'.format(args.get('log_dir')))\n",
    "        # if not args.debug:\n",
    "        # self.logger.info(\"Argument: %r\", args)\n",
    "        # for arg, value in sorted(vars(args).items()):\n",
    "        #     self.logger.info(\"Argument %s: %r\", arg, value)\n",
    "\n",
    "    def val_epoch(self, epoch, val_dataloader):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_dataloader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = self.model(data)\n",
    "                # if self.args.get('real_value'):\n",
    "                # label = self.scaler.inverse_transform(label)\n",
    "                loss = self.loss(output, label)\n",
    "                # a whole batch of Metr_LA is filtered\n",
    "                if not torch.isnan(loss):\n",
    "                    total_val_loss += loss.item()\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        self.logger.info('**********Val Epoch {}: average Loss: {:.6f}'.format(epoch, val_loss))\n",
    "        return val_loss\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        loss_values=[]\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data = data\n",
    "            label = target  # (..., 1)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # data and target shape: B, T, N, F; output shape: B, T, N, F\n",
    "            output = self.model(data)\n",
    "            # if self.args.get('real_value'):\n",
    "            #   label = self.scaler.inverse_transform(label)\n",
    "\n",
    "\n",
    "            loss = self.loss(output, label)\n",
    "            loss = self.loss(output, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # add max grad clipping\n",
    "            if self.args.get('grad_norm'):\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.get('max_grad_norm'))\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            # log information\n",
    "            if batch_idx % self.args.get('log_step') == 0:\n",
    "                self.logger.info('Train Epoch {}: {}/{} Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx, self.train_per_epoch, loss.item()))\n",
    "        train_epoch_loss = total_loss / self.train_per_epoch\n",
    "        self.logger.info(\n",
    "            '**********Train Epoch {}: averaged Loss: {:.6f}'.format(epoch, train_epoch_loss))\n",
    "\n",
    "        # learning rate decay\n",
    "        if self.args.get('lr_decay'):\n",
    "            self.lr_scheduler.step()\n",
    "        return train_epoch_loss\n",
    "\n",
    "    def train(self):\n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        not_improved_count = 0\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, self.args.get('epochs') + 1):\n",
    "            # epoch_time = time.time()\n",
    "            train_epoch_loss = self.train_epoch(epoch)\n",
    "            # print(time.time()-epoch_time)\n",
    "            # exit()\n",
    "            if self.val_loader == None:\n",
    "                val_dataloader = self.test_loader\n",
    "            else:\n",
    "                val_dataloader = self.val_loader\n",
    "            val_epoch_loss = self.val_epoch(epoch, val_dataloader)\n",
    "\n",
    "            # print('LR:', self.optimizer.param_groups[0]['lr'])\n",
    "            train_loss_list.append(train_epoch_loss)\n",
    "            val_loss_list.append(val_epoch_loss)\n",
    "            if train_epoch_loss > 1e6:\n",
    "                self.logger.warning('Gradient explosion detected. Ending...')\n",
    "                break\n",
    "            # if self.val_loader == None:\n",
    "            # val_epoch_loss = train_epoch_loss\n",
    "            if val_epoch_loss < best_loss:\n",
    "                best_loss = val_epoch_loss\n",
    "                not_improved_count = 0\n",
    "                best_state = True\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "                best_state = False\n",
    "            # early stop\n",
    "            if self.args.get('early_stop'):\n",
    "                if not_improved_count == self.args.get('early_stop_patience'):\n",
    "                    self.logger.info(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                                     \"Training stops.\".format(self.args.get('early_stop_patience')))\n",
    "                    break\n",
    "            # save the best state\n",
    "            if best_state == True:\n",
    "                self.logger.info('*********************************Current best model saved!')\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        self.logger.info(\"Total training time: {:.4f}min, best loss: {:.6f}\".format((training_time / 60), best_loss))\n",
    "\n",
    "        with open('milan_sms_mamaba.txt', 'a') as f:\n",
    "            f.write(str(epoch))\n",
    "            f.write('\\n')\n",
    "            f.write(str(training_time / 60))\n",
    "            f.write('\\n')\n",
    "\n",
    "        # save the best model to file\n",
    "        if not self.args.get('debug'):\n",
    "            torch.save(best_model, self.best_path)\n",
    "            self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "        # test\n",
    "        self.model.load_state_dict(best_model)\n",
    "        # self.val_epoch(self.args.epochs, self.test_loader)\n",
    "        y1, y2 = self.test(self.model, self.args, self.test_loader, self.logger)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        state = {\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'config': self.args\n",
    "        }\n",
    "        torch.save(state, self.best_path)\n",
    "        self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def test(model, args, data_loader, logger, path=None):\n",
    "        if path != None:\n",
    "            check_point = torch.load(path)\n",
    "            state_dict = check_point['state_dict']\n",
    "            args = check_point['config']\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(args.get('device'))\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = model(data)\n",
    "\n",
    "                y_true.append(label)\n",
    "                y_pred.append(output)\n",
    "\n",
    "                #print(model.forward(data, [], teacher_forcing_ratio=0))\n",
    "        # y_true = scaler.inverse_transform(torch.cat(y_true, dim=0))\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        y_true = torch.cat(y_true, dim=0)\n",
    "        # if not args.get('real_value'):\n",
    "        #    y_pred = torch.cat(y_pred, dim=0)\n",
    "        # else:\n",
    "        # y_pred = scaler.inverse_transform(torch.cat(y_pred, dim=0))\n",
    "        # np.save('./{}_true.npy'.format(args.get('dataset')), y_true.cpu().numpy())\n",
    "        # np.save('./{}_pred.npy'.format(args.get('dataset')), y_pred.cpu().numpy())\n",
    "        # for t in range(y_true.shape[1]):\n",
    "        #    mae, rmse, mape, _ = All_Metrics(y_pred[:, t, ...], y_true[:, t, ...],\n",
    "        #                                        args.get('mae_thresh'), args.get('mape_thresh'))\n",
    "        #    logger.info(\"Horizon {:02d}, MAE: {:.2f}, RMSE: {:.2f}, MAPE: {:.4f}%\".format(\n",
    "        #        t + 1, mae, rmse, mape*100))\n",
    "        mae, rmse, _ = All_Metrics(y_pred, y_true, args.get('mae_thresh'), args.get('mape_thresh'))\n",
    "        logger.info(\"Average Horizon, MAE: {:.4f}, MSE: {:.4f}\".format(\n",
    "            mae, rmse))\n",
    "        return y_pred, y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_sampling_threshold(global_step, k):\n",
    "        \"\"\"\n",
    "        Computes the sampling probability for scheduled sampling using inverse sigmoid.\n",
    "        :param global_step:\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return k / (k + math.exp(global_step / k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbc569",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e1fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14619/4118148576.py:92: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  X, Y = TensorFloat(X), TensorFloat(Y)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('combined_dataframe_DJI.csv', index_col=\"Date\", parse_dates=True)\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "name = X[\"Name\"][0]\n",
    "del X[\"Name\"]\n",
    "cols = X.columns\n",
    "X[\"Target\"] = (X[\"Price\"].pct_change().shift(-1) > 0).astype(int)\n",
    "X.dropna(inplace=True)\n",
    "    # Fit the standard scaler using the training dataset\n",
    "    \n",
    "class MinMaxNorm01(object):\n",
    "    \"\"\"scale data to range [0, 1]\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.min = x.min()\n",
    "        self.max = x.max()\n",
    "        #print('Min:{}, Max:{}'.format(self.min, self.max))\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = 1.0 * (x - self.min) / (self.max - self.min)\n",
    "        return x\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        x = x * (self.max - self.min) + self.min\n",
    "        return x\n",
    "\n",
    "a=X.to_numpy()\n",
    "\n",
    "#data1=train_data.to_numpy()\n",
    "\n",
    "mmn = MinMaxNorm01()\n",
    "\n",
    "data=a\n",
    "\n",
    "\n",
    "dataset = mmn.fit_transform(data)\n",
    "\n",
    "window=5\n",
    "predict=1\n",
    "\n",
    "ran=data.shape[0]\n",
    "i=0\n",
    "X=[]\n",
    "Y=[]\n",
    "while i+window<ran:\n",
    "\n",
    "    X.append(torch.Tensor(dataset[i:i+window,1:]))\n",
    "    Y.append(torch.Tensor(dataset[i+window:i+window+predict,0]))\n",
    "    i+=1\n",
    "\n",
    "XX=torch.stack(X,dim=0)\n",
    "YY=torch.stack(Y,dim=0)\n",
    "YY=YY[:,:,None]\n",
    "\n",
    "\n",
    "## 原程式碼\n",
    "# test_len = int(0.15*XX.shape[0])\n",
    "# val_len = int(0.05*XX.shape[0])\n",
    "# train_len =  XX.shape[0]-test_len-val_len\n",
    "\n",
    "\n",
    "\n",
    "# X_test=torch.Tensor.float(XX[:test_len,:,:]).cuda()\n",
    "\n",
    "\n",
    "# Y_test=torch.Tensor.float(YY[:test_len,:,:]).cuda()\n",
    "\n",
    "# X_train=torch.Tensor.float(XX[test_len:test_len+train_len,:,:]).cuda()\n",
    "# Y_train=torch.Tensor.float(YY[test_len:test_len+train_len,:,:]).cuda()\n",
    "\n",
    "# X_val=torch.Tensor.float(XX[-val_len:,:,:]).cuda()\n",
    "# Y_val=torch.Tensor.float(YY[-val_len:,:,:]).cuda()\n",
    "#######################################\n",
    "## 更改成下方 用前面預測後面 ############\n",
    "#######################################\n",
    "total_len = XX.shape[0]\n",
    "test_len = int(0.15 * total_len)\n",
    "val_len = int(0.05 * total_len)\n",
    "train_len = total_len - test_len - val_len\n",
    "\n",
    "# 根據時間順序切割\n",
    "X_train = XX[:train_len]\n",
    "Y_train = YY[:train_len]\n",
    "\n",
    "X_val = XX[train_len:train_len + val_len]\n",
    "Y_val = YY[train_len:train_len + val_len]\n",
    "\n",
    "X_test = XX[train_len + val_len:]\n",
    "Y_test = YY[train_len + val_len:]\n",
    "\n",
    "# 移至 GPU\n",
    "X_train = X_train.cuda()\n",
    "Y_train = Y_train.cuda()\n",
    "X_val = X_val.cuda()\n",
    "Y_val = Y_val.cuda()\n",
    "X_test = X_test.cuda()\n",
    "Y_test = Y_test.cuda()\n",
    "########################\n",
    "########################\n",
    "\n",
    "def data_loader(X, Y, batch_size, shuffle=True, drop_last=True):\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    TensorFloat = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    X, Y = TensorFloat(X), TensorFloat(Y)\n",
    "    data = torch.utils.data.TensorDataset(X, Y)\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                             shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader\n",
    "\n",
    "train_loader = data_loader(X_train, Y_train, 64, shuffle=False, drop_last=False)\n",
    "val_loader = data_loader(X_val, Y_val, 64, shuffle=False, drop_last=False)\n",
    "test_loader = data_loader(X_test, Y_test, 64, shuffle=False, drop_last=False)\n",
    "\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408e1a9",
   "metadata": {},
   "source": [
    "#### correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9dc1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Ensure the tensors are of type float32\n",
    "    x = x.float()\n",
    "    y = y.float()\n",
    "\n",
    "    # Compute the mean of each tensor\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "\n",
    "    # Compute the deviations from the mean\n",
    "    dev_x = x - mean_x\n",
    "    dev_y = y - mean_y\n",
    "\n",
    "    # Compute the covariance between x and y\n",
    "    covariance = torch.sum(dev_x * dev_y)\n",
    "\n",
    "    # Compute the standard deviations of x and y\n",
    "    std_x = torch.sqrt(torch.sum(dev_x ** 2))\n",
    "    std_y = torch.sqrt(torch.sum(dev_y ** 2))\n",
    "\n",
    "    # Compute the Pearson correlation coefficient\n",
    "    pearson_corr = covariance / (std_x * std_y)\n",
    "\n",
    "    return pearson_corr\n",
    "\n",
    "def rank_tensor(x):\n",
    "    \"\"\"\n",
    "    Return the ranks of elements in a tensor.\n",
    "    \n",
    "    Args:\n",
    "    x (torch.Tensor): Input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Ranks of the input tensor elements.\n",
    "    \"\"\"\n",
    "    # Get the sorted indices\n",
    "    sorted_indices = torch.argsort(x)\n",
    "    \n",
    "    # Create an empty tensor to hold the ranks\n",
    "    ranks = torch.zeros_like(sorted_indices, dtype=torch.float)\n",
    "    \n",
    "    # Assign ranks based on sorted indices\n",
    "    ranks[sorted_indices] = torch.arange(1, len(x) + 1).float()\n",
    "    \n",
    "    return ranks\n",
    "\n",
    "def rank_information_coefficient(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Rank Information Coefficient (RIC) or Spearman's Rank Correlation Coefficient.\n",
    "    \n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Rank Information Coefficient (RIC).\n",
    "    \"\"\"\n",
    "    # Get the ranks of the elements in x and y\n",
    "    rank_x = rank_tensor(x)\n",
    "    rank_y = rank_tensor(y)\n",
    "\n",
    "    # Calculate the mean rank for both tensors\n",
    "    mean_rank_x = torch.mean(rank_x)\n",
    "    mean_rank_y = torch.mean(rank_y)\n",
    "\n",
    "    # Calculate the covariance of the rank variables\n",
    "    covariance = torch.sum((rank_x - mean_rank_x) * (rank_y - mean_rank_y))\n",
    "\n",
    "    # Calculate the standard deviations of the ranks\n",
    "    std_rank_x = torch.sqrt(torch.sum((rank_x - mean_rank_x) ** 2))\n",
    "    std_rank_y = torch.sqrt(torch.sum((rank_y - mean_rank_y) ** 2))\n",
    "\n",
    "    # Calculate the Spearman rank correlation (RIC)\n",
    "    ric = covariance / (std_rank_x * std_rank_y)\n",
    "    \n",
    "    return ric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f52c11",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 19:32: Experiment log path in: ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([82, 10]) True\n",
      "embed_w torch.Size([10, 10]) True\n",
      "weights_pool torch.Size([10, 3, 5, 1]) True\n",
      "bias_pool torch.Size([10, 1]) True\n",
      "mam1.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.lm_head.bias torch.Size([82]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([82]) True\n",
      "mam1.nnl.bias torch.Size([82]) True\n",
      "proj.weight torch.Size([1, 82]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 240912\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 19:32: Train Epoch 1: 0/37 Loss: 8941.763672\n",
      "2025-07-08 19:32: Train Epoch 1: 20/37 Loss: 1836.471924\n",
      "2025-07-08 19:33: **********Train Epoch 1: averaged Loss: 2644.391642\n",
      "2025-07-08 19:33: **********Val Epoch 1: average Loss: 8528.557943\n",
      "2025-07-08 19:33: *********************************Current best model saved!\n",
      "2025-07-08 19:33: Train Epoch 2: 0/37 Loss: 15249.052734\n",
      "2025-07-08 19:33: Train Epoch 2: 20/37 Loss: 4613.915039\n",
      "2025-07-08 19:33: **********Train Epoch 2: averaged Loss: 4662.328801\n",
      "2025-07-08 19:33: **********Val Epoch 2: average Loss: 3844.338542\n",
      "2025-07-08 19:33: *********************************Current best model saved!\n",
      "2025-07-08 19:33: Train Epoch 3: 0/37 Loss: 19064.849609\n",
      "2025-07-08 19:33: Train Epoch 3: 20/37 Loss: 4329.066406\n",
      "2025-07-08 19:33: **********Train Epoch 3: averaged Loss: 6098.615439\n",
      "2025-07-08 19:33: **********Val Epoch 3: average Loss: 7684.807943\n",
      "2025-07-08 19:33: Train Epoch 4: 0/37 Loss: 16938.089844\n",
      "2025-07-08 19:33: Train Epoch 4: 20/37 Loss: 3597.266602\n",
      "2025-07-08 19:33: **********Train Epoch 4: averaged Loss: 5272.958994\n",
      "2025-07-08 19:33: **********Val Epoch 4: average Loss: 5124.536621\n",
      "2025-07-08 19:33: Train Epoch 5: 0/37 Loss: 17743.136719\n",
      "2025-07-08 19:33: Train Epoch 5: 20/37 Loss: 4557.585449\n",
      "2025-07-08 19:33: **********Train Epoch 5: averaged Loss: 7486.203744\n",
      "2025-07-08 19:33: **********Val Epoch 5: average Loss: 7205.851074\n",
      "2025-07-08 19:33: Train Epoch 6: 0/37 Loss: 16602.148438\n",
      "2025-07-08 19:33: Train Epoch 6: 20/37 Loss: 1518.313477\n",
      "2025-07-08 19:33: **********Train Epoch 6: averaged Loss: 9295.192391\n",
      "2025-07-08 19:33: **********Val Epoch 6: average Loss: 12302.293620\n",
      "2025-07-08 19:33: Train Epoch 7: 0/37 Loss: 12269.387695\n",
      "2025-07-08 19:33: Train Epoch 7: 20/37 Loss: 1086.256104\n",
      "2025-07-08 19:33: **********Train Epoch 7: averaged Loss: 5960.119375\n",
      "2025-07-08 19:33: **********Val Epoch 7: average Loss: 10779.949219\n",
      "2025-07-08 19:33: Train Epoch 8: 0/37 Loss: 12054.034180\n",
      "2025-07-08 19:33: Train Epoch 8: 20/37 Loss: 1663.525024\n",
      "2025-07-08 19:33: **********Train Epoch 8: averaged Loss: 7877.822749\n",
      "2025-07-08 19:33: **********Val Epoch 8: average Loss: 13916.300130\n",
      "2025-07-08 19:33: Train Epoch 9: 0/37 Loss: 8774.224609\n",
      "2025-07-08 19:33: Train Epoch 9: 20/37 Loss: 737.128052\n",
      "2025-07-08 19:33: **********Train Epoch 9: averaged Loss: 5830.749927\n",
      "2025-07-08 19:33: **********Val Epoch 9: average Loss: 12262.606120\n",
      "2025-07-08 19:33: Train Epoch 10: 0/37 Loss: 12207.000977\n",
      "2025-07-08 19:33: Train Epoch 10: 20/37 Loss: 1449.825562\n",
      "2025-07-08 19:33: **********Train Epoch 10: averaged Loss: 6727.025016\n",
      "2025-07-08 19:33: **********Val Epoch 10: average Loss: 13356.882161\n",
      "2025-07-08 19:33: Train Epoch 11: 0/37 Loss: 9841.946289\n",
      "2025-07-08 19:33: Train Epoch 11: 20/37 Loss: 651.017639\n",
      "2025-07-08 19:33: **********Train Epoch 11: averaged Loss: 6069.152087\n",
      "2025-07-08 19:33: **********Val Epoch 11: average Loss: 12269.934896\n",
      "2025-07-08 19:33: Train Epoch 12: 0/37 Loss: 10031.818359\n",
      "2025-07-08 19:33: Train Epoch 12: 20/37 Loss: 670.213745\n",
      "2025-07-08 19:33: **********Train Epoch 12: averaged Loss: 6232.663220\n",
      "2025-07-08 19:33: **********Val Epoch 12: average Loss: 12988.840169\n",
      "2025-07-08 19:33: Train Epoch 13: 0/37 Loss: 9434.019531\n",
      "2025-07-08 19:33: Train Epoch 13: 20/37 Loss: 614.876770\n",
      "2025-07-08 19:33: **********Train Epoch 13: averaged Loss: 5973.417633\n",
      "2025-07-08 19:33: **********Val Epoch 13: average Loss: 12530.396810\n",
      "2025-07-08 19:33: Train Epoch 14: 0/37 Loss: 11067.984375\n",
      "2025-07-08 19:33: Train Epoch 14: 20/37 Loss: 611.394348\n",
      "2025-07-08 19:33: **********Train Epoch 14: averaged Loss: 6218.571420\n",
      "2025-07-08 19:33: **********Val Epoch 14: average Loss: 12992.156250\n",
      "2025-07-08 19:33: Train Epoch 15: 0/37 Loss: 8881.465820\n",
      "2025-07-08 19:33: Train Epoch 15: 20/37 Loss: 687.787781\n",
      "2025-07-08 19:33: **********Train Epoch 15: averaged Loss: 6072.638202\n",
      "2025-07-08 19:33: **********Val Epoch 15: average Loss: 12466.909505\n",
      "2025-07-08 19:33: Train Epoch 16: 0/37 Loss: 10133.564453\n",
      "2025-07-08 19:33: Train Epoch 16: 20/37 Loss: 2437.544434\n",
      "2025-07-08 19:33: **********Train Epoch 16: averaged Loss: 5796.493750\n",
      "2025-07-08 19:33: **********Val Epoch 16: average Loss: 12070.763021\n",
      "2025-07-08 19:33: Train Epoch 17: 0/37 Loss: 10901.577148\n",
      "2025-07-08 19:33: Train Epoch 17: 20/37 Loss: 2272.326660\n",
      "2025-07-08 19:33: **********Train Epoch 17: averaged Loss: 7012.585898\n",
      "2025-07-08 19:33: **********Val Epoch 17: average Loss: 12936.747396\n",
      "2025-07-08 19:33: Train Epoch 18: 0/37 Loss: 8189.321777\n",
      "2025-07-08 19:33: Train Epoch 18: 20/37 Loss: 1749.438721\n",
      "2025-07-08 19:33: **********Train Epoch 18: averaged Loss: 5514.257791\n",
      "2025-07-08 19:33: **********Val Epoch 18: average Loss: 10278.768555\n",
      "2025-07-08 19:33: Train Epoch 19: 0/37 Loss: 11063.231445\n",
      "2025-07-08 19:33: Train Epoch 19: 20/37 Loss: 799.770264\n",
      "2025-07-08 19:33: **********Train Epoch 19: averaged Loss: 4949.006447\n",
      "2025-07-08 19:33: **********Val Epoch 19: average Loss: 9699.159831\n",
      "2025-07-08 19:33: Train Epoch 20: 0/37 Loss: 17416.300781\n",
      "2025-07-08 19:33: Train Epoch 20: 20/37 Loss: 3557.559570\n",
      "2025-07-08 19:33: **********Train Epoch 20: averaged Loss: 6083.692314\n",
      "2025-07-08 19:33: **********Val Epoch 20: average Loss: 3971.775960\n",
      "2025-07-08 19:33: Train Epoch 21: 0/37 Loss: 17250.285156\n",
      "2025-07-08 19:33: Train Epoch 21: 20/37 Loss: 883.401062\n",
      "2025-07-08 19:33: **********Train Epoch 21: averaged Loss: 5464.046463\n",
      "2025-07-08 19:33: **********Val Epoch 21: average Loss: 8909.644531\n",
      "2025-07-08 19:33: Train Epoch 22: 0/37 Loss: 13835.431641\n",
      "2025-07-08 19:33: Train Epoch 22: 20/37 Loss: 648.388306\n",
      "2025-07-08 19:33: **********Train Epoch 22: averaged Loss: 6550.066141\n",
      "2025-07-08 19:33: **********Val Epoch 22: average Loss: 5701.403158\n",
      "2025-07-08 19:33: Train Epoch 23: 0/37 Loss: 13777.856445\n",
      "2025-07-08 19:33: Train Epoch 23: 20/37 Loss: 1445.987305\n",
      "2025-07-08 19:33: **********Train Epoch 23: averaged Loss: 5799.030724\n",
      "2025-07-08 19:33: **********Val Epoch 23: average Loss: 5342.609212\n",
      "2025-07-08 19:33: Train Epoch 24: 0/37 Loss: 9928.622070\n",
      "2025-07-08 19:33: Train Epoch 24: 20/37 Loss: 1613.162964\n",
      "2025-07-08 19:33: **********Train Epoch 24: averaged Loss: 3914.761463\n",
      "2025-07-08 19:33: **********Val Epoch 24: average Loss: 2544.334147\n",
      "2025-07-08 19:33: *********************************Current best model saved!\n",
      "2025-07-08 19:33: Train Epoch 25: 0/37 Loss: 5681.471680\n",
      "2025-07-08 19:33: Train Epoch 25: 20/37 Loss: 2015.505981\n",
      "2025-07-08 19:33: **********Train Epoch 25: averaged Loss: 2816.778632\n",
      "2025-07-08 19:33: **********Val Epoch 25: average Loss: 3159.867757\n",
      "2025-07-08 19:33: Train Epoch 26: 0/37 Loss: 12143.764648\n",
      "2025-07-08 19:33: Train Epoch 26: 20/37 Loss: 1369.895996\n",
      "2025-07-08 19:33: **********Train Epoch 26: averaged Loss: 4791.017080\n",
      "2025-07-08 19:33: **********Val Epoch 26: average Loss: 5351.874186\n",
      "2025-07-08 19:33: Train Epoch 27: 0/37 Loss: 7867.035156\n",
      "2025-07-08 19:33: Train Epoch 27: 20/37 Loss: 569.551453\n",
      "2025-07-08 19:33: **********Train Epoch 27: averaged Loss: 2550.430742\n",
      "2025-07-08 19:33: **********Val Epoch 27: average Loss: 5250.347331\n",
      "2025-07-08 19:33: Train Epoch 28: 0/37 Loss: 9973.302734\n",
      "2025-07-08 19:33: Train Epoch 28: 20/37 Loss: 1183.673584\n",
      "2025-07-08 19:33: **********Train Epoch 28: averaged Loss: 3355.012631\n",
      "2025-07-08 19:33: **********Val Epoch 28: average Loss: 6226.715169\n",
      "2025-07-08 19:33: Train Epoch 29: 0/37 Loss: 7140.757324\n",
      "2025-07-08 19:33: Train Epoch 29: 20/37 Loss: 2943.716797\n",
      "2025-07-08 19:33: **********Train Epoch 29: averaged Loss: 3160.146740\n",
      "2025-07-08 19:33: **********Val Epoch 29: average Loss: 3370.472371\n",
      "2025-07-08 19:33: Train Epoch 30: 0/37 Loss: 7601.254883\n",
      "2025-07-08 19:33: Train Epoch 30: 20/37 Loss: 794.546814\n",
      "2025-07-08 19:33: **********Train Epoch 30: averaged Loss: 4124.593601\n",
      "2025-07-08 19:33: **********Val Epoch 30: average Loss: 8185.041016\n",
      "2025-07-08 19:33: Train Epoch 31: 0/37 Loss: 3280.607910\n",
      "2025-07-08 19:33: Train Epoch 31: 20/37 Loss: 1652.223633\n",
      "2025-07-08 19:33: **********Train Epoch 31: averaged Loss: 1483.012478\n",
      "2025-07-08 19:33: **********Val Epoch 31: average Loss: 4010.383789\n",
      "2025-07-08 19:33: Train Epoch 32: 0/37 Loss: 6092.520020\n",
      "2025-07-08 19:33: Train Epoch 32: 20/37 Loss: 1204.833984\n",
      "2025-07-08 19:33: **********Train Epoch 32: averaged Loss: 3157.184499\n",
      "2025-07-08 19:33: **********Val Epoch 32: average Loss: 7993.898600\n",
      "2025-07-08 19:33: Train Epoch 33: 0/37 Loss: 2893.799805\n",
      "2025-07-08 19:33: Train Epoch 33: 20/37 Loss: 583.112793\n",
      "2025-07-08 19:33: **********Train Epoch 33: averaged Loss: 1848.155069\n",
      "2025-07-08 19:33: **********Val Epoch 33: average Loss: 5119.232747\n",
      "2025-07-08 19:33: Train Epoch 34: 0/37 Loss: 7957.809570\n",
      "2025-07-08 19:33: Train Epoch 34: 20/37 Loss: 451.842773\n",
      "2025-07-08 19:33: **********Train Epoch 34: averaged Loss: 3071.619837\n",
      "2025-07-08 19:33: **********Val Epoch 34: average Loss: 4074.393555\n",
      "2025-07-08 19:33: Train Epoch 35: 0/37 Loss: 3500.333252\n",
      "2025-07-08 19:33: Train Epoch 35: 20/37 Loss: 162.536331\n",
      "2025-07-08 19:34: **********Train Epoch 35: averaged Loss: 1004.131243\n",
      "2025-07-08 19:34: **********Val Epoch 35: average Loss: 1597.277852\n",
      "2025-07-08 19:34: *********************************Current best model saved!\n",
      "2025-07-08 19:34: Train Epoch 36: 0/37 Loss: 1020.001770\n",
      "2025-07-08 19:34: Train Epoch 36: 20/37 Loss: 1117.913574\n",
      "2025-07-08 19:34: **********Train Epoch 36: averaged Loss: 1279.089106\n",
      "2025-07-08 19:34: **********Val Epoch 36: average Loss: 1413.273153\n",
      "2025-07-08 19:34: *********************************Current best model saved!\n",
      "2025-07-08 19:34: Train Epoch 37: 0/37 Loss: 320.180664\n",
      "2025-07-08 19:34: Train Epoch 37: 20/37 Loss: 1752.046875\n",
      "2025-07-08 19:34: **********Train Epoch 37: averaged Loss: 725.281118\n",
      "2025-07-08 19:34: **********Val Epoch 37: average Loss: 2856.098226\n",
      "2025-07-08 19:34: Train Epoch 38: 0/37 Loss: 3278.643555\n",
      "2025-07-08 19:34: Train Epoch 38: 20/37 Loss: 829.471436\n",
      "2025-07-08 19:34: **********Train Epoch 38: averaged Loss: 1447.833993\n",
      "2025-07-08 19:34: **********Val Epoch 38: average Loss: 3355.224528\n",
      "2025-07-08 19:34: Train Epoch 39: 0/37 Loss: 8173.066406\n",
      "2025-07-08 19:34: Train Epoch 39: 20/37 Loss: 1358.922852\n",
      "2025-07-08 19:34: **********Train Epoch 39: averaged Loss: 2867.439163\n",
      "2025-07-08 19:34: **********Val Epoch 39: average Loss: 4297.746582\n",
      "2025-07-08 19:34: Train Epoch 40: 0/37 Loss: 5277.218750\n",
      "2025-07-08 19:34: Train Epoch 40: 20/37 Loss: 752.290833\n",
      "2025-07-08 19:34: **********Train Epoch 40: averaged Loss: 1576.151655\n",
      "2025-07-08 19:34: **********Val Epoch 40: average Loss: 1882.859090\n",
      "2025-07-08 19:34: Train Epoch 41: 0/37 Loss: 5073.686523\n",
      "2025-07-08 19:34: Train Epoch 41: 20/37 Loss: 161.313950\n",
      "2025-07-08 19:34: **********Train Epoch 41: averaged Loss: 2077.024925\n",
      "2025-07-08 19:34: **********Val Epoch 41: average Loss: 3717.704590\n",
      "2025-07-08 19:34: Train Epoch 42: 0/37 Loss: 1222.720459\n",
      "2025-07-08 19:34: Train Epoch 42: 20/37 Loss: 857.632141\n",
      "2025-07-08 19:34: **********Train Epoch 42: averaged Loss: 1012.827606\n",
      "2025-07-08 19:34: **********Val Epoch 42: average Loss: 1822.003906\n",
      "2025-07-08 19:34: Train Epoch 43: 0/37 Loss: 819.104065\n",
      "2025-07-08 19:34: Train Epoch 43: 20/37 Loss: 1130.311646\n",
      "2025-07-08 19:34: **********Train Epoch 43: averaged Loss: 885.956567\n",
      "2025-07-08 19:34: **********Val Epoch 43: average Loss: 1660.701558\n",
      "2025-07-08 19:34: Train Epoch 44: 0/37 Loss: 6223.646484\n",
      "2025-07-08 19:34: Train Epoch 44: 20/37 Loss: 1156.411743\n",
      "2025-07-08 19:34: **********Train Epoch 44: averaged Loss: 2370.739335\n",
      "2025-07-08 19:34: **********Val Epoch 44: average Loss: 3133.741130\n",
      "2025-07-08 19:34: Train Epoch 45: 0/37 Loss: 906.380005\n",
      "2025-07-08 19:34: Train Epoch 45: 20/37 Loss: 837.206787\n",
      "2025-07-08 19:34: **********Train Epoch 45: averaged Loss: 950.728935\n",
      "2025-07-08 19:34: **********Val Epoch 45: average Loss: 1605.193034\n",
      "2025-07-08 19:34: Train Epoch 46: 0/37 Loss: 299.105591\n",
      "2025-07-08 19:34: Train Epoch 46: 20/37 Loss: 888.198730\n",
      "2025-07-08 19:34: **********Train Epoch 46: averaged Loss: 581.465223\n",
      "2025-07-08 19:34: **********Val Epoch 46: average Loss: 4199.000651\n",
      "2025-07-08 19:34: Train Epoch 47: 0/37 Loss: 6936.888672\n",
      "2025-07-08 19:34: Train Epoch 47: 20/37 Loss: 696.435242\n",
      "2025-07-08 19:34: **********Train Epoch 47: averaged Loss: 1960.111185\n",
      "2025-07-08 19:34: **********Val Epoch 47: average Loss: 3077.439412\n",
      "2025-07-08 19:34: Train Epoch 48: 0/37 Loss: 2327.890625\n",
      "2025-07-08 19:34: Train Epoch 48: 20/37 Loss: 1695.465088\n",
      "2025-07-08 19:34: **********Train Epoch 48: averaged Loss: 1062.592447\n",
      "2025-07-08 19:34: **********Val Epoch 48: average Loss: 3256.288452\n",
      "2025-07-08 19:34: Train Epoch 49: 0/37 Loss: 3954.534668\n",
      "2025-07-08 19:34: Train Epoch 49: 20/37 Loss: 508.424408\n",
      "2025-07-08 19:34: **********Train Epoch 49: averaged Loss: 1370.698514\n",
      "2025-07-08 19:34: **********Val Epoch 49: average Loss: 2755.170654\n",
      "2025-07-08 19:34: Train Epoch 50: 0/37 Loss: 4331.773438\n",
      "2025-07-08 19:34: Train Epoch 50: 20/37 Loss: 1694.483887\n",
      "2025-07-08 19:34: **********Train Epoch 50: averaged Loss: 1414.074420\n",
      "2025-07-08 19:34: **********Val Epoch 50: average Loss: 2253.987915\n",
      "2025-07-08 19:34: Train Epoch 51: 0/37 Loss: 1497.763184\n",
      "2025-07-08 19:34: Train Epoch 51: 20/37 Loss: 364.604675\n",
      "2025-07-08 19:34: **********Train Epoch 51: averaged Loss: 909.717705\n",
      "2025-07-08 19:34: **********Val Epoch 51: average Loss: 917.116720\n",
      "2025-07-08 19:34: *********************************Current best model saved!\n",
      "2025-07-08 19:34: Train Epoch 52: 0/37 Loss: 5736.807617\n",
      "2025-07-08 19:34: Train Epoch 52: 20/37 Loss: 384.106079\n",
      "2025-07-08 19:34: **********Train Epoch 52: averaged Loss: 910.860540\n",
      "2025-07-08 19:34: **********Val Epoch 52: average Loss: 3625.350830\n",
      "2025-07-08 19:34: Train Epoch 53: 0/37 Loss: 5036.404297\n",
      "2025-07-08 19:34: Train Epoch 53: 20/37 Loss: 912.843079\n",
      "2025-07-08 19:34: **********Train Epoch 53: averaged Loss: 1889.421047\n",
      "2025-07-08 19:34: **********Val Epoch 53: average Loss: 3626.650472\n",
      "2025-07-08 19:34: Train Epoch 54: 0/37 Loss: 4010.717041\n",
      "2025-07-08 19:34: Train Epoch 54: 20/37 Loss: 314.092102\n",
      "2025-07-08 19:34: **********Train Epoch 54: averaged Loss: 1680.323005\n",
      "2025-07-08 19:34: **********Val Epoch 54: average Loss: 1205.340780\n",
      "2025-07-08 19:34: Train Epoch 55: 0/37 Loss: 3891.365234\n",
      "2025-07-08 19:34: Train Epoch 55: 20/37 Loss: 733.512878\n",
      "2025-07-08 19:34: **********Train Epoch 55: averaged Loss: 1629.244272\n",
      "2025-07-08 19:34: **********Val Epoch 55: average Loss: 2840.590495\n",
      "2025-07-08 19:34: Train Epoch 56: 0/37 Loss: 1406.980591\n",
      "2025-07-08 19:34: Train Epoch 56: 20/37 Loss: 1401.289307\n",
      "2025-07-08 19:34: **********Train Epoch 56: averaged Loss: 820.689420\n",
      "2025-07-08 19:34: **********Val Epoch 56: average Loss: 951.924967\n",
      "2025-07-08 19:34: Train Epoch 57: 0/37 Loss: 5270.007324\n",
      "2025-07-08 19:34: Train Epoch 57: 20/37 Loss: 353.988708\n",
      "2025-07-08 19:34: **********Train Epoch 57: averaged Loss: 1605.807776\n",
      "2025-07-08 19:34: **********Val Epoch 57: average Loss: 3724.312012\n",
      "2025-07-08 19:34: Train Epoch 58: 0/37 Loss: 6682.864258\n",
      "2025-07-08 19:34: Train Epoch 58: 20/37 Loss: 1545.474976\n",
      "2025-07-08 19:34: **********Train Epoch 58: averaged Loss: 2008.080660\n",
      "2025-07-08 19:34: **********Val Epoch 58: average Loss: 2740.594401\n",
      "2025-07-08 19:34: Train Epoch 59: 0/37 Loss: 3658.622070\n",
      "2025-07-08 19:34: Train Epoch 59: 20/37 Loss: 1113.379150\n",
      "2025-07-08 19:34: **********Train Epoch 59: averaged Loss: 1060.249530\n",
      "2025-07-08 19:34: **********Val Epoch 59: average Loss: 768.515462\n",
      "2025-07-08 19:34: *********************************Current best model saved!\n",
      "2025-07-08 19:34: Train Epoch 60: 0/37 Loss: 4659.699219\n",
      "2025-07-08 19:34: Train Epoch 60: 20/37 Loss: 341.261230\n",
      "2025-07-08 19:34: **********Train Epoch 60: averaged Loss: 1025.164062\n",
      "2025-07-08 19:34: **********Val Epoch 60: average Loss: 3564.081462\n",
      "2025-07-08 19:34: Train Epoch 61: 0/37 Loss: 5032.923828\n",
      "2025-07-08 19:34: Train Epoch 61: 20/37 Loss: 520.082764\n",
      "2025-07-08 19:34: **********Train Epoch 61: averaged Loss: 1540.262053\n",
      "2025-07-08 19:34: **********Val Epoch 61: average Loss: 2144.871989\n",
      "2025-07-08 19:34: Train Epoch 62: 0/37 Loss: 682.686890\n",
      "2025-07-08 19:34: Train Epoch 62: 20/37 Loss: 257.829224\n",
      "2025-07-08 19:34: **********Train Epoch 62: averaged Loss: 573.174916\n",
      "2025-07-08 19:34: **********Val Epoch 62: average Loss: 3683.590251\n",
      "2025-07-08 19:34: Train Epoch 63: 0/37 Loss: 1262.238647\n",
      "2025-07-08 19:34: Train Epoch 63: 20/37 Loss: 286.557983\n",
      "2025-07-08 19:34: **********Train Epoch 63: averaged Loss: 651.765890\n",
      "2025-07-08 19:34: **********Val Epoch 63: average Loss: 1902.507609\n",
      "2025-07-08 19:34: Train Epoch 64: 0/37 Loss: 4868.545898\n",
      "2025-07-08 19:34: Train Epoch 64: 20/37 Loss: 403.511047\n",
      "2025-07-08 19:34: **********Train Epoch 64: averaged Loss: 1270.470412\n",
      "2025-07-08 19:34: **********Val Epoch 64: average Loss: 3415.760579\n",
      "2025-07-08 19:34: Train Epoch 65: 0/37 Loss: 3175.728027\n",
      "2025-07-08 19:34: Train Epoch 65: 20/37 Loss: 545.532288\n",
      "2025-07-08 19:34: **********Train Epoch 65: averaged Loss: 985.372317\n",
      "2025-07-08 19:34: **********Val Epoch 65: average Loss: 3968.390951\n",
      "2025-07-08 19:34: Train Epoch 66: 0/37 Loss: 1796.137329\n",
      "2025-07-08 19:34: Train Epoch 66: 20/37 Loss: 343.755493\n",
      "2025-07-08 19:34: **********Train Epoch 66: averaged Loss: 685.047838\n",
      "2025-07-08 19:34: **********Val Epoch 66: average Loss: 2576.876465\n",
      "2025-07-08 19:34: Train Epoch 67: 0/37 Loss: 2163.264893\n",
      "2025-07-08 19:34: Train Epoch 67: 20/37 Loss: 184.140167\n",
      "2025-07-08 19:34: **********Train Epoch 67: averaged Loss: 696.361126\n",
      "2025-07-08 19:34: **********Val Epoch 67: average Loss: 2688.036418\n",
      "2025-07-08 19:34: Train Epoch 68: 0/37 Loss: 828.873169\n",
      "2025-07-08 19:34: Train Epoch 68: 20/37 Loss: 1256.012451\n",
      "2025-07-08 19:34: **********Train Epoch 68: averaged Loss: 486.473048\n",
      "2025-07-08 19:34: **********Val Epoch 68: average Loss: 2587.266195\n",
      "2025-07-08 19:34: Train Epoch 69: 0/37 Loss: 2238.934814\n",
      "2025-07-08 19:34: Train Epoch 69: 20/37 Loss: 734.833435\n",
      "2025-07-08 19:34: **********Train Epoch 69: averaged Loss: 872.125492\n",
      "2025-07-08 19:34: **********Val Epoch 69: average Loss: 3291.134603\n",
      "2025-07-08 19:34: Train Epoch 70: 0/37 Loss: 4935.808594\n",
      "2025-07-08 19:35: Train Epoch 70: 20/37 Loss: 319.386780\n",
      "2025-07-08 19:35: **********Train Epoch 70: averaged Loss: 1556.862461\n",
      "2025-07-08 19:35: **********Val Epoch 70: average Loss: 1816.363525\n",
      "2025-07-08 19:35: Train Epoch 71: 0/37 Loss: 1626.790527\n",
      "2025-07-08 19:35: Train Epoch 71: 20/37 Loss: 197.189484\n",
      "2025-07-08 19:35: **********Train Epoch 71: averaged Loss: 535.276685\n",
      "2025-07-08 19:35: **********Val Epoch 71: average Loss: 2451.528564\n",
      "2025-07-08 19:35: Train Epoch 72: 0/37 Loss: 1077.282959\n",
      "2025-07-08 19:35: Train Epoch 72: 20/37 Loss: 422.132568\n",
      "2025-07-08 19:35: **********Train Epoch 72: averaged Loss: 1127.126959\n",
      "2025-07-08 19:35: **********Val Epoch 72: average Loss: 2717.854248\n",
      "2025-07-08 19:35: Train Epoch 73: 0/37 Loss: 7402.034668\n",
      "2025-07-08 19:35: Train Epoch 73: 20/37 Loss: 526.264343\n",
      "2025-07-08 19:35: **********Train Epoch 73: averaged Loss: 2832.070270\n",
      "2025-07-08 19:35: **********Val Epoch 73: average Loss: 863.773885\n",
      "2025-07-08 19:35: Train Epoch 74: 0/37 Loss: 4646.893066\n",
      "2025-07-08 19:35: Train Epoch 74: 20/37 Loss: 383.379150\n",
      "2025-07-08 19:35: **********Train Epoch 74: averaged Loss: 1150.073269\n",
      "2025-07-08 19:35: **********Val Epoch 74: average Loss: 2939.512939\n",
      "2025-07-08 19:35: Train Epoch 75: 0/37 Loss: 6301.150391\n",
      "2025-07-08 19:35: Train Epoch 75: 20/37 Loss: 281.502686\n",
      "2025-07-08 19:35: **********Train Epoch 75: averaged Loss: 1614.029678\n",
      "2025-07-08 19:35: **********Val Epoch 75: average Loss: 3739.349854\n",
      "2025-07-08 19:35: Train Epoch 76: 0/37 Loss: 1907.122925\n",
      "2025-07-08 19:35: Train Epoch 76: 20/37 Loss: 409.059387\n",
      "2025-07-08 19:35: **********Train Epoch 76: averaged Loss: 725.448688\n",
      "2025-07-08 19:35: **********Val Epoch 76: average Loss: 2563.957926\n",
      "2025-07-08 19:35: Train Epoch 77: 0/37 Loss: 192.028671\n",
      "2025-07-08 19:35: Train Epoch 77: 20/37 Loss: 562.239197\n",
      "2025-07-08 19:35: **********Train Epoch 77: averaged Loss: 586.930777\n",
      "2025-07-08 19:35: **********Val Epoch 77: average Loss: 2405.377075\n",
      "2025-07-08 19:35: Train Epoch 78: 0/37 Loss: 864.665283\n",
      "2025-07-08 19:35: Train Epoch 78: 20/37 Loss: 471.723541\n",
      "2025-07-08 19:35: **********Train Epoch 78: averaged Loss: 801.211755\n",
      "2025-07-08 19:35: **********Val Epoch 78: average Loss: 1439.792379\n",
      "2025-07-08 19:35: Train Epoch 79: 0/37 Loss: 3281.331543\n",
      "2025-07-08 19:35: Train Epoch 79: 20/37 Loss: 294.294159\n",
      "2025-07-08 19:35: **********Train Epoch 79: averaged Loss: 1232.600507\n",
      "2025-07-08 19:35: **********Val Epoch 79: average Loss: 1720.188761\n",
      "2025-07-08 19:35: Train Epoch 80: 0/37 Loss: 4699.935547\n",
      "2025-07-08 19:35: Train Epoch 80: 20/37 Loss: 210.980011\n",
      "2025-07-08 19:35: **********Train Epoch 80: averaged Loss: 1309.288994\n",
      "2025-07-08 19:35: **********Val Epoch 80: average Loss: 2787.689697\n",
      "2025-07-08 19:35: Train Epoch 81: 0/37 Loss: 2081.354004\n",
      "2025-07-08 19:35: Train Epoch 81: 20/37 Loss: 1087.878906\n",
      "2025-07-08 19:35: **********Train Epoch 81: averaged Loss: 661.558481\n",
      "2025-07-08 19:35: **********Val Epoch 81: average Loss: 1961.612142\n",
      "2025-07-08 19:35: Train Epoch 82: 0/37 Loss: 2205.723145\n",
      "2025-07-08 19:35: Train Epoch 82: 20/37 Loss: 411.214233\n",
      "2025-07-08 19:35: **********Train Epoch 82: averaged Loss: 1336.352998\n",
      "2025-07-08 19:35: **********Val Epoch 82: average Loss: 2577.168091\n",
      "2025-07-08 19:35: Train Epoch 83: 0/37 Loss: 8672.707031\n",
      "2025-07-08 19:35: Train Epoch 83: 20/37 Loss: 623.490112\n",
      "2025-07-08 19:35: **********Train Epoch 83: averaged Loss: 2962.815318\n",
      "2025-07-08 19:35: **********Val Epoch 83: average Loss: 2938.988525\n",
      "2025-07-08 19:35: Train Epoch 84: 0/37 Loss: 3129.138184\n",
      "2025-07-08 19:35: Train Epoch 84: 20/37 Loss: 1118.744995\n",
      "2025-07-08 19:35: **********Train Epoch 84: averaged Loss: 1037.614835\n",
      "2025-07-08 19:35: **********Val Epoch 84: average Loss: 1004.034241\n",
      "2025-07-08 19:35: Train Epoch 85: 0/37 Loss: 449.592224\n",
      "2025-07-08 19:35: Train Epoch 85: 20/37 Loss: 716.280396\n",
      "2025-07-08 19:35: **********Train Epoch 85: averaged Loss: 611.107473\n",
      "2025-07-08 19:35: **********Val Epoch 85: average Loss: 3526.944743\n",
      "2025-07-08 19:35: Train Epoch 86: 0/37 Loss: 5426.216797\n",
      "2025-07-08 19:35: Train Epoch 86: 20/37 Loss: 780.560181\n",
      "2025-07-08 19:35: **********Train Epoch 86: averaged Loss: 1581.020377\n",
      "2025-07-08 19:35: **********Val Epoch 86: average Loss: 3394.622070\n",
      "2025-07-08 19:35: Train Epoch 87: 0/37 Loss: 3690.271973\n",
      "2025-07-08 19:35: Train Epoch 87: 20/37 Loss: 1383.804688\n",
      "2025-07-08 19:35: **********Train Epoch 87: averaged Loss: 1398.720466\n",
      "2025-07-08 19:35: **********Val Epoch 87: average Loss: 1509.835978\n",
      "2025-07-08 19:35: Train Epoch 88: 0/37 Loss: 1901.446167\n",
      "2025-07-08 19:35: Train Epoch 88: 20/37 Loss: 323.047241\n",
      "2025-07-08 19:35: **********Train Epoch 88: averaged Loss: 653.140375\n",
      "2025-07-08 19:35: **********Val Epoch 88: average Loss: 2673.030924\n",
      "2025-07-08 19:35: Train Epoch 89: 0/37 Loss: 4731.603027\n",
      "2025-07-08 19:35: Train Epoch 89: 20/37 Loss: 637.875244\n",
      "2025-07-08 19:35: **********Train Epoch 89: averaged Loss: 1440.688036\n",
      "2025-07-08 19:35: **********Val Epoch 89: average Loss: 3023.487427\n",
      "2025-07-08 19:35: Train Epoch 90: 0/37 Loss: 986.279663\n",
      "2025-07-08 19:35: Train Epoch 90: 20/37 Loss: 370.827118\n",
      "2025-07-08 19:35: **********Train Epoch 90: averaged Loss: 538.729519\n",
      "2025-07-08 19:35: **********Val Epoch 90: average Loss: 2868.520182\n",
      "2025-07-08 19:35: Train Epoch 91: 0/37 Loss: 212.899780\n",
      "2025-07-08 19:35: Train Epoch 91: 20/37 Loss: 666.837158\n",
      "2025-07-08 19:35: **********Train Epoch 91: averaged Loss: 455.431103\n",
      "2025-07-08 19:35: **********Val Epoch 91: average Loss: 2290.449626\n",
      "2025-07-08 19:35: Train Epoch 92: 0/37 Loss: 1209.553955\n",
      "2025-07-08 19:35: Train Epoch 92: 20/37 Loss: 862.138428\n",
      "2025-07-08 19:35: **********Train Epoch 92: averaged Loss: 651.242518\n",
      "2025-07-08 19:35: **********Val Epoch 92: average Loss: 3392.662679\n",
      "2025-07-08 19:35: Train Epoch 93: 0/37 Loss: 1051.696045\n",
      "2025-07-08 19:35: Train Epoch 93: 20/37 Loss: 134.419342\n",
      "2025-07-08 19:35: **********Train Epoch 93: averaged Loss: 508.161961\n",
      "2025-07-08 19:35: **********Val Epoch 93: average Loss: 2280.865479\n",
      "2025-07-08 19:35: Train Epoch 94: 0/37 Loss: 2693.755859\n",
      "2025-07-08 19:35: Train Epoch 94: 20/37 Loss: 418.843506\n",
      "2025-07-08 19:35: **********Train Epoch 94: averaged Loss: 682.897011\n",
      "2025-07-08 19:35: **********Val Epoch 94: average Loss: 3320.204997\n",
      "2025-07-08 19:35: Train Epoch 95: 0/37 Loss: 1034.562256\n",
      "2025-07-08 19:35: Train Epoch 95: 20/37 Loss: 473.155457\n",
      "2025-07-08 19:35: **********Train Epoch 95: averaged Loss: 500.090702\n",
      "2025-07-08 19:35: **********Val Epoch 95: average Loss: 3391.324788\n",
      "2025-07-08 19:35: Train Epoch 96: 0/37 Loss: 1595.521240\n",
      "2025-07-08 19:35: Train Epoch 96: 20/37 Loss: 404.231567\n",
      "2025-07-08 19:35: **********Train Epoch 96: averaged Loss: 506.171006\n",
      "2025-07-08 19:35: **********Val Epoch 96: average Loss: 3267.221761\n",
      "2025-07-08 19:35: Train Epoch 97: 0/37 Loss: 3211.914551\n",
      "2025-07-08 19:35: Train Epoch 97: 20/37 Loss: 843.944214\n",
      "2025-07-08 19:35: **********Train Epoch 97: averaged Loss: 862.407552\n",
      "2025-07-08 19:35: **********Val Epoch 97: average Loss: 2883.613403\n",
      "2025-07-08 19:35: Train Epoch 98: 0/37 Loss: 1211.111816\n",
      "2025-07-08 19:35: Train Epoch 98: 20/37 Loss: 282.277161\n",
      "2025-07-08 19:35: **********Train Epoch 98: averaged Loss: 598.178428\n",
      "2025-07-08 19:35: **********Val Epoch 98: average Loss: 3480.272705\n",
      "2025-07-08 19:35: Train Epoch 99: 0/37 Loss: 2620.830078\n",
      "2025-07-08 19:35: Train Epoch 99: 20/37 Loss: 708.161499\n",
      "2025-07-08 19:35: **********Train Epoch 99: averaged Loss: 754.152754\n",
      "2025-07-08 19:35: **********Val Epoch 99: average Loss: 2627.591960\n",
      "2025-07-08 19:35: Train Epoch 100: 0/37 Loss: 445.675964\n",
      "2025-07-08 19:35: Train Epoch 100: 20/37 Loss: 1124.875977\n",
      "2025-07-08 19:35: **********Train Epoch 100: averaged Loss: 491.783196\n",
      "2025-07-08 19:35: **********Val Epoch 100: average Loss: 3498.423177\n",
      "2025-07-08 19:35: Train Epoch 101: 0/37 Loss: 2527.133301\n",
      "2025-07-08 19:35: Train Epoch 101: 20/37 Loss: 548.043213\n",
      "2025-07-08 19:35: **********Train Epoch 101: averaged Loss: 803.366963\n",
      "2025-07-08 19:35: **********Val Epoch 101: average Loss: 3823.293376\n",
      "2025-07-08 19:35: Train Epoch 102: 0/37 Loss: 587.317505\n",
      "2025-07-08 19:35: Train Epoch 102: 20/37 Loss: 680.948608\n",
      "2025-07-08 19:35: **********Train Epoch 102: averaged Loss: 488.150757\n",
      "2025-07-08 19:35: **********Val Epoch 102: average Loss: 3163.560710\n",
      "2025-07-08 19:35: Train Epoch 103: 0/37 Loss: 2447.984375\n",
      "2025-07-08 19:35: Train Epoch 103: 20/37 Loss: 142.571228\n",
      "2025-07-08 19:35: **********Train Epoch 103: averaged Loss: 639.031761\n",
      "2025-07-08 19:35: **********Val Epoch 103: average Loss: 2512.432861\n",
      "2025-07-08 19:35: Train Epoch 104: 0/37 Loss: 3219.765137\n",
      "2025-07-08 19:36: Train Epoch 104: 20/37 Loss: 306.056763\n",
      "2025-07-08 19:36: **********Train Epoch 104: averaged Loss: 861.672593\n",
      "2025-07-08 19:36: **********Val Epoch 104: average Loss: 3460.106445\n",
      "2025-07-08 19:36: Train Epoch 105: 0/37 Loss: 3263.127686\n",
      "2025-07-08 19:36: Train Epoch 105: 20/37 Loss: 408.065186\n",
      "2025-07-08 19:36: **********Train Epoch 105: averaged Loss: 707.148440\n",
      "2025-07-08 19:36: **********Val Epoch 105: average Loss: 2374.135742\n",
      "2025-07-08 19:36: Train Epoch 106: 0/37 Loss: 2005.339844\n",
      "2025-07-08 19:36: Train Epoch 106: 20/37 Loss: 427.041504\n",
      "2025-07-08 19:36: **********Train Epoch 106: averaged Loss: 630.749773\n",
      "2025-07-08 19:36: **********Val Epoch 106: average Loss: 1874.618103\n",
      "2025-07-08 19:36: Train Epoch 107: 0/37 Loss: 1750.219727\n",
      "2025-07-08 19:36: Train Epoch 107: 20/37 Loss: 1134.727417\n",
      "2025-07-08 19:36: **********Train Epoch 107: averaged Loss: 881.749320\n",
      "2025-07-08 19:36: **********Val Epoch 107: average Loss: 1743.660116\n",
      "2025-07-08 19:36: Train Epoch 108: 0/37 Loss: 2803.565430\n",
      "2025-07-08 19:36: Train Epoch 108: 20/37 Loss: 318.986023\n",
      "2025-07-08 19:36: **********Train Epoch 108: averaged Loss: 657.970218\n",
      "2025-07-08 19:36: **********Val Epoch 108: average Loss: 2728.592041\n",
      "2025-07-08 19:36: Train Epoch 109: 0/37 Loss: 3892.256104\n",
      "2025-07-08 19:36: Train Epoch 109: 20/37 Loss: 328.539124\n",
      "2025-07-08 19:36: **********Train Epoch 109: averaged Loss: 911.466735\n",
      "2025-07-08 19:36: **********Val Epoch 109: average Loss: 4423.520671\n",
      "2025-07-08 19:36: Train Epoch 110: 0/37 Loss: 4345.986328\n",
      "2025-07-08 19:36: Train Epoch 110: 20/37 Loss: 538.433960\n",
      "2025-07-08 19:36: **********Train Epoch 110: averaged Loss: 1534.857989\n",
      "2025-07-08 19:36: **********Val Epoch 110: average Loss: 3165.838460\n",
      "2025-07-08 19:36: Train Epoch 111: 0/37 Loss: 2196.628906\n",
      "2025-07-08 19:36: Train Epoch 111: 20/37 Loss: 293.365021\n",
      "2025-07-08 19:36: **********Train Epoch 111: averaged Loss: 956.694087\n",
      "2025-07-08 19:36: **********Val Epoch 111: average Loss: 1158.568400\n",
      "2025-07-08 19:36: Train Epoch 112: 0/37 Loss: 1671.214722\n",
      "2025-07-08 19:36: Train Epoch 112: 20/37 Loss: 708.112549\n",
      "2025-07-08 19:36: **********Train Epoch 112: averaged Loss: 505.191161\n",
      "2025-07-08 19:36: **********Val Epoch 112: average Loss: 3540.957682\n",
      "2025-07-08 19:36: Train Epoch 113: 0/37 Loss: 3627.635254\n",
      "2025-07-08 19:36: Train Epoch 113: 20/37 Loss: 1015.304993\n",
      "2025-07-08 19:36: **********Train Epoch 113: averaged Loss: 984.922698\n",
      "2025-07-08 19:36: **********Val Epoch 113: average Loss: 2389.127523\n",
      "2025-07-08 19:36: Train Epoch 114: 0/37 Loss: 2435.210449\n",
      "2025-07-08 19:36: Train Epoch 114: 20/37 Loss: 947.952148\n",
      "2025-07-08 19:36: **********Train Epoch 114: averaged Loss: 864.623238\n",
      "2025-07-08 19:36: **********Val Epoch 114: average Loss: 3552.480387\n",
      "2025-07-08 19:36: Train Epoch 115: 0/37 Loss: 3484.578369\n",
      "2025-07-08 19:36: Train Epoch 115: 20/37 Loss: 694.733032\n",
      "2025-07-08 19:36: **********Train Epoch 115: averaged Loss: 1105.076769\n",
      "2025-07-08 19:36: **********Val Epoch 115: average Loss: 1577.590149\n",
      "2025-07-08 19:36: Train Epoch 116: 0/37 Loss: 776.377563\n",
      "2025-07-08 19:36: Train Epoch 116: 20/37 Loss: 248.911240\n",
      "2025-07-08 19:36: **********Train Epoch 116: averaged Loss: 514.075225\n",
      "2025-07-08 19:36: **********Val Epoch 116: average Loss: 2295.389974\n",
      "2025-07-08 19:36: Train Epoch 117: 0/37 Loss: 815.206482\n",
      "2025-07-08 19:36: Train Epoch 117: 20/37 Loss: 315.689636\n",
      "2025-07-08 19:36: **********Train Epoch 117: averaged Loss: 627.326580\n",
      "2025-07-08 19:36: **********Val Epoch 117: average Loss: 3597.326986\n",
      "2025-07-08 19:36: Train Epoch 118: 0/37 Loss: 3406.676514\n",
      "2025-07-08 19:36: Train Epoch 118: 20/37 Loss: 222.982590\n",
      "2025-07-08 19:36: **********Train Epoch 118: averaged Loss: 803.455128\n",
      "2025-07-08 19:36: **********Val Epoch 118: average Loss: 1997.500854\n",
      "2025-07-08 19:36: Train Epoch 119: 0/37 Loss: 4496.958496\n",
      "2025-07-08 19:36: Train Epoch 119: 20/37 Loss: 262.961792\n",
      "2025-07-08 19:36: **********Train Epoch 119: averaged Loss: 1105.263631\n",
      "2025-07-08 19:36: **********Val Epoch 119: average Loss: 3913.490885\n",
      "2025-07-08 19:36: Train Epoch 120: 0/37 Loss: 4073.447266\n",
      "2025-07-08 19:36: Train Epoch 120: 20/37 Loss: 1225.250610\n",
      "2025-07-08 19:36: **********Train Epoch 120: averaged Loss: 1056.170441\n",
      "2025-07-08 19:36: **********Val Epoch 120: average Loss: 2106.370524\n",
      "2025-07-08 19:36: Train Epoch 121: 0/37 Loss: 252.323807\n",
      "2025-07-08 19:36: Train Epoch 121: 20/37 Loss: 490.202209\n",
      "2025-07-08 19:36: **********Train Epoch 121: averaged Loss: 349.064482\n",
      "2025-07-08 19:36: **********Val Epoch 121: average Loss: 2661.137614\n",
      "2025-07-08 19:36: Train Epoch 122: 0/37 Loss: 1870.262695\n",
      "2025-07-08 19:36: Train Epoch 122: 20/37 Loss: 367.629028\n",
      "2025-07-08 19:36: **********Train Epoch 122: averaged Loss: 561.507252\n",
      "2025-07-08 19:36: **********Val Epoch 122: average Loss: 1433.171305\n",
      "2025-07-08 19:36: Train Epoch 123: 0/37 Loss: 173.169739\n",
      "2025-07-08 19:36: Train Epoch 123: 20/37 Loss: 598.996216\n",
      "2025-07-08 19:36: **********Train Epoch 123: averaged Loss: 435.148358\n",
      "2025-07-08 19:36: **********Val Epoch 123: average Loss: 3379.594645\n",
      "2025-07-08 19:36: Train Epoch 124: 0/37 Loss: 2587.193604\n",
      "2025-07-08 19:36: Train Epoch 124: 20/37 Loss: 753.577271\n",
      "2025-07-08 19:36: **********Train Epoch 124: averaged Loss: 828.112037\n",
      "2025-07-08 19:36: **********Val Epoch 124: average Loss: 2609.648397\n",
      "2025-07-08 19:36: Train Epoch 125: 0/37 Loss: 144.420120\n",
      "2025-07-08 19:36: Train Epoch 125: 20/37 Loss: 175.933670\n",
      "2025-07-08 19:36: **********Train Epoch 125: averaged Loss: 417.912618\n",
      "2025-07-08 19:36: **********Val Epoch 125: average Loss: 3034.978271\n",
      "2025-07-08 19:36: Train Epoch 126: 0/37 Loss: 837.593079\n",
      "2025-07-08 19:36: Train Epoch 126: 20/37 Loss: 123.852432\n",
      "2025-07-08 19:36: **********Train Epoch 126: averaged Loss: 427.166981\n",
      "2025-07-08 19:36: **********Val Epoch 126: average Loss: 2050.195231\n",
      "2025-07-08 19:36: Train Epoch 127: 0/37 Loss: 3532.756836\n",
      "2025-07-08 19:36: Train Epoch 127: 20/37 Loss: 396.049622\n",
      "2025-07-08 19:36: **********Train Epoch 127: averaged Loss: 912.975517\n",
      "2025-07-08 19:36: **********Val Epoch 127: average Loss: 3374.294108\n",
      "2025-07-08 19:36: Train Epoch 128: 0/37 Loss: 2946.891846\n",
      "2025-07-08 19:36: Train Epoch 128: 20/37 Loss: 273.624512\n",
      "2025-07-08 19:36: **********Train Epoch 128: averaged Loss: 726.695150\n",
      "2025-07-08 19:36: **********Val Epoch 128: average Loss: 2157.772135\n",
      "2025-07-08 19:36: Train Epoch 129: 0/37 Loss: 4181.588867\n",
      "2025-07-08 19:36: Train Epoch 129: 20/37 Loss: 353.064606\n",
      "2025-07-08 19:36: **********Train Epoch 129: averaged Loss: 1099.901817\n",
      "2025-07-08 19:36: **********Val Epoch 129: average Loss: 3449.287842\n",
      "2025-07-08 19:36: Train Epoch 130: 0/37 Loss: 2686.329834\n",
      "2025-07-08 19:36: Train Epoch 130: 20/37 Loss: 208.602966\n",
      "2025-07-08 19:36: **********Train Epoch 130: averaged Loss: 665.749155\n",
      "2025-07-08 19:36: **********Val Epoch 130: average Loss: 2812.401042\n",
      "2025-07-08 19:36: Train Epoch 131: 0/37 Loss: 463.423828\n",
      "2025-07-08 19:36: Train Epoch 131: 20/37 Loss: 213.457855\n",
      "2025-07-08 19:36: **********Train Epoch 131: averaged Loss: 345.661757\n",
      "2025-07-08 19:36: **********Val Epoch 131: average Loss: 2194.600993\n",
      "2025-07-08 19:36: Train Epoch 132: 0/37 Loss: 1068.029297\n",
      "2025-07-08 19:36: Train Epoch 132: 20/37 Loss: 530.061218\n",
      "2025-07-08 19:36: **********Train Epoch 132: averaged Loss: 686.581051\n",
      "2025-07-08 19:36: **********Val Epoch 132: average Loss: 2280.718791\n",
      "2025-07-08 19:36: Train Epoch 133: 0/37 Loss: 3260.325684\n",
      "2025-07-08 19:36: Train Epoch 133: 20/37 Loss: 595.811768\n",
      "2025-07-08 19:36: **********Train Epoch 133: averaged Loss: 1276.546939\n",
      "2025-07-08 19:36: **********Val Epoch 133: average Loss: 2242.949870\n",
      "2025-07-08 19:36: Train Epoch 134: 0/37 Loss: 4171.639648\n",
      "2025-07-08 19:36: Train Epoch 134: 20/37 Loss: 266.416931\n",
      "2025-07-08 19:36: **********Train Epoch 134: averaged Loss: 974.963210\n",
      "2025-07-08 19:36: **********Val Epoch 134: average Loss: 2067.113688\n",
      "2025-07-08 19:36: Train Epoch 135: 0/37 Loss: 3753.406982\n",
      "2025-07-08 19:36: Train Epoch 135: 20/37 Loss: 308.858215\n",
      "2025-07-08 19:36: **********Train Epoch 135: averaged Loss: 831.102631\n",
      "2025-07-08 19:36: **********Val Epoch 135: average Loss: 1113.258443\n",
      "2025-07-08 19:36: Train Epoch 136: 0/37 Loss: 1153.909058\n",
      "2025-07-08 19:36: Train Epoch 136: 20/37 Loss: 1095.224487\n",
      "2025-07-08 19:36: **********Train Epoch 136: averaged Loss: 670.246661\n",
      "2025-07-08 19:36: **********Val Epoch 136: average Loss: 2378.472860\n",
      "2025-07-08 19:36: Train Epoch 137: 0/37 Loss: 4950.851074\n",
      "2025-07-08 19:36: Train Epoch 137: 20/37 Loss: 193.813522\n",
      "2025-07-08 19:37: **********Train Epoch 137: averaged Loss: 1329.855657\n",
      "2025-07-08 19:37: **********Val Epoch 137: average Loss: 2319.777547\n",
      "2025-07-08 19:37: Train Epoch 138: 0/37 Loss: 1512.861694\n",
      "2025-07-08 19:37: Train Epoch 138: 20/37 Loss: 308.254883\n",
      "2025-07-08 19:37: **********Train Epoch 138: averaged Loss: 486.373249\n",
      "2025-07-08 19:37: **********Val Epoch 138: average Loss: 3073.170817\n",
      "2025-07-08 19:37: Train Epoch 139: 0/37 Loss: 578.800659\n",
      "2025-07-08 19:37: Train Epoch 139: 20/37 Loss: 359.124939\n",
      "2025-07-08 19:37: **********Train Epoch 139: averaged Loss: 398.189443\n",
      "2025-07-08 19:37: **********Val Epoch 139: average Loss: 2677.253337\n",
      "2025-07-08 19:37: Train Epoch 140: 0/37 Loss: 733.965149\n",
      "2025-07-08 19:37: Train Epoch 140: 20/37 Loss: 153.045761\n",
      "2025-07-08 19:37: **********Train Epoch 140: averaged Loss: 434.604467\n",
      "2025-07-08 19:37: **********Val Epoch 140: average Loss: 1700.436971\n",
      "2025-07-08 19:37: Train Epoch 141: 0/37 Loss: 586.055176\n",
      "2025-07-08 19:37: Train Epoch 141: 20/37 Loss: 552.935303\n",
      "2025-07-08 19:37: **********Train Epoch 141: averaged Loss: 393.817399\n",
      "2025-07-08 19:37: **********Val Epoch 141: average Loss: 2196.160360\n",
      "2025-07-08 19:37: Train Epoch 142: 0/37 Loss: 868.782410\n",
      "2025-07-08 19:37: Train Epoch 142: 20/37 Loss: 313.149658\n",
      "2025-07-08 19:37: **********Train Epoch 142: averaged Loss: 452.292399\n",
      "2025-07-08 19:37: **********Val Epoch 142: average Loss: 2426.420451\n",
      "2025-07-08 19:37: Train Epoch 143: 0/37 Loss: 1560.063232\n",
      "2025-07-08 19:37: Train Epoch 143: 20/37 Loss: 215.252823\n",
      "2025-07-08 19:37: **********Train Epoch 143: averaged Loss: 447.634872\n",
      "2025-07-08 19:37: **********Val Epoch 143: average Loss: 1771.741984\n",
      "2025-07-08 19:37: Train Epoch 144: 0/37 Loss: 603.398865\n",
      "2025-07-08 19:37: Train Epoch 144: 20/37 Loss: 688.994995\n",
      "2025-07-08 19:37: **********Train Epoch 144: averaged Loss: 355.626556\n",
      "2025-07-08 19:37: **********Val Epoch 144: average Loss: 1552.084188\n",
      "2025-07-08 19:37: Train Epoch 145: 0/37 Loss: 338.885773\n",
      "2025-07-08 19:37: Train Epoch 145: 20/37 Loss: 612.994934\n",
      "2025-07-08 19:37: **********Train Epoch 145: averaged Loss: 413.462914\n",
      "2025-07-08 19:37: **********Val Epoch 145: average Loss: 1137.719930\n",
      "2025-07-08 19:37: Train Epoch 146: 0/37 Loss: 1284.888184\n",
      "2025-07-08 19:37: Train Epoch 146: 20/37 Loss: 334.955750\n",
      "2025-07-08 19:37: **********Train Epoch 146: averaged Loss: 926.025980\n",
      "2025-07-08 19:37: **********Val Epoch 146: average Loss: 2312.449544\n",
      "2025-07-08 19:37: Train Epoch 147: 0/37 Loss: 5038.294922\n",
      "2025-07-08 19:37: Train Epoch 147: 20/37 Loss: 496.689148\n",
      "2025-07-08 19:37: **********Train Epoch 147: averaged Loss: 2370.273899\n",
      "2025-07-08 19:37: **********Val Epoch 147: average Loss: 2464.556315\n",
      "2025-07-08 19:37: Train Epoch 148: 0/37 Loss: 5239.576172\n",
      "2025-07-08 19:37: Train Epoch 148: 20/37 Loss: 366.258423\n",
      "2025-07-08 19:37: **********Train Epoch 148: averaged Loss: 1041.595758\n",
      "2025-07-08 19:37: **********Val Epoch 148: average Loss: 1670.526042\n",
      "2025-07-08 19:37: Train Epoch 149: 0/37 Loss: 5698.126465\n",
      "2025-07-08 19:37: Train Epoch 149: 20/37 Loss: 1105.283936\n",
      "2025-07-08 19:37: **********Train Epoch 149: averaged Loss: 1396.144998\n",
      "2025-07-08 19:37: **********Val Epoch 149: average Loss: 1672.165527\n",
      "2025-07-08 19:37: Train Epoch 150: 0/37 Loss: 799.837036\n",
      "2025-07-08 19:37: Train Epoch 150: 20/37 Loss: 329.937805\n",
      "2025-07-08 19:37: **********Train Epoch 150: averaged Loss: 801.433769\n",
      "2025-07-08 19:37: **********Val Epoch 150: average Loss: 2658.234945\n",
      "2025-07-08 19:37: Train Epoch 151: 0/37 Loss: 3247.586426\n",
      "2025-07-08 19:37: Train Epoch 151: 20/37 Loss: 1160.386597\n",
      "2025-07-08 19:37: **********Train Epoch 151: averaged Loss: 1386.819282\n",
      "2025-07-08 19:37: **********Val Epoch 151: average Loss: 1019.656982\n",
      "2025-07-08 19:37: Train Epoch 152: 0/37 Loss: 4342.440918\n",
      "2025-07-08 19:37: Train Epoch 152: 20/37 Loss: 268.888000\n",
      "2025-07-08 19:37: **********Train Epoch 152: averaged Loss: 962.745100\n",
      "2025-07-08 19:37: **********Val Epoch 152: average Loss: 2380.138306\n",
      "2025-07-08 19:37: Train Epoch 153: 0/37 Loss: 5760.710938\n",
      "2025-07-08 19:37: Train Epoch 153: 20/37 Loss: 778.014160\n",
      "2025-07-08 19:37: **********Train Epoch 153: averaged Loss: 1436.020427\n",
      "2025-07-08 19:37: **********Val Epoch 153: average Loss: 793.180898\n",
      "2025-07-08 19:37: Train Epoch 154: 0/37 Loss: 2900.337646\n",
      "2025-07-08 19:37: Train Epoch 154: 20/37 Loss: 298.658081\n",
      "2025-07-08 19:37: **********Train Epoch 154: averaged Loss: 1222.732255\n",
      "2025-07-08 19:37: **********Val Epoch 154: average Loss: 3863.901204\n",
      "2025-07-08 19:37: Train Epoch 155: 0/37 Loss: 5390.764648\n",
      "2025-07-08 19:37: Train Epoch 155: 20/37 Loss: 436.735535\n",
      "2025-07-08 19:37: **********Train Epoch 155: averaged Loss: 1628.913673\n",
      "2025-07-08 19:37: **********Val Epoch 155: average Loss: 2581.254639\n",
      "2025-07-08 19:37: Train Epoch 156: 0/37 Loss: 7094.756836\n",
      "2025-07-08 19:37: Train Epoch 156: 20/37 Loss: 234.733170\n",
      "2025-07-08 19:37: **********Train Epoch 156: averaged Loss: 1270.493287\n",
      "2025-07-08 19:37: **********Val Epoch 156: average Loss: 2122.949056\n",
      "2025-07-08 19:37: Train Epoch 157: 0/37 Loss: 5307.035156\n",
      "2025-07-08 19:37: Train Epoch 157: 20/37 Loss: 731.871094\n",
      "2025-07-08 19:37: **********Train Epoch 157: averaged Loss: 1288.096184\n",
      "2025-07-08 19:37: **********Val Epoch 157: average Loss: 876.082072\n",
      "2025-07-08 19:37: Train Epoch 158: 0/37 Loss: 1319.431885\n",
      "2025-07-08 19:37: Train Epoch 158: 20/37 Loss: 1028.705322\n",
      "2025-07-08 19:37: **********Train Epoch 158: averaged Loss: 741.861381\n",
      "2025-07-08 19:37: **********Val Epoch 158: average Loss: 2358.342407\n",
      "2025-07-08 19:37: Train Epoch 159: 0/37 Loss: 5897.281250\n",
      "2025-07-08 19:37: Train Epoch 159: 20/37 Loss: 594.291199\n",
      "2025-07-08 19:37: **********Train Epoch 159: averaged Loss: 1957.897456\n",
      "2025-07-08 19:37: **********Val Epoch 159: average Loss: 2923.569173\n",
      "2025-07-08 19:37: Train Epoch 160: 0/37 Loss: 999.033630\n",
      "2025-07-08 19:37: Train Epoch 160: 20/37 Loss: 850.674133\n",
      "2025-07-08 19:37: **********Train Epoch 160: averaged Loss: 677.484966\n",
      "2025-07-08 19:37: **********Val Epoch 160: average Loss: 1388.931885\n",
      "2025-07-08 19:37: Train Epoch 161: 0/37 Loss: 3976.930664\n",
      "2025-07-08 19:37: Train Epoch 161: 20/37 Loss: 451.257141\n",
      "2025-07-08 19:37: **********Train Epoch 161: averaged Loss: 1015.973789\n",
      "2025-07-08 19:37: **********Val Epoch 161: average Loss: 595.894877\n",
      "2025-07-08 19:37: *********************************Current best model saved!\n",
      "2025-07-08 19:37: Train Epoch 162: 0/37 Loss: 2265.711670\n",
      "2025-07-08 19:37: Train Epoch 162: 20/37 Loss: 689.209351\n",
      "2025-07-08 19:37: **********Train Epoch 162: averaged Loss: 712.263571\n",
      "2025-07-08 19:37: **********Val Epoch 162: average Loss: 2720.537516\n",
      "2025-07-08 19:37: Train Epoch 163: 0/37 Loss: 6165.639648\n",
      "2025-07-08 19:37: Train Epoch 163: 20/37 Loss: 372.841309\n",
      "2025-07-08 19:37: **********Train Epoch 163: averaged Loss: 1994.888512\n",
      "2025-07-08 19:37: **********Val Epoch 163: average Loss: 5082.393229\n",
      "2025-07-08 19:37: Train Epoch 164: 0/37 Loss: 1400.836182\n",
      "2025-07-08 19:37: Train Epoch 164: 20/37 Loss: 377.653870\n",
      "2025-07-08 19:37: **********Train Epoch 164: averaged Loss: 641.280697\n",
      "2025-07-08 19:37: **********Val Epoch 164: average Loss: 2135.153035\n",
      "2025-07-08 19:37: Train Epoch 165: 0/37 Loss: 3117.482178\n",
      "2025-07-08 19:37: Train Epoch 165: 20/37 Loss: 399.634583\n",
      "2025-07-08 19:37: **********Train Epoch 165: averaged Loss: 799.564791\n",
      "2025-07-08 19:37: **********Val Epoch 165: average Loss: 1568.892375\n",
      "2025-07-08 19:37: Train Epoch 166: 0/37 Loss: 1356.937866\n",
      "2025-07-08 19:37: Train Epoch 166: 20/37 Loss: 746.023499\n",
      "2025-07-08 19:37: **********Train Epoch 166: averaged Loss: 601.337415\n",
      "2025-07-08 19:37: **********Val Epoch 166: average Loss: 3352.845378\n",
      "2025-07-08 19:37: Train Epoch 167: 0/37 Loss: 4568.593750\n",
      "2025-07-08 19:37: Train Epoch 167: 20/37 Loss: 762.142029\n",
      "2025-07-08 19:37: **********Train Epoch 167: averaged Loss: 1872.212788\n",
      "2025-07-08 19:37: **********Val Epoch 167: average Loss: 4661.446289\n",
      "2025-07-08 19:37: Train Epoch 168: 0/37 Loss: 409.299683\n",
      "2025-07-08 19:37: Train Epoch 168: 20/37 Loss: 588.510498\n",
      "2025-07-08 19:37: **********Train Epoch 168: averaged Loss: 1128.608765\n",
      "2025-07-08 19:37: **********Val Epoch 168: average Loss: 5240.818522\n",
      "2025-07-08 19:37: Train Epoch 169: 0/37 Loss: 4945.124512\n",
      "2025-07-08 19:37: Train Epoch 169: 20/37 Loss: 622.428650\n",
      "2025-07-08 19:37: **********Train Epoch 169: averaged Loss: 1632.422906\n",
      "2025-07-08 19:37: **********Val Epoch 169: average Loss: 3087.610840\n",
      "2025-07-08 19:37: Train Epoch 170: 0/37 Loss: 288.090424\n",
      "2025-07-08 19:37: Train Epoch 170: 20/37 Loss: 412.151489\n",
      "2025-07-08 19:38: **********Train Epoch 170: averaged Loss: 497.688519\n",
      "2025-07-08 19:38: **********Val Epoch 170: average Loss: 2158.898112\n",
      "2025-07-08 19:38: Train Epoch 171: 0/37 Loss: 3878.413574\n",
      "2025-07-08 19:38: Train Epoch 171: 20/37 Loss: 705.616821\n",
      "2025-07-08 19:38: **********Train Epoch 171: averaged Loss: 1107.369913\n",
      "2025-07-08 19:38: **********Val Epoch 171: average Loss: 1901.254924\n",
      "2025-07-08 19:38: Train Epoch 172: 0/37 Loss: 1597.399292\n",
      "2025-07-08 19:38: Train Epoch 172: 20/37 Loss: 203.554672\n",
      "2025-07-08 19:38: **********Train Epoch 172: averaged Loss: 493.141555\n",
      "2025-07-08 19:38: **********Val Epoch 172: average Loss: 1679.806356\n",
      "2025-07-08 19:38: Train Epoch 173: 0/37 Loss: 2814.236328\n",
      "2025-07-08 19:38: Train Epoch 173: 20/37 Loss: 834.076599\n",
      "2025-07-08 19:38: **********Train Epoch 173: averaged Loss: 899.534157\n",
      "2025-07-08 19:38: **********Val Epoch 173: average Loss: 1875.712992\n",
      "2025-07-08 19:38: Train Epoch 174: 0/37 Loss: 3300.097656\n",
      "2025-07-08 19:38: Train Epoch 174: 20/37 Loss: 652.281738\n",
      "2025-07-08 19:38: **********Train Epoch 174: averaged Loss: 1010.670879\n",
      "2025-07-08 19:38: **********Val Epoch 174: average Loss: 1435.643880\n",
      "2025-07-08 19:38: Train Epoch 175: 0/37 Loss: 2014.983887\n",
      "2025-07-08 19:38: Train Epoch 175: 20/37 Loss: 1079.577637\n",
      "2025-07-08 19:38: **********Train Epoch 175: averaged Loss: 738.459287\n",
      "2025-07-08 19:38: **********Val Epoch 175: average Loss: 2018.346354\n",
      "2025-07-08 19:38: Train Epoch 176: 0/37 Loss: 2855.645508\n",
      "2025-07-08 19:38: Train Epoch 176: 20/37 Loss: 837.652649\n",
      "2025-07-08 19:38: **********Train Epoch 176: averaged Loss: 904.854222\n",
      "2025-07-08 19:38: **********Val Epoch 176: average Loss: 1918.493815\n",
      "2025-07-08 19:38: Train Epoch 177: 0/37 Loss: 865.088623\n",
      "2025-07-08 19:38: Train Epoch 177: 20/37 Loss: 174.787674\n",
      "2025-07-08 19:38: **********Train Epoch 177: averaged Loss: 331.823708\n",
      "2025-07-08 19:38: **********Val Epoch 177: average Loss: 2305.606405\n",
      "2025-07-08 19:38: Train Epoch 178: 0/37 Loss: 1372.908569\n",
      "2025-07-08 19:38: Train Epoch 178: 20/37 Loss: 491.314636\n",
      "2025-07-08 19:38: **********Train Epoch 178: averaged Loss: 429.088604\n",
      "2025-07-08 19:38: **********Val Epoch 178: average Loss: 1796.100850\n",
      "2025-07-08 19:38: Train Epoch 179: 0/37 Loss: 1741.217651\n",
      "2025-07-08 19:38: Train Epoch 179: 20/37 Loss: 591.977661\n",
      "2025-07-08 19:38: **********Train Epoch 179: averaged Loss: 475.646114\n",
      "2025-07-08 19:38: **********Val Epoch 179: average Loss: 1432.402791\n",
      "2025-07-08 19:38: Train Epoch 180: 0/37 Loss: 1714.044189\n",
      "2025-07-08 19:38: Train Epoch 180: 20/37 Loss: 653.258179\n",
      "2025-07-08 19:38: **********Train Epoch 180: averaged Loss: 561.592629\n",
      "2025-07-08 19:38: **********Val Epoch 180: average Loss: 2215.426188\n",
      "2025-07-08 19:38: Train Epoch 181: 0/37 Loss: 3202.452637\n",
      "2025-07-08 19:38: Train Epoch 181: 20/37 Loss: 318.435547\n",
      "2025-07-08 19:38: **********Train Epoch 181: averaged Loss: 908.826103\n",
      "2025-07-08 19:38: **********Val Epoch 181: average Loss: 2722.001872\n",
      "2025-07-08 19:38: Train Epoch 182: 0/37 Loss: 3212.774414\n",
      "2025-07-08 19:38: Train Epoch 182: 20/37 Loss: 472.703430\n",
      "2025-07-08 19:38: **********Train Epoch 182: averaged Loss: 725.964250\n",
      "2025-07-08 19:38: **********Val Epoch 182: average Loss: 1285.257080\n",
      "2025-07-08 19:38: Train Epoch 183: 0/37 Loss: 2065.882324\n",
      "2025-07-08 19:38: Train Epoch 183: 20/37 Loss: 256.186035\n",
      "2025-07-08 19:38: **********Train Epoch 183: averaged Loss: 842.178443\n",
      "2025-07-08 19:38: **********Val Epoch 183: average Loss: 2431.754395\n",
      "2025-07-08 19:38: Train Epoch 184: 0/37 Loss: 3690.173340\n",
      "2025-07-08 19:38: Train Epoch 184: 20/37 Loss: 255.177307\n",
      "2025-07-08 19:38: **********Train Epoch 184: averaged Loss: 1265.143859\n",
      "2025-07-08 19:38: **********Val Epoch 184: average Loss: 3013.051758\n",
      "2025-07-08 19:38: Train Epoch 185: 0/37 Loss: 4386.216797\n",
      "2025-07-08 19:38: Train Epoch 185: 20/37 Loss: 246.066025\n",
      "2025-07-08 19:38: **********Train Epoch 185: averaged Loss: 1077.239567\n",
      "2025-07-08 19:38: **********Val Epoch 185: average Loss: 3077.433024\n",
      "2025-07-08 19:38: Train Epoch 186: 0/37 Loss: 3817.734375\n",
      "2025-07-08 19:38: Train Epoch 186: 20/37 Loss: 233.492905\n",
      "2025-07-08 19:38: **********Train Epoch 186: averaged Loss: 1079.489636\n",
      "2025-07-08 19:38: **********Val Epoch 186: average Loss: 1429.706950\n",
      "2025-07-08 19:38: Train Epoch 187: 0/37 Loss: 776.707764\n",
      "2025-07-08 19:38: Train Epoch 187: 20/37 Loss: 134.061096\n",
      "2025-07-08 19:38: **********Train Epoch 187: averaged Loss: 451.974075\n",
      "2025-07-08 19:38: **********Val Epoch 187: average Loss: 614.636770\n",
      "2025-07-08 19:38: Train Epoch 188: 0/37 Loss: 745.308716\n",
      "2025-07-08 19:38: Train Epoch 188: 20/37 Loss: 450.741547\n",
      "2025-07-08 19:38: **********Train Epoch 188: averaged Loss: 496.081968\n",
      "2025-07-08 19:38: **********Val Epoch 188: average Loss: 2249.871908\n",
      "2025-07-08 19:38: Train Epoch 189: 0/37 Loss: 2664.337402\n",
      "2025-07-08 19:38: Train Epoch 189: 20/37 Loss: 431.119568\n",
      "2025-07-08 19:38: **********Train Epoch 189: averaged Loss: 672.338560\n",
      "2025-07-08 19:38: **********Val Epoch 189: average Loss: 1512.098307\n",
      "2025-07-08 19:38: Train Epoch 190: 0/37 Loss: 2504.373047\n",
      "2025-07-08 19:38: Train Epoch 190: 20/37 Loss: 276.911194\n",
      "2025-07-08 19:38: **********Train Epoch 190: averaged Loss: 971.676807\n",
      "2025-07-08 19:38: **********Val Epoch 190: average Loss: 3651.145589\n",
      "2025-07-08 19:38: Train Epoch 191: 0/37 Loss: 4406.010742\n",
      "2025-07-08 19:38: Train Epoch 191: 20/37 Loss: 246.715546\n",
      "2025-07-08 19:38: **********Train Epoch 191: averaged Loss: 1869.911043\n",
      "2025-07-08 19:38: **********Val Epoch 191: average Loss: 3822.795085\n",
      "2025-07-08 19:38: Train Epoch 192: 0/37 Loss: 2075.000488\n",
      "2025-07-08 19:38: Train Epoch 192: 20/37 Loss: 783.255127\n",
      "2025-07-08 19:38: **********Train Epoch 192: averaged Loss: 1232.565803\n",
      "2025-07-08 19:38: **********Val Epoch 192: average Loss: 4135.081543\n",
      "2025-07-08 19:38: Train Epoch 193: 0/37 Loss: 5878.048828\n",
      "2025-07-08 19:38: Train Epoch 193: 20/37 Loss: 229.668839\n",
      "2025-07-08 19:38: **********Train Epoch 193: averaged Loss: 1399.168041\n",
      "2025-07-08 19:38: **********Val Epoch 193: average Loss: 736.445007\n",
      "2025-07-08 19:38: Train Epoch 194: 0/37 Loss: 3135.124512\n",
      "2025-07-08 19:38: Train Epoch 194: 20/37 Loss: 890.348389\n",
      "2025-07-08 19:38: **********Train Epoch 194: averaged Loss: 1131.565442\n",
      "2025-07-08 19:38: **********Val Epoch 194: average Loss: 2834.984497\n",
      "2025-07-08 19:38: Train Epoch 195: 0/37 Loss: 7367.057617\n",
      "2025-07-08 19:38: Train Epoch 195: 20/37 Loss: 409.170441\n",
      "2025-07-08 19:38: **********Train Epoch 195: averaged Loss: 1733.649891\n",
      "2025-07-08 19:38: **********Val Epoch 195: average Loss: 943.926666\n",
      "2025-07-08 19:38: Train Epoch 196: 0/37 Loss: 3682.477051\n",
      "2025-07-08 19:38: Train Epoch 196: 20/37 Loss: 280.347412\n",
      "2025-07-08 19:38: **********Train Epoch 196: averaged Loss: 935.671050\n",
      "2025-07-08 19:38: **********Val Epoch 196: average Loss: 3545.855713\n",
      "2025-07-08 19:38: Train Epoch 197: 0/37 Loss: 5456.817383\n",
      "2025-07-08 19:38: Train Epoch 197: 20/37 Loss: 144.122147\n",
      "2025-07-08 19:38: **********Train Epoch 197: averaged Loss: 1364.641272\n",
      "2025-07-08 19:38: **********Val Epoch 197: average Loss: 2150.743652\n",
      "2025-07-08 19:38: Train Epoch 198: 0/37 Loss: 1259.819214\n",
      "2025-07-08 19:38: Train Epoch 198: 20/37 Loss: 152.071808\n",
      "2025-07-08 19:38: **********Train Epoch 198: averaged Loss: 492.974091\n",
      "2025-07-08 19:38: **********Val Epoch 198: average Loss: 841.035055\n",
      "2025-07-08 19:38: Train Epoch 199: 0/37 Loss: 470.376648\n",
      "2025-07-08 19:38: Train Epoch 199: 20/37 Loss: 380.919495\n",
      "2025-07-08 19:38: **********Train Epoch 199: averaged Loss: 495.598086\n",
      "2025-07-08 19:38: **********Val Epoch 199: average Loss: 2262.288818\n",
      "2025-07-08 19:38: Train Epoch 200: 0/37 Loss: 3513.199219\n",
      "2025-07-08 19:38: Train Epoch 200: 20/37 Loss: 263.361145\n",
      "2025-07-08 19:38: **********Train Epoch 200: averaged Loss: 1086.767448\n",
      "2025-07-08 19:38: **********Val Epoch 200: average Loss: 2642.328410\n",
      "2025-07-08 19:38: Train Epoch 201: 0/37 Loss: 269.941284\n",
      "2025-07-08 19:38: Train Epoch 201: 20/37 Loss: 226.902908\n",
      "2025-07-08 19:38: **********Train Epoch 201: averaged Loss: 416.443439\n",
      "2025-07-08 19:38: **********Val Epoch 201: average Loss: 669.283061\n",
      "2025-07-08 19:38: Train Epoch 202: 0/37 Loss: 420.925903\n",
      "2025-07-08 19:38: Train Epoch 202: 20/37 Loss: 412.571594\n",
      "2025-07-08 19:38: **********Train Epoch 202: averaged Loss: 405.070861\n",
      "2025-07-08 19:38: **********Val Epoch 202: average Loss: 2489.899618\n",
      "2025-07-08 19:38: Train Epoch 203: 0/37 Loss: 3104.477051\n",
      "2025-07-08 19:38: Train Epoch 203: 20/37 Loss: 541.271484\n",
      "2025-07-08 19:38: **********Train Epoch 203: averaged Loss: 922.695087\n",
      "2025-07-08 19:38: **********Val Epoch 203: average Loss: 1086.342550\n",
      "2025-07-08 19:38: Train Epoch 204: 0/37 Loss: 904.347900\n",
      "2025-07-08 19:39: Train Epoch 204: 20/37 Loss: 226.837204\n",
      "2025-07-08 19:39: **********Train Epoch 204: averaged Loss: 417.699827\n",
      "2025-07-08 19:39: **********Val Epoch 204: average Loss: 2326.506104\n",
      "2025-07-08 19:39: Train Epoch 205: 0/37 Loss: 1663.734863\n",
      "2025-07-08 19:39: Train Epoch 205: 20/37 Loss: 173.533783\n",
      "2025-07-08 19:39: **********Train Epoch 205: averaged Loss: 445.388331\n",
      "2025-07-08 19:39: **********Val Epoch 205: average Loss: 1595.690999\n",
      "2025-07-08 19:39: Train Epoch 206: 0/37 Loss: 711.084778\n",
      "2025-07-08 19:39: Train Epoch 206: 20/37 Loss: 236.125137\n",
      "2025-07-08 19:39: **********Train Epoch 206: averaged Loss: 428.863719\n",
      "2025-07-08 19:39: **********Val Epoch 206: average Loss: 2416.069661\n",
      "2025-07-08 19:39: Train Epoch 207: 0/37 Loss: 2110.006592\n",
      "2025-07-08 19:39: Train Epoch 207: 20/37 Loss: 124.598038\n",
      "2025-07-08 19:39: **********Train Epoch 207: averaged Loss: 533.106182\n",
      "2025-07-08 19:39: **********Val Epoch 207: average Loss: 1463.186808\n",
      "2025-07-08 19:39: Train Epoch 208: 0/37 Loss: 2148.821289\n",
      "2025-07-08 19:39: Train Epoch 208: 20/37 Loss: 380.194885\n",
      "2025-07-08 19:39: **********Train Epoch 208: averaged Loss: 754.332593\n",
      "2025-07-08 19:39: **********Val Epoch 208: average Loss: 2519.460856\n",
      "2025-07-08 19:39: Train Epoch 209: 0/37 Loss: 2686.290039\n",
      "2025-07-08 19:39: Train Epoch 209: 20/37 Loss: 403.464722\n",
      "2025-07-08 19:39: **********Train Epoch 209: averaged Loss: 1187.608897\n",
      "2025-07-08 19:39: **********Val Epoch 209: average Loss: 2530.514119\n",
      "2025-07-08 19:39: Train Epoch 210: 0/37 Loss: 3672.449219\n",
      "2025-07-08 19:39: Train Epoch 210: 20/37 Loss: 364.762634\n",
      "2025-07-08 19:39: **********Train Epoch 210: averaged Loss: 1419.924665\n",
      "2025-07-08 19:39: **********Val Epoch 210: average Loss: 3289.311279\n",
      "2025-07-08 19:39: Train Epoch 211: 0/37 Loss: 5782.431641\n",
      "2025-07-08 19:39: Train Epoch 211: 20/37 Loss: 190.895660\n",
      "2025-07-08 19:39: **********Train Epoch 211: averaged Loss: 2132.845052\n",
      "2025-07-08 19:39: **********Val Epoch 211: average Loss: 4765.729655\n",
      "2025-07-08 19:39: Train Epoch 212: 0/37 Loss: 720.034424\n",
      "2025-07-08 19:39: Train Epoch 212: 20/37 Loss: 765.007141\n",
      "2025-07-08 19:39: **********Train Epoch 212: averaged Loss: 572.858586\n",
      "2025-07-08 19:39: **********Val Epoch 212: average Loss: 1834.619263\n",
      "2025-07-08 19:39: Train Epoch 213: 0/37 Loss: 2808.430908\n",
      "2025-07-08 19:39: Train Epoch 213: 20/37 Loss: 246.312592\n",
      "2025-07-08 19:39: **********Train Epoch 213: averaged Loss: 661.308115\n",
      "2025-07-08 19:39: **********Val Epoch 213: average Loss: 1057.694641\n",
      "2025-07-08 19:39: Train Epoch 214: 0/37 Loss: 1994.222534\n",
      "2025-07-08 19:39: Train Epoch 214: 20/37 Loss: 691.023560\n",
      "2025-07-08 19:39: **********Train Epoch 214: averaged Loss: 774.206189\n",
      "2025-07-08 19:39: **********Val Epoch 214: average Loss: 3171.376302\n",
      "2025-07-08 19:39: Train Epoch 215: 0/37 Loss: 5225.746094\n",
      "2025-07-08 19:39: Train Epoch 215: 20/37 Loss: 468.608887\n",
      "2025-07-08 19:39: **********Train Epoch 215: averaged Loss: 1906.447644\n",
      "2025-07-08 19:39: **********Val Epoch 215: average Loss: 3582.109049\n",
      "2025-07-08 19:39: Train Epoch 216: 0/37 Loss: 277.817688\n",
      "2025-07-08 19:39: Train Epoch 216: 20/37 Loss: 646.252808\n",
      "2025-07-08 19:39: **********Train Epoch 216: averaged Loss: 1080.239189\n",
      "2025-07-08 19:39: **********Val Epoch 216: average Loss: 4455.261882\n",
      "2025-07-08 19:39: Train Epoch 217: 0/37 Loss: 5456.657227\n",
      "2025-07-08 19:39: Train Epoch 217: 20/37 Loss: 521.867310\n",
      "2025-07-08 19:39: **********Train Epoch 217: averaged Loss: 1666.235722\n",
      "2025-07-08 19:39: **********Val Epoch 217: average Loss: 2596.721842\n",
      "2025-07-08 19:39: Train Epoch 218: 0/37 Loss: 376.406982\n",
      "2025-07-08 19:39: Train Epoch 218: 20/37 Loss: 329.864624\n",
      "2025-07-08 19:39: **********Train Epoch 218: averaged Loss: 568.562425\n",
      "2025-07-08 19:39: **********Val Epoch 218: average Loss: 2078.492188\n",
      "2025-07-08 19:39: Train Epoch 219: 0/37 Loss: 3668.125000\n",
      "2025-07-08 19:39: Train Epoch 219: 20/37 Loss: 634.528015\n",
      "2025-07-08 19:39: **********Train Epoch 219: averaged Loss: 964.586138\n",
      "2025-07-08 19:39: **********Val Epoch 219: average Loss: 1638.913289\n",
      "2025-07-08 19:39: Train Epoch 220: 0/37 Loss: 575.213501\n",
      "2025-07-08 19:39: Train Epoch 220: 20/37 Loss: 543.769287\n",
      "2025-07-08 19:39: **********Train Epoch 220: averaged Loss: 319.439287\n",
      "2025-07-08 19:39: **********Val Epoch 220: average Loss: 2619.238444\n",
      "2025-07-08 19:39: Train Epoch 221: 0/37 Loss: 1654.042969\n",
      "2025-07-08 19:39: Train Epoch 221: 20/37 Loss: 256.606506\n",
      "2025-07-08 19:39: **********Train Epoch 221: averaged Loss: 448.328096\n",
      "2025-07-08 19:39: **********Val Epoch 221: average Loss: 2275.144124\n",
      "2025-07-08 19:39: Train Epoch 222: 0/37 Loss: 1650.301514\n",
      "2025-07-08 19:39: Train Epoch 222: 20/37 Loss: 227.879929\n",
      "2025-07-08 19:39: **********Train Epoch 222: averaged Loss: 446.674956\n",
      "2025-07-08 19:39: **********Val Epoch 222: average Loss: 1457.397725\n",
      "2025-07-08 19:39: Train Epoch 223: 0/37 Loss: 1224.313232\n",
      "2025-07-08 19:39: Train Epoch 223: 20/37 Loss: 372.341553\n",
      "2025-07-08 19:39: **********Train Epoch 223: averaged Loss: 405.858810\n",
      "2025-07-08 19:39: **********Val Epoch 223: average Loss: 1456.869202\n",
      "2025-07-08 19:39: Train Epoch 224: 0/37 Loss: 2349.458984\n",
      "2025-07-08 19:39: Train Epoch 224: 20/37 Loss: 729.263733\n",
      "2025-07-08 19:39: **********Train Epoch 224: averaged Loss: 694.638664\n",
      "2025-07-08 19:39: **********Val Epoch 224: average Loss: 1263.457316\n",
      "2025-07-08 19:39: Train Epoch 225: 0/37 Loss: 2056.479980\n",
      "2025-07-08 19:39: Train Epoch 225: 20/37 Loss: 852.624329\n",
      "2025-07-08 19:39: **********Train Epoch 225: averaged Loss: 844.847841\n",
      "2025-07-08 19:39: **********Val Epoch 225: average Loss: 1190.739827\n",
      "2025-07-08 19:39: Train Epoch 226: 0/37 Loss: 2791.589111\n",
      "2025-07-08 19:39: Train Epoch 226: 20/37 Loss: 337.417969\n",
      "2025-07-08 19:39: **********Train Epoch 226: averaged Loss: 823.211048\n",
      "2025-07-08 19:39: **********Val Epoch 226: average Loss: 744.454987\n",
      "2025-07-08 19:39: Train Epoch 227: 0/37 Loss: 3449.104248\n",
      "2025-07-08 19:39: Train Epoch 227: 20/37 Loss: 257.697815\n",
      "2025-07-08 19:39: **********Train Epoch 227: averaged Loss: 923.508715\n",
      "2025-07-08 19:39: **********Val Epoch 227: average Loss: 2590.445068\n",
      "2025-07-08 19:39: Train Epoch 228: 0/37 Loss: 3851.560547\n",
      "2025-07-08 19:39: Train Epoch 228: 20/37 Loss: 462.900574\n",
      "2025-07-08 19:39: **********Train Epoch 228: averaged Loss: 1301.403184\n",
      "2025-07-08 19:39: **********Val Epoch 228: average Loss: 3193.775960\n",
      "2025-07-08 19:39: Train Epoch 229: 0/37 Loss: 1581.320557\n",
      "2025-07-08 19:39: Train Epoch 229: 20/37 Loss: 300.854919\n",
      "2025-07-08 19:39: **********Train Epoch 229: averaged Loss: 470.909201\n",
      "2025-07-08 19:39: **********Val Epoch 229: average Loss: 2755.244629\n",
      "2025-07-08 19:39: Train Epoch 230: 0/37 Loss: 3063.703613\n",
      "2025-07-08 19:39: Train Epoch 230: 20/37 Loss: 552.234436\n",
      "2025-07-08 19:39: **********Train Epoch 230: averaged Loss: 884.186284\n",
      "2025-07-08 19:39: **********Val Epoch 230: average Loss: 1093.966899\n",
      "2025-07-08 19:39: Train Epoch 231: 0/37 Loss: 282.674957\n",
      "2025-07-08 19:39: Train Epoch 231: 20/37 Loss: 196.821655\n",
      "2025-07-08 19:39: **********Train Epoch 231: averaged Loss: 332.797915\n",
      "2025-07-08 19:39: **********Val Epoch 231: average Loss: 2464.923665\n",
      "2025-07-08 19:39: Train Epoch 232: 0/37 Loss: 2414.969238\n",
      "2025-07-08 19:39: Train Epoch 232: 20/37 Loss: 330.591064\n",
      "2025-07-08 19:39: **********Train Epoch 232: averaged Loss: 562.517585\n",
      "2025-07-08 19:39: **********Val Epoch 232: average Loss: 1077.782043\n",
      "2025-07-08 19:39: Train Epoch 233: 0/37 Loss: 689.122192\n",
      "2025-07-08 19:39: Train Epoch 233: 20/37 Loss: 260.771057\n",
      "2025-07-08 19:39: **********Train Epoch 233: averaged Loss: 552.945019\n",
      "2025-07-08 19:39: **********Val Epoch 233: average Loss: 1774.796224\n",
      "2025-07-08 19:39: Train Epoch 234: 0/37 Loss: 2672.510742\n",
      "2025-07-08 19:39: Train Epoch 234: 20/37 Loss: 138.279984\n",
      "2025-07-08 19:39: **********Train Epoch 234: averaged Loss: 774.528697\n",
      "2025-07-08 19:39: **********Val Epoch 234: average Loss: 837.643677\n",
      "2025-07-08 19:39: Train Epoch 235: 0/37 Loss: 3817.369873\n",
      "2025-07-08 19:39: Train Epoch 235: 20/37 Loss: 293.050476\n",
      "2025-07-08 19:39: **********Train Epoch 235: averaged Loss: 1050.822699\n",
      "2025-07-08 19:39: **********Val Epoch 235: average Loss: 2716.950602\n",
      "2025-07-08 19:39: Train Epoch 236: 0/37 Loss: 4698.942383\n",
      "2025-07-08 19:39: Train Epoch 236: 20/37 Loss: 228.697128\n",
      "2025-07-08 19:39: **********Train Epoch 236: averaged Loss: 1237.846200\n",
      "2025-07-08 19:39: **********Val Epoch 236: average Loss: 2585.169189\n",
      "2025-07-08 19:39: Train Epoch 237: 0/37 Loss: 779.030212\n",
      "2025-07-08 19:39: Train Epoch 237: 20/37 Loss: 157.277466\n",
      "2025-07-08 19:39: **********Train Epoch 237: averaged Loss: 420.348805\n",
      "2025-07-08 19:39: **********Val Epoch 237: average Loss: 983.233948\n",
      "2025-07-08 19:39: Train Epoch 238: 0/37 Loss: 1182.153564\n",
      "2025-07-08 19:39: Train Epoch 238: 20/37 Loss: 378.561584\n",
      "2025-07-08 19:40: **********Train Epoch 238: averaged Loss: 515.463494\n",
      "2025-07-08 19:40: **********Val Epoch 238: average Loss: 2134.711711\n",
      "2025-07-08 19:40: Train Epoch 239: 0/37 Loss: 3036.402832\n",
      "2025-07-08 19:40: Train Epoch 239: 20/37 Loss: 153.519180\n",
      "2025-07-08 19:40: **********Train Epoch 239: averaged Loss: 885.834012\n",
      "2025-07-08 19:40: **********Val Epoch 239: average Loss: 2727.405721\n",
      "2025-07-08 19:40: Train Epoch 240: 0/37 Loss: 1056.959839\n",
      "2025-07-08 19:40: Train Epoch 240: 20/37 Loss: 177.745697\n",
      "2025-07-08 19:40: **********Train Epoch 240: averaged Loss: 491.488672\n",
      "2025-07-08 19:40: **********Val Epoch 240: average Loss: 1430.410238\n",
      "2025-07-08 19:40: Train Epoch 241: 0/37 Loss: 892.928955\n",
      "2025-07-08 19:40: Train Epoch 241: 20/37 Loss: 476.421906\n",
      "2025-07-08 19:40: **********Train Epoch 241: averaged Loss: 434.785368\n",
      "2025-07-08 19:40: **********Val Epoch 241: average Loss: 883.101329\n",
      "2025-07-08 19:40: Train Epoch 242: 0/37 Loss: 374.668060\n",
      "2025-07-08 19:40: Train Epoch 242: 20/37 Loss: 532.934814\n",
      "2025-07-08 19:40: **********Train Epoch 242: averaged Loss: 467.700917\n",
      "2025-07-08 19:40: **********Val Epoch 242: average Loss: 2082.759806\n",
      "2025-07-08 19:40: Train Epoch 243: 0/37 Loss: 3142.125488\n",
      "2025-07-08 19:40: Train Epoch 243: 20/37 Loss: 487.567871\n",
      "2025-07-08 19:40: **********Train Epoch 243: averaged Loss: 930.763121\n",
      "2025-07-08 19:40: **********Val Epoch 243: average Loss: 1864.740641\n",
      "2025-07-08 19:40: Train Epoch 244: 0/37 Loss: 879.583069\n",
      "2025-07-08 19:40: Train Epoch 244: 20/37 Loss: 161.972794\n",
      "2025-07-08 19:40: **********Train Epoch 244: averaged Loss: 506.790912\n",
      "2025-07-08 19:40: **********Val Epoch 244: average Loss: 1400.723287\n",
      "2025-07-08 19:40: Train Epoch 245: 0/37 Loss: 749.060486\n",
      "2025-07-08 19:40: Train Epoch 245: 20/37 Loss: 208.843063\n",
      "2025-07-08 19:40: **********Train Epoch 245: averaged Loss: 359.357354\n",
      "2025-07-08 19:40: **********Val Epoch 245: average Loss: 1064.282369\n",
      "2025-07-08 19:40: Train Epoch 246: 0/37 Loss: 1716.113037\n",
      "2025-07-08 19:40: Train Epoch 246: 20/37 Loss: 630.036316\n",
      "2025-07-08 19:40: **********Train Epoch 246: averaged Loss: 576.359446\n",
      "2025-07-08 19:40: **********Val Epoch 246: average Loss: 2012.359375\n",
      "2025-07-08 19:40: Train Epoch 247: 0/37 Loss: 2626.891602\n",
      "2025-07-08 19:40: Train Epoch 247: 20/37 Loss: 570.838745\n",
      "2025-07-08 19:40: **********Train Epoch 247: averaged Loss: 665.600483\n",
      "2025-07-08 19:40: **********Val Epoch 247: average Loss: 1699.540934\n",
      "2025-07-08 19:40: Train Epoch 248: 0/37 Loss: 1048.703735\n",
      "2025-07-08 19:40: Train Epoch 248: 20/37 Loss: 252.610214\n",
      "2025-07-08 19:40: **********Train Epoch 248: averaged Loss: 400.764391\n",
      "2025-07-08 19:40: **********Val Epoch 248: average Loss: 2000.133708\n",
      "2025-07-08 19:40: Train Epoch 249: 0/37 Loss: 1323.776001\n",
      "2025-07-08 19:40: Train Epoch 249: 20/37 Loss: 286.246826\n",
      "2025-07-08 19:40: **********Train Epoch 249: averaged Loss: 407.288722\n",
      "2025-07-08 19:40: **********Val Epoch 249: average Loss: 929.577515\n",
      "2025-07-08 19:40: Train Epoch 250: 0/37 Loss: 1173.002197\n",
      "2025-07-08 19:40: Train Epoch 250: 20/37 Loss: 336.923767\n",
      "2025-07-08 19:40: **********Train Epoch 250: averaged Loss: 507.284569\n",
      "2025-07-08 19:40: **********Val Epoch 250: average Loss: 1887.777425\n",
      "2025-07-08 19:40: Train Epoch 251: 0/37 Loss: 960.180176\n",
      "2025-07-08 19:40: Train Epoch 251: 20/37 Loss: 333.920990\n",
      "2025-07-08 19:40: **********Train Epoch 251: averaged Loss: 461.102579\n",
      "2025-07-08 19:40: **********Val Epoch 251: average Loss: 1198.495605\n",
      "2025-07-08 19:40: Train Epoch 252: 0/37 Loss: 1339.105957\n",
      "2025-07-08 19:40: Train Epoch 252: 20/37 Loss: 297.663879\n",
      "2025-07-08 19:40: **********Train Epoch 252: averaged Loss: 520.747125\n",
      "2025-07-08 19:40: **********Val Epoch 252: average Loss: 1274.726786\n",
      "2025-07-08 19:40: Train Epoch 253: 0/37 Loss: 267.965302\n",
      "2025-07-08 19:40: Train Epoch 253: 20/37 Loss: 384.678650\n",
      "2025-07-08 19:40: **********Train Epoch 253: averaged Loss: 349.492441\n",
      "2025-07-08 19:40: **********Val Epoch 253: average Loss: 592.933390\n",
      "2025-07-08 19:40: *********************************Current best model saved!\n",
      "2025-07-08 19:40: Train Epoch 254: 0/37 Loss: 966.653442\n",
      "2025-07-08 19:40: Train Epoch 254: 20/37 Loss: 367.102295\n",
      "2025-07-08 19:40: **********Train Epoch 254: averaged Loss: 690.387817\n",
      "2025-07-08 19:40: **********Val Epoch 254: average Loss: 1319.642761\n",
      "2025-07-08 19:40: Train Epoch 255: 0/37 Loss: 1339.544434\n",
      "2025-07-08 19:40: Train Epoch 255: 20/37 Loss: 429.443787\n",
      "2025-07-08 19:40: **********Train Epoch 255: averaged Loss: 966.310837\n",
      "2025-07-08 19:40: **********Val Epoch 255: average Loss: 3038.171224\n",
      "2025-07-08 19:40: Train Epoch 256: 0/37 Loss: 4406.796875\n",
      "2025-07-08 19:40: Train Epoch 256: 20/37 Loss: 1170.823730\n",
      "2025-07-08 19:40: **********Train Epoch 256: averaged Loss: 1395.391013\n",
      "2025-07-08 19:40: **********Val Epoch 256: average Loss: 1317.108114\n",
      "2025-07-08 19:40: Train Epoch 257: 0/37 Loss: 10314.613281\n",
      "2025-07-08 19:40: Train Epoch 257: 20/37 Loss: 288.847717\n",
      "2025-07-08 19:40: **********Train Epoch 257: averaged Loss: 2351.139036\n",
      "2025-07-08 19:40: **********Val Epoch 257: average Loss: 2246.663696\n",
      "2025-07-08 19:40: Train Epoch 258: 0/37 Loss: 3374.134277\n",
      "2025-07-08 19:40: Train Epoch 258: 20/37 Loss: 522.334778\n",
      "2025-07-08 19:40: **********Train Epoch 258: averaged Loss: 721.666049\n",
      "2025-07-08 19:40: **********Val Epoch 258: average Loss: 2256.493042\n",
      "2025-07-08 19:40: Train Epoch 259: 0/37 Loss: 756.461670\n",
      "2025-07-08 19:40: Train Epoch 259: 20/37 Loss: 378.239197\n",
      "2025-07-08 19:40: **********Train Epoch 259: averaged Loss: 350.955665\n",
      "2025-07-08 19:40: **********Val Epoch 259: average Loss: 2357.584432\n",
      "2025-07-08 19:40: Train Epoch 260: 0/37 Loss: 2608.838379\n",
      "2025-07-08 19:40: Train Epoch 260: 20/37 Loss: 778.932922\n",
      "2025-07-08 19:40: **********Train Epoch 260: averaged Loss: 741.961913\n",
      "2025-07-08 19:40: **********Val Epoch 260: average Loss: 911.738973\n",
      "2025-07-08 19:40: Train Epoch 261: 0/37 Loss: 1405.709106\n",
      "2025-07-08 19:40: Train Epoch 261: 20/37 Loss: 725.251831\n",
      "2025-07-08 19:40: **********Train Epoch 261: averaged Loss: 578.815949\n",
      "2025-07-08 19:40: **********Val Epoch 261: average Loss: 2758.253825\n",
      "2025-07-08 19:40: Train Epoch 262: 0/37 Loss: 3692.171387\n",
      "2025-07-08 19:40: Train Epoch 262: 20/37 Loss: 131.698441\n",
      "2025-07-08 19:40: **********Train Epoch 262: averaged Loss: 1089.939799\n",
      "2025-07-08 19:40: **********Val Epoch 262: average Loss: 1788.071981\n",
      "2025-07-08 19:40: Train Epoch 263: 0/37 Loss: 442.085236\n",
      "2025-07-08 19:40: Train Epoch 263: 20/37 Loss: 141.037842\n",
      "2025-07-08 19:40: **********Train Epoch 263: averaged Loss: 371.466168\n",
      "2025-07-08 19:40: **********Val Epoch 263: average Loss: 603.464671\n",
      "2025-07-08 19:40: Train Epoch 264: 0/37 Loss: 370.443298\n",
      "2025-07-08 19:40: Train Epoch 264: 20/37 Loss: 530.497742\n",
      "2025-07-08 19:40: **********Train Epoch 264: averaged Loss: 579.676875\n",
      "2025-07-08 19:40: **********Val Epoch 264: average Loss: 1402.214722\n",
      "2025-07-08 19:40: Train Epoch 265: 0/37 Loss: 3164.077393\n",
      "2025-07-08 19:40: Train Epoch 265: 20/37 Loss: 183.542786\n",
      "2025-07-08 19:40: **********Train Epoch 265: averaged Loss: 1031.811926\n",
      "2025-07-08 19:40: **********Val Epoch 265: average Loss: 2010.065837\n",
      "2025-07-08 19:40: Train Epoch 266: 0/37 Loss: 702.643921\n",
      "2025-07-08 19:40: Train Epoch 266: 20/37 Loss: 174.292862\n",
      "2025-07-08 19:40: **********Train Epoch 266: averaged Loss: 349.870299\n",
      "2025-07-08 19:40: **********Val Epoch 266: average Loss: 925.541707\n",
      "2025-07-08 19:40: Train Epoch 267: 0/37 Loss: 507.431335\n",
      "2025-07-08 19:40: Train Epoch 267: 20/37 Loss: 426.828247\n",
      "2025-07-08 19:40: **********Train Epoch 267: averaged Loss: 516.755712\n",
      "2025-07-08 19:40: **********Val Epoch 267: average Loss: 2279.644775\n",
      "2025-07-08 19:40: Train Epoch 268: 0/37 Loss: 2794.060791\n",
      "2025-07-08 19:40: Train Epoch 268: 20/37 Loss: 402.302429\n",
      "2025-07-08 19:40: **********Train Epoch 268: averaged Loss: 846.875668\n",
      "2025-07-08 19:40: **********Val Epoch 268: average Loss: 2124.292114\n",
      "2025-07-08 19:40: Train Epoch 269: 0/37 Loss: 561.425293\n",
      "2025-07-08 19:40: Train Epoch 269: 20/37 Loss: 129.250977\n",
      "2025-07-08 19:40: **********Train Epoch 269: averaged Loss: 448.743140\n",
      "2025-07-08 19:40: **********Val Epoch 269: average Loss: 1476.264547\n",
      "2025-07-08 19:40: Train Epoch 270: 0/37 Loss: 604.942383\n",
      "2025-07-08 19:40: Train Epoch 270: 20/37 Loss: 217.542648\n",
      "2025-07-08 19:40: **********Train Epoch 270: averaged Loss: 364.281788\n",
      "2025-07-08 19:40: **********Val Epoch 270: average Loss: 1518.588277\n",
      "2025-07-08 19:40: Train Epoch 271: 0/37 Loss: 1665.520264\n",
      "2025-07-08 19:40: Train Epoch 271: 20/37 Loss: 775.245850\n",
      "2025-07-08 19:40: **********Train Epoch 271: averaged Loss: 560.518874\n",
      "2025-07-08 19:40: **********Val Epoch 271: average Loss: 1290.230042\n",
      "2025-07-08 19:40: Train Epoch 272: 0/37 Loss: 1934.300537\n",
      "2025-07-08 19:40: Train Epoch 272: 20/37 Loss: 524.864990\n",
      "2025-07-08 19:41: **********Train Epoch 272: averaged Loss: 693.507967\n",
      "2025-07-08 19:41: **********Val Epoch 272: average Loss: 2260.673340\n",
      "2025-07-08 19:41: Train Epoch 273: 0/37 Loss: 2923.250000\n",
      "2025-07-08 19:41: Train Epoch 273: 20/37 Loss: 129.603683\n",
      "2025-07-08 19:41: **********Train Epoch 273: averaged Loss: 867.752252\n",
      "2025-07-08 19:41: **********Val Epoch 273: average Loss: 1619.602580\n",
      "2025-07-08 19:41: Train Epoch 274: 0/37 Loss: 931.919312\n",
      "2025-07-08 19:41: Train Epoch 274: 20/37 Loss: 215.431107\n",
      "2025-07-08 19:41: **********Train Epoch 274: averaged Loss: 417.676541\n",
      "2025-07-08 19:41: **********Val Epoch 274: average Loss: 2598.177979\n",
      "2025-07-08 19:41: Train Epoch 275: 0/37 Loss: 610.566711\n",
      "2025-07-08 19:41: Train Epoch 275: 20/37 Loss: 458.870148\n",
      "2025-07-08 19:41: **********Train Epoch 275: averaged Loss: 380.689943\n",
      "2025-07-08 19:41: **********Val Epoch 275: average Loss: 1316.493408\n",
      "2025-07-08 19:41: Train Epoch 276: 0/37 Loss: 1244.132202\n",
      "2025-07-08 19:41: Train Epoch 276: 20/37 Loss: 462.968048\n",
      "2025-07-08 19:41: **********Train Epoch 276: averaged Loss: 523.726294\n",
      "2025-07-08 19:41: **********Val Epoch 276: average Loss: 2390.312663\n",
      "2025-07-08 19:41: Train Epoch 277: 0/37 Loss: 761.250000\n",
      "2025-07-08 19:41: Train Epoch 277: 20/37 Loss: 206.075836\n",
      "2025-07-08 19:41: **********Train Epoch 277: averaged Loss: 421.732931\n",
      "2025-07-08 19:41: **********Val Epoch 277: average Loss: 1119.149089\n",
      "2025-07-08 19:41: Train Epoch 278: 0/37 Loss: 1018.898926\n",
      "2025-07-08 19:41: Train Epoch 278: 20/37 Loss: 133.805222\n",
      "2025-07-08 19:41: **********Train Epoch 278: averaged Loss: 351.107780\n",
      "2025-07-08 19:41: **********Val Epoch 278: average Loss: 1654.171285\n",
      "2025-07-08 19:41: Train Epoch 279: 0/37 Loss: 1027.934204\n",
      "2025-07-08 19:41: Train Epoch 279: 20/37 Loss: 203.802597\n",
      "2025-07-08 19:41: **********Train Epoch 279: averaged Loss: 381.632376\n",
      "2025-07-08 19:41: **********Val Epoch 279: average Loss: 1597.158122\n",
      "2025-07-08 19:41: Train Epoch 280: 0/37 Loss: 916.395630\n",
      "2025-07-08 19:41: Train Epoch 280: 20/37 Loss: 179.680664\n",
      "2025-07-08 19:41: **********Train Epoch 280: averaged Loss: 390.573281\n",
      "2025-07-08 19:41: **********Val Epoch 280: average Loss: 1248.002401\n",
      "2025-07-08 19:41: Train Epoch 281: 0/37 Loss: 947.989258\n",
      "2025-07-08 19:41: Train Epoch 281: 20/37 Loss: 244.668869\n",
      "2025-07-08 19:41: **********Train Epoch 281: averaged Loss: 356.456235\n",
      "2025-07-08 19:41: **********Val Epoch 281: average Loss: 854.410339\n",
      "2025-07-08 19:41: Train Epoch 282: 0/37 Loss: 861.971252\n",
      "2025-07-08 19:41: Train Epoch 282: 20/37 Loss: 151.537079\n",
      "2025-07-08 19:41: **********Train Epoch 282: averaged Loss: 412.897828\n",
      "2025-07-08 19:41: **********Val Epoch 282: average Loss: 1509.713745\n",
      "2025-07-08 19:41: Train Epoch 283: 0/37 Loss: 1234.551514\n",
      "2025-07-08 19:41: Train Epoch 283: 20/37 Loss: 143.916687\n",
      "2025-07-08 19:41: **********Train Epoch 283: averaged Loss: 553.804072\n",
      "2025-07-08 19:41: **********Val Epoch 283: average Loss: 998.471273\n",
      "2025-07-08 19:41: Train Epoch 284: 0/37 Loss: 324.821289\n",
      "2025-07-08 19:41: Train Epoch 284: 20/37 Loss: 378.316956\n",
      "2025-07-08 19:41: **********Train Epoch 284: averaged Loss: 358.499031\n",
      "2025-07-08 19:41: **********Val Epoch 284: average Loss: 1692.716736\n",
      "2025-07-08 19:41: Train Epoch 285: 0/37 Loss: 1129.181396\n",
      "2025-07-08 19:41: Train Epoch 285: 20/37 Loss: 466.912262\n",
      "2025-07-08 19:41: **********Train Epoch 285: averaged Loss: 516.494893\n",
      "2025-07-08 19:41: **********Val Epoch 285: average Loss: 1488.400920\n",
      "2025-07-08 19:41: Train Epoch 286: 0/37 Loss: 1233.308350\n",
      "2025-07-08 19:41: Train Epoch 286: 20/37 Loss: 291.040680\n",
      "2025-07-08 19:41: **********Train Epoch 286: averaged Loss: 451.538917\n",
      "2025-07-08 19:41: **********Val Epoch 286: average Loss: 2207.083333\n",
      "2025-07-08 19:41: Train Epoch 287: 0/37 Loss: 1769.115601\n",
      "2025-07-08 19:41: Train Epoch 287: 20/37 Loss: 209.135986\n",
      "2025-07-08 19:41: **********Train Epoch 287: averaged Loss: 511.448867\n",
      "2025-07-08 19:41: **********Val Epoch 287: average Loss: 1092.390951\n",
      "2025-07-08 19:41: Train Epoch 288: 0/37 Loss: 770.352600\n",
      "2025-07-08 19:41: Train Epoch 288: 20/37 Loss: 731.057373\n",
      "2025-07-08 19:41: **********Train Epoch 288: averaged Loss: 749.125242\n",
      "2025-07-08 19:41: **********Val Epoch 288: average Loss: 2103.921794\n",
      "2025-07-08 19:41: Train Epoch 289: 0/37 Loss: 6410.828613\n",
      "2025-07-08 19:41: Train Epoch 289: 20/37 Loss: 234.445938\n",
      "2025-07-08 19:41: **********Train Epoch 289: averaged Loss: 1857.048188\n",
      "2025-07-08 19:41: **********Val Epoch 289: average Loss: 1601.086690\n",
      "2025-07-08 19:41: Train Epoch 290: 0/37 Loss: 1276.785278\n",
      "2025-07-08 19:41: Train Epoch 290: 20/37 Loss: 425.666870\n",
      "2025-07-08 19:41: **********Train Epoch 290: averaged Loss: 508.371132\n",
      "2025-07-08 19:41: **********Val Epoch 290: average Loss: 2309.818848\n",
      "2025-07-08 19:41: Train Epoch 291: 0/37 Loss: 4051.741455\n",
      "2025-07-08 19:41: Train Epoch 291: 20/37 Loss: 800.354004\n",
      "2025-07-08 19:41: **********Train Epoch 291: averaged Loss: 941.926492\n",
      "2025-07-08 19:41: **********Val Epoch 291: average Loss: 1343.404989\n",
      "2025-07-08 19:41: Train Epoch 292: 0/37 Loss: 1569.754150\n",
      "2025-07-08 19:41: Train Epoch 292: 20/37 Loss: 391.364563\n",
      "2025-07-08 19:41: **********Train Epoch 292: averaged Loss: 470.825451\n",
      "2025-07-08 19:41: **********Val Epoch 292: average Loss: 876.320353\n",
      "2025-07-08 19:41: Train Epoch 293: 0/37 Loss: 2412.528564\n",
      "2025-07-08 19:41: Train Epoch 293: 20/37 Loss: 437.151611\n",
      "2025-07-08 19:41: **********Train Epoch 293: averaged Loss: 699.362652\n",
      "2025-07-08 19:41: **********Val Epoch 293: average Loss: 1372.639038\n",
      "2025-07-08 19:41: Train Epoch 294: 0/37 Loss: 3024.213379\n",
      "2025-07-08 19:41: Train Epoch 294: 20/37 Loss: 216.695969\n",
      "2025-07-08 19:41: **********Train Epoch 294: averaged Loss: 900.118062\n",
      "2025-07-08 19:41: **********Val Epoch 294: average Loss: 1880.440267\n",
      "2025-07-08 19:41: Train Epoch 295: 0/37 Loss: 2929.796631\n",
      "2025-07-08 19:41: Train Epoch 295: 20/37 Loss: 592.547852\n",
      "2025-07-08 19:41: **********Train Epoch 295: averaged Loss: 759.353445\n",
      "2025-07-08 19:41: **********Val Epoch 295: average Loss: 959.037842\n",
      "2025-07-08 19:41: Train Epoch 296: 0/37 Loss: 839.198364\n",
      "2025-07-08 19:41: Train Epoch 296: 20/37 Loss: 355.607910\n",
      "2025-07-08 19:41: **********Train Epoch 296: averaged Loss: 777.810287\n",
      "2025-07-08 19:41: **********Val Epoch 296: average Loss: 1881.088094\n",
      "2025-07-08 19:41: Train Epoch 297: 0/37 Loss: 2475.925049\n",
      "2025-07-08 19:41: Train Epoch 297: 20/37 Loss: 903.653687\n",
      "2025-07-08 19:41: **********Train Epoch 297: averaged Loss: 1453.046142\n",
      "2025-07-08 19:41: **********Val Epoch 297: average Loss: 3552.440348\n",
      "2025-07-08 19:41: Train Epoch 298: 0/37 Loss: 4063.745605\n",
      "2025-07-08 19:41: Train Epoch 298: 20/37 Loss: 409.433716\n",
      "2025-07-08 19:41: **********Train Epoch 298: averaged Loss: 1676.849491\n",
      "2025-07-08 19:41: **********Val Epoch 298: average Loss: 4599.523275\n",
      "2025-07-08 19:41: Train Epoch 299: 0/37 Loss: 5723.707031\n",
      "2025-07-08 19:41: Train Epoch 299: 20/37 Loss: 543.430298\n",
      "2025-07-08 19:41: **********Train Epoch 299: averaged Loss: 1732.090911\n",
      "2025-07-08 19:41: **********Val Epoch 299: average Loss: 1114.877930\n",
      "2025-07-08 19:41: Train Epoch 300: 0/37 Loss: 287.941772\n",
      "2025-07-08 19:41: Train Epoch 300: 20/37 Loss: 135.912537\n",
      "2025-07-08 19:41: **********Train Epoch 300: averaged Loss: 388.965361\n",
      "2025-07-08 19:41: **********Val Epoch 300: average Loss: 584.439423\n",
      "2025-07-08 19:41: *********************************Current best model saved!\n",
      "2025-07-08 19:41: Train Epoch 301: 0/37 Loss: 1387.804077\n",
      "2025-07-08 19:41: Train Epoch 301: 20/37 Loss: 707.774963\n",
      "2025-07-08 19:41: **********Train Epoch 301: averaged Loss: 750.113678\n",
      "2025-07-08 19:41: **********Val Epoch 301: average Loss: 1146.223348\n",
      "2025-07-08 19:41: Train Epoch 302: 0/37 Loss: 4039.251953\n",
      "2025-07-08 19:41: Train Epoch 302: 20/37 Loss: 211.126602\n",
      "2025-07-08 19:41: **********Train Epoch 302: averaged Loss: 1321.543609\n",
      "2025-07-08 19:41: **********Val Epoch 302: average Loss: 2054.065145\n",
      "2025-07-08 19:41: Train Epoch 303: 0/37 Loss: 243.493500\n",
      "2025-07-08 19:41: Train Epoch 303: 20/37 Loss: 282.975830\n",
      "2025-07-08 19:41: **********Train Epoch 303: averaged Loss: 432.773310\n",
      "2025-07-08 19:41: **********Val Epoch 303: average Loss: 1633.497253\n",
      "2025-07-08 19:41: Train Epoch 304: 0/37 Loss: 2785.416992\n",
      "2025-07-08 19:41: Train Epoch 304: 20/37 Loss: 308.867188\n",
      "2025-07-08 19:41: **********Train Epoch 304: averaged Loss: 801.804763\n",
      "2025-07-08 19:41: **********Val Epoch 304: average Loss: 1469.523905\n",
      "2025-07-08 19:41: Train Epoch 305: 0/37 Loss: 329.898071\n",
      "2025-07-08 19:41: Train Epoch 305: 20/37 Loss: 225.718307\n",
      "2025-07-08 19:41: **********Train Epoch 305: averaged Loss: 342.990080\n",
      "2025-07-08 19:41: **********Val Epoch 305: average Loss: 1579.733724\n",
      "2025-07-08 19:41: Train Epoch 306: 0/37 Loss: 282.746277\n",
      "2025-07-08 19:41: Train Epoch 306: 20/37 Loss: 214.622986\n",
      "2025-07-08 19:42: **********Train Epoch 306: averaged Loss: 269.023557\n",
      "2025-07-08 19:42: **********Val Epoch 306: average Loss: 1290.966919\n",
      "2025-07-08 19:42: Train Epoch 307: 0/37 Loss: 277.792328\n",
      "2025-07-08 19:42: Train Epoch 307: 20/37 Loss: 356.397339\n",
      "2025-07-08 19:42: **********Train Epoch 307: averaged Loss: 487.583922\n",
      "2025-07-08 19:42: **********Val Epoch 307: average Loss: 2109.383952\n",
      "2025-07-08 19:42: Train Epoch 308: 0/37 Loss: 2799.780273\n",
      "2025-07-08 19:42: Train Epoch 308: 20/37 Loss: 174.974823\n",
      "2025-07-08 19:42: **********Train Epoch 308: averaged Loss: 1024.041440\n",
      "2025-07-08 19:42: **********Val Epoch 308: average Loss: 1373.082682\n",
      "2025-07-08 19:42: Train Epoch 309: 0/37 Loss: 2337.311768\n",
      "2025-07-08 19:42: Train Epoch 309: 20/37 Loss: 494.373993\n",
      "2025-07-08 19:42: **********Train Epoch 309: averaged Loss: 961.647460\n",
      "2025-07-08 19:42: **********Val Epoch 309: average Loss: 1890.402384\n",
      "2025-07-08 19:42: Train Epoch 310: 0/37 Loss: 4820.977539\n",
      "2025-07-08 19:42: Train Epoch 310: 20/37 Loss: 299.881317\n",
      "2025-07-08 19:42: **********Train Epoch 310: averaged Loss: 1571.058362\n",
      "2025-07-08 19:42: **********Val Epoch 310: average Loss: 3499.286377\n",
      "2025-07-08 19:42: Train Epoch 311: 0/37 Loss: 1509.820312\n",
      "2025-07-08 19:42: Train Epoch 311: 20/37 Loss: 709.618286\n",
      "2025-07-08 19:42: **********Train Epoch 311: averaged Loss: 630.162465\n",
      "2025-07-08 19:42: **********Val Epoch 311: average Loss: 2272.560425\n",
      "2025-07-08 19:42: Train Epoch 312: 0/37 Loss: 3326.045166\n",
      "2025-07-08 19:42: Train Epoch 312: 20/37 Loss: 300.221039\n",
      "2025-07-08 19:42: **********Train Epoch 312: averaged Loss: 960.857417\n",
      "2025-07-08 19:42: **********Val Epoch 312: average Loss: 1723.970093\n",
      "2025-07-08 19:42: Train Epoch 313: 0/37 Loss: 344.255127\n",
      "2025-07-08 19:42: Train Epoch 313: 20/37 Loss: 149.291107\n",
      "2025-07-08 19:42: **********Train Epoch 313: averaged Loss: 357.189070\n",
      "2025-07-08 19:42: **********Val Epoch 313: average Loss: 1083.348368\n",
      "2025-07-08 19:42: Train Epoch 314: 0/37 Loss: 1342.682373\n",
      "2025-07-08 19:42: Train Epoch 314: 20/37 Loss: 720.788574\n",
      "2025-07-08 19:42: **********Train Epoch 314: averaged Loss: 561.392520\n",
      "2025-07-08 19:42: **********Val Epoch 314: average Loss: 2199.919718\n",
      "2025-07-08 19:42: Train Epoch 315: 0/37 Loss: 3017.057861\n",
      "2025-07-08 19:42: Train Epoch 315: 20/37 Loss: 166.848480\n",
      "2025-07-08 19:42: **********Train Epoch 315: averaged Loss: 869.587270\n",
      "2025-07-08 19:42: **********Val Epoch 315: average Loss: 1869.321859\n",
      "2025-07-08 19:42: Train Epoch 316: 0/37 Loss: 219.953720\n",
      "2025-07-08 19:42: Train Epoch 316: 20/37 Loss: 117.404831\n",
      "2025-07-08 19:42: **********Train Epoch 316: averaged Loss: 439.200358\n",
      "2025-07-08 19:42: **********Val Epoch 316: average Loss: 1042.848490\n",
      "2025-07-08 19:42: Train Epoch 317: 0/37 Loss: 526.823669\n",
      "2025-07-08 19:42: Train Epoch 317: 20/37 Loss: 177.745010\n",
      "2025-07-08 19:42: **********Train Epoch 317: averaged Loss: 319.374279\n",
      "2025-07-08 19:42: **********Val Epoch 317: average Loss: 1343.320923\n",
      "2025-07-08 19:42: Train Epoch 318: 0/37 Loss: 1752.790771\n",
      "2025-07-08 19:42: Train Epoch 318: 20/37 Loss: 624.164673\n",
      "2025-07-08 19:42: **********Train Epoch 318: averaged Loss: 591.120964\n",
      "2025-07-08 19:42: **********Val Epoch 318: average Loss: 2036.738322\n",
      "2025-07-08 19:42: Train Epoch 319: 0/37 Loss: 2375.753418\n",
      "2025-07-08 19:42: Train Epoch 319: 20/37 Loss: 456.790894\n",
      "2025-07-08 19:42: **********Train Epoch 319: averaged Loss: 566.641955\n",
      "2025-07-08 19:42: **********Val Epoch 319: average Loss: 1410.716471\n",
      "2025-07-08 19:42: Train Epoch 320: 0/37 Loss: 418.274841\n",
      "2025-07-08 19:42: Train Epoch 320: 20/37 Loss: 152.540024\n",
      "2025-07-08 19:42: **********Train Epoch 320: averaged Loss: 292.097343\n",
      "2025-07-08 19:42: **********Val Epoch 320: average Loss: 1532.594421\n",
      "2025-07-08 19:42: Train Epoch 321: 0/37 Loss: 570.600830\n",
      "2025-07-08 19:42: Train Epoch 321: 20/37 Loss: 166.971588\n",
      "2025-07-08 19:42: **********Train Epoch 321: averaged Loss: 312.032981\n",
      "2025-07-08 19:42: **********Val Epoch 321: average Loss: 1332.634359\n",
      "2025-07-08 19:42: Train Epoch 322: 0/37 Loss: 767.234497\n",
      "2025-07-08 19:42: Train Epoch 322: 20/37 Loss: 241.022003\n",
      "2025-07-08 19:42: **********Train Epoch 322: averaged Loss: 332.518116\n",
      "2025-07-08 19:42: **********Val Epoch 322: average Loss: 504.386281\n",
      "2025-07-08 19:42: *********************************Current best model saved!\n",
      "2025-07-08 19:42: Train Epoch 323: 0/37 Loss: 666.335938\n",
      "2025-07-08 19:42: Train Epoch 323: 20/37 Loss: 169.392288\n",
      "2025-07-08 19:42: **********Train Epoch 323: averaged Loss: 553.136651\n",
      "2025-07-08 19:42: **********Val Epoch 323: average Loss: 1083.227417\n",
      "2025-07-08 19:42: Train Epoch 324: 0/37 Loss: 1719.245972\n",
      "2025-07-08 19:42: Train Epoch 324: 20/37 Loss: 435.969116\n",
      "2025-07-08 19:42: **********Train Epoch 324: averaged Loss: 575.653083\n",
      "2025-07-08 19:42: **********Val Epoch 324: average Loss: 991.692596\n",
      "2025-07-08 19:42: Train Epoch 325: 0/37 Loss: 2959.864746\n",
      "2025-07-08 19:42: Train Epoch 325: 20/37 Loss: 599.859375\n",
      "2025-07-08 19:42: **********Train Epoch 325: averaged Loss: 1019.504876\n",
      "2025-07-08 19:42: **********Val Epoch 325: average Loss: 1957.400553\n",
      "2025-07-08 19:42: Train Epoch 326: 0/37 Loss: 1280.453979\n",
      "2025-07-08 19:42: Train Epoch 326: 20/37 Loss: 467.617645\n",
      "2025-07-08 19:42: **********Train Epoch 326: averaged Loss: 761.048840\n",
      "2025-07-08 19:42: **********Val Epoch 326: average Loss: 2211.868856\n",
      "2025-07-08 19:42: Train Epoch 327: 0/37 Loss: 2939.302979\n",
      "2025-07-08 19:42: Train Epoch 327: 20/37 Loss: 217.337036\n",
      "2025-07-08 19:42: **********Train Epoch 327: averaged Loss: 1134.251293\n",
      "2025-07-08 19:42: **********Val Epoch 327: average Loss: 452.153544\n",
      "2025-07-08 19:42: *********************************Current best model saved!\n",
      "2025-07-08 19:42: Train Epoch 328: 0/37 Loss: 2984.573242\n",
      "2025-07-08 19:42: Train Epoch 328: 20/37 Loss: 416.419617\n",
      "2025-07-08 19:42: **********Train Epoch 328: averaged Loss: 952.994599\n",
      "2025-07-08 19:42: **********Val Epoch 328: average Loss: 2753.277262\n",
      "2025-07-08 19:42: Train Epoch 329: 0/37 Loss: 5813.732422\n",
      "2025-07-08 19:42: Train Epoch 329: 20/37 Loss: 361.367798\n",
      "2025-07-08 19:42: **********Train Epoch 329: averaged Loss: 1454.152368\n",
      "2025-07-08 19:42: **********Val Epoch 329: average Loss: 2686.832031\n",
      "2025-07-08 19:42: Train Epoch 330: 0/37 Loss: 4065.992676\n",
      "2025-07-08 19:42: Train Epoch 330: 20/37 Loss: 523.944214\n",
      "2025-07-08 19:42: **********Train Epoch 330: averaged Loss: 1013.414336\n",
      "2025-07-08 19:42: **********Val Epoch 330: average Loss: 1119.497884\n",
      "2025-07-08 19:42: Train Epoch 331: 0/37 Loss: 217.459564\n",
      "2025-07-08 19:42: Train Epoch 331: 20/37 Loss: 172.564072\n",
      "2025-07-08 19:42: **********Train Epoch 331: averaged Loss: 309.625913\n",
      "2025-07-08 19:42: **********Val Epoch 331: average Loss: 2528.863403\n",
      "2025-07-08 19:42: Train Epoch 332: 0/37 Loss: 2079.751221\n",
      "2025-07-08 19:42: Train Epoch 332: 20/37 Loss: 226.144165\n",
      "2025-07-08 19:42: **********Train Epoch 332: averaged Loss: 489.859554\n",
      "2025-07-08 19:42: **********Val Epoch 332: average Loss: 1205.863790\n",
      "2025-07-08 19:42: Train Epoch 333: 0/37 Loss: 372.402649\n",
      "2025-07-08 19:42: Train Epoch 333: 20/37 Loss: 282.610107\n",
      "2025-07-08 19:42: **********Train Epoch 333: averaged Loss: 265.672174\n",
      "2025-07-08 19:42: **********Val Epoch 333: average Loss: 1775.448730\n",
      "2025-07-08 19:42: Train Epoch 334: 0/37 Loss: 1224.862549\n",
      "2025-07-08 19:42: Train Epoch 334: 20/37 Loss: 158.854233\n",
      "2025-07-08 19:42: **********Train Epoch 334: averaged Loss: 379.801041\n",
      "2025-07-08 19:42: **********Val Epoch 334: average Loss: 1375.535278\n",
      "2025-07-08 19:42: Train Epoch 335: 0/37 Loss: 889.986694\n",
      "2025-07-08 19:42: Train Epoch 335: 20/37 Loss: 144.465637\n",
      "2025-07-08 19:42: **********Train Epoch 335: averaged Loss: 298.911328\n",
      "2025-07-08 19:42: **********Val Epoch 335: average Loss: 998.033783\n",
      "2025-07-08 19:42: Train Epoch 336: 0/37 Loss: 886.396179\n",
      "2025-07-08 19:42: Train Epoch 336: 20/37 Loss: 164.223404\n",
      "2025-07-08 19:42: **********Train Epoch 336: averaged Loss: 309.800490\n",
      "2025-07-08 19:42: **********Val Epoch 336: average Loss: 891.458221\n",
      "2025-07-08 19:42: Train Epoch 337: 0/37 Loss: 886.506470\n",
      "2025-07-08 19:42: Train Epoch 337: 20/37 Loss: 192.122711\n",
      "2025-07-08 19:42: **********Train Epoch 337: averaged Loss: 329.079349\n",
      "2025-07-08 19:42: **********Val Epoch 337: average Loss: 687.618601\n",
      "2025-07-08 19:42: Train Epoch 338: 0/37 Loss: 386.290222\n",
      "2025-07-08 19:42: Train Epoch 338: 20/37 Loss: 189.605728\n",
      "2025-07-08 19:42: **********Train Epoch 338: averaged Loss: 341.422087\n",
      "2025-07-08 19:42: **********Val Epoch 338: average Loss: 1334.576782\n",
      "2025-07-08 19:42: Train Epoch 339: 0/37 Loss: 1035.038818\n",
      "2025-07-08 19:42: Train Epoch 339: 20/37 Loss: 264.016083\n",
      "2025-07-08 19:42: **********Train Epoch 339: averaged Loss: 366.142488\n",
      "2025-07-08 19:42: **********Val Epoch 339: average Loss: 743.764730\n",
      "2025-07-08 19:42: Train Epoch 340: 0/37 Loss: 585.142944\n",
      "2025-07-08 19:42: Train Epoch 340: 20/37 Loss: 139.997147\n",
      "2025-07-08 19:43: **********Train Epoch 340: averaged Loss: 290.231238\n",
      "2025-07-08 19:43: **********Val Epoch 340: average Loss: 381.494466\n",
      "2025-07-08 19:43: *********************************Current best model saved!\n",
      "2025-07-08 19:43: Train Epoch 341: 0/37 Loss: 1584.260376\n",
      "2025-07-08 19:43: Train Epoch 341: 20/37 Loss: 234.712631\n",
      "2025-07-08 19:43: **********Train Epoch 341: averaged Loss: 898.461228\n",
      "2025-07-08 19:43: **********Val Epoch 341: average Loss: 1998.025106\n",
      "2025-07-08 19:43: Train Epoch 342: 0/37 Loss: 2146.027588\n",
      "2025-07-08 19:43: Train Epoch 342: 20/37 Loss: 1215.342896\n",
      "2025-07-08 19:43: **********Train Epoch 342: averaged Loss: 1284.612362\n",
      "2025-07-08 19:43: **********Val Epoch 342: average Loss: 801.961995\n",
      "2025-07-08 19:43: Train Epoch 343: 0/37 Loss: 5188.937988\n",
      "2025-07-08 19:43: Train Epoch 343: 20/37 Loss: 352.041443\n",
      "2025-07-08 19:43: **********Train Epoch 343: averaged Loss: 1493.867571\n",
      "2025-07-08 19:43: **********Val Epoch 343: average Loss: 1292.493774\n",
      "2025-07-08 19:43: Train Epoch 344: 0/37 Loss: 4769.515625\n",
      "2025-07-08 19:43: Train Epoch 344: 20/37 Loss: 301.028503\n",
      "2025-07-08 19:43: **********Train Epoch 344: averaged Loss: 1088.734657\n",
      "2025-07-08 19:43: **********Val Epoch 344: average Loss: 966.869120\n",
      "2025-07-08 19:43: Train Epoch 345: 0/37 Loss: 2899.925781\n",
      "2025-07-08 19:43: Train Epoch 345: 20/37 Loss: 292.783508\n",
      "2025-07-08 19:43: **********Train Epoch 345: averaged Loss: 916.128366\n",
      "2025-07-08 19:43: **********Val Epoch 345: average Loss: 1956.869344\n",
      "2025-07-08 19:43: Train Epoch 346: 0/37 Loss: 5013.875000\n",
      "2025-07-08 19:43: Train Epoch 346: 20/37 Loss: 239.757614\n",
      "2025-07-08 19:43: **********Train Epoch 346: averaged Loss: 1413.044041\n",
      "2025-07-08 19:43: **********Val Epoch 346: average Loss: 383.186086\n",
      "2025-07-08 19:43: Train Epoch 347: 0/37 Loss: 3118.735596\n",
      "2025-07-08 19:43: Train Epoch 347: 20/37 Loss: 408.561646\n",
      "2025-07-08 19:43: **********Train Epoch 347: averaged Loss: 672.923932\n",
      "2025-07-08 19:43: **********Val Epoch 347: average Loss: 2845.087972\n",
      "2025-07-08 19:43: Train Epoch 348: 0/37 Loss: 3950.796387\n",
      "2025-07-08 19:43: Train Epoch 348: 20/37 Loss: 747.809448\n",
      "2025-07-08 19:43: **********Train Epoch 348: averaged Loss: 902.774628\n",
      "2025-07-08 19:43: **********Val Epoch 348: average Loss: 1493.845500\n",
      "2025-07-08 19:43: Train Epoch 349: 0/37 Loss: 1334.449463\n",
      "2025-07-08 19:43: Train Epoch 349: 20/37 Loss: 714.368286\n",
      "2025-07-08 19:43: **********Train Epoch 349: averaged Loss: 649.817528\n",
      "2025-07-08 19:43: **********Val Epoch 349: average Loss: 2183.487630\n",
      "2025-07-08 19:43: Train Epoch 350: 0/37 Loss: 3035.756348\n",
      "2025-07-08 19:43: Train Epoch 350: 20/37 Loss: 343.718079\n",
      "2025-07-08 19:43: **********Train Epoch 350: averaged Loss: 917.535838\n",
      "2025-07-08 19:43: **********Val Epoch 350: average Loss: 1435.290934\n",
      "2025-07-08 19:43: Train Epoch 351: 0/37 Loss: 807.138916\n",
      "2025-07-08 19:43: Train Epoch 351: 20/37 Loss: 295.245972\n",
      "2025-07-08 19:43: **********Train Epoch 351: averaged Loss: 368.491867\n",
      "2025-07-08 19:43: **********Val Epoch 351: average Loss: 2891.982178\n",
      "2025-07-08 19:43: Train Epoch 352: 0/37 Loss: 2943.426270\n",
      "2025-07-08 19:43: Train Epoch 352: 20/37 Loss: 442.710510\n",
      "2025-07-08 19:43: **********Train Epoch 352: averaged Loss: 851.107001\n",
      "2025-07-08 19:43: **********Val Epoch 352: average Loss: 1343.887777\n",
      "2025-07-08 19:43: Train Epoch 353: 0/37 Loss: 1905.317505\n",
      "2025-07-08 19:43: Train Epoch 353: 20/37 Loss: 612.694824\n",
      "2025-07-08 19:43: **********Train Epoch 353: averaged Loss: 530.580160\n",
      "2025-07-08 19:43: **********Val Epoch 353: average Loss: 1680.302144\n",
      "2025-07-08 19:43: Train Epoch 354: 0/37 Loss: 2073.722412\n",
      "2025-07-08 19:43: Train Epoch 354: 20/37 Loss: 516.314209\n",
      "2025-07-08 19:43: **********Train Epoch 354: averaged Loss: 564.705971\n",
      "2025-07-08 19:43: **********Val Epoch 354: average Loss: 1533.929362\n",
      "2025-07-08 19:43: Train Epoch 355: 0/37 Loss: 1542.100952\n",
      "2025-07-08 19:43: Train Epoch 355: 20/37 Loss: 556.363098\n",
      "2025-07-08 19:43: **********Train Epoch 355: averaged Loss: 504.665522\n",
      "2025-07-08 19:43: **********Val Epoch 355: average Loss: 1392.361857\n",
      "2025-07-08 19:43: Train Epoch 356: 0/37 Loss: 1144.080322\n",
      "2025-07-08 19:43: Train Epoch 356: 20/37 Loss: 332.352173\n",
      "2025-07-08 19:43: **********Train Epoch 356: averaged Loss: 560.418650\n",
      "2025-07-08 19:43: **********Val Epoch 356: average Loss: 1141.624837\n",
      "2025-07-08 19:43: Train Epoch 357: 0/37 Loss: 1055.938965\n",
      "2025-07-08 19:43: Train Epoch 357: 20/37 Loss: 174.996262\n",
      "2025-07-08 19:43: **********Train Epoch 357: averaged Loss: 336.272851\n",
      "2025-07-08 19:43: **********Val Epoch 357: average Loss: 1765.112956\n",
      "2025-07-08 19:43: Train Epoch 358: 0/37 Loss: 1006.237671\n",
      "2025-07-08 19:43: Train Epoch 358: 20/37 Loss: 489.335632\n",
      "2025-07-08 19:43: **********Train Epoch 358: averaged Loss: 456.452779\n",
      "2025-07-08 19:43: **********Val Epoch 358: average Loss: 1252.610433\n",
      "2025-07-08 19:43: Train Epoch 359: 0/37 Loss: 722.867126\n",
      "2025-07-08 19:43: Train Epoch 359: 20/37 Loss: 124.363693\n",
      "2025-07-08 19:43: **********Train Epoch 359: averaged Loss: 332.833187\n",
      "2025-07-08 19:43: **********Val Epoch 359: average Loss: 1194.095703\n",
      "2025-07-08 19:43: Train Epoch 360: 0/37 Loss: 218.569244\n",
      "2025-07-08 19:43: Train Epoch 360: 20/37 Loss: 197.581192\n",
      "2025-07-08 19:43: **********Train Epoch 360: averaged Loss: 350.007661\n",
      "2025-07-08 19:43: **********Val Epoch 360: average Loss: 2160.663737\n",
      "2025-07-08 19:43: Train Epoch 361: 0/37 Loss: 1750.915161\n",
      "2025-07-08 19:43: Train Epoch 361: 20/37 Loss: 183.722321\n",
      "2025-07-08 19:43: **********Train Epoch 361: averaged Loss: 467.572269\n",
      "2025-07-08 19:43: **********Val Epoch 361: average Loss: 1189.008708\n",
      "2025-07-08 19:43: Train Epoch 362: 0/37 Loss: 1587.214844\n",
      "2025-07-08 19:43: Train Epoch 362: 20/37 Loss: 172.193512\n",
      "2025-07-08 19:43: **********Train Epoch 362: averaged Loss: 596.289034\n",
      "2025-07-08 19:43: **********Val Epoch 362: average Loss: 2631.537720\n",
      "2025-07-08 19:43: Train Epoch 363: 0/37 Loss: 2013.079346\n",
      "2025-07-08 19:43: Train Epoch 363: 20/37 Loss: 223.259216\n",
      "2025-07-08 19:43: **********Train Epoch 363: averaged Loss: 530.950758\n",
      "2025-07-08 19:43: **********Val Epoch 363: average Loss: 1045.891754\n",
      "2025-07-08 19:43: Train Epoch 364: 0/37 Loss: 1786.871094\n",
      "2025-07-08 19:43: Train Epoch 364: 20/37 Loss: 407.689819\n",
      "2025-07-08 19:43: **********Train Epoch 364: averaged Loss: 854.971566\n",
      "2025-07-08 19:43: **********Val Epoch 364: average Loss: 3191.225586\n",
      "2025-07-08 19:43: Train Epoch 365: 0/37 Loss: 3844.069824\n",
      "2025-07-08 19:43: Train Epoch 365: 20/37 Loss: 208.045639\n",
      "2025-07-08 19:43: **********Train Epoch 365: averaged Loss: 1635.860385\n",
      "2025-07-08 19:43: **********Val Epoch 365: average Loss: 2618.200521\n",
      "2025-07-08 19:43: Train Epoch 366: 0/37 Loss: 1252.203857\n",
      "2025-07-08 19:43: Train Epoch 366: 20/37 Loss: 505.231445\n",
      "2025-07-08 19:43: **********Train Epoch 366: averaged Loss: 818.152674\n",
      "2025-07-08 19:43: **********Val Epoch 366: average Loss: 1816.610881\n",
      "2025-07-08 19:43: Train Epoch 367: 0/37 Loss: 6302.715332\n",
      "2025-07-08 19:43: Train Epoch 367: 20/37 Loss: 127.378479\n",
      "2025-07-08 19:43: **********Train Epoch 367: averaged Loss: 1730.973370\n",
      "2025-07-08 19:43: **********Val Epoch 367: average Loss: 2318.408162\n",
      "2025-07-08 19:43: Train Epoch 368: 0/37 Loss: 1088.964966\n",
      "2025-07-08 19:43: Train Epoch 368: 20/37 Loss: 602.529541\n",
      "2025-07-08 19:43: **********Train Epoch 368: averaged Loss: 591.250676\n",
      "2025-07-08 19:43: **********Val Epoch 368: average Loss: 1750.804525\n",
      "2025-07-08 19:43: Train Epoch 369: 0/37 Loss: 3562.686279\n",
      "2025-07-08 19:43: Train Epoch 369: 20/37 Loss: 124.875092\n",
      "2025-07-08 19:43: **********Train Epoch 369: averaged Loss: 920.912752\n",
      "2025-07-08 19:43: **********Val Epoch 369: average Loss: 1665.634135\n",
      "2025-07-08 19:43: Train Epoch 370: 0/37 Loss: 905.864990\n",
      "2025-07-08 19:43: Train Epoch 370: 20/37 Loss: 148.802795\n",
      "2025-07-08 19:43: **********Train Epoch 370: averaged Loss: 366.219765\n",
      "2025-07-08 19:43: **********Val Epoch 370: average Loss: 1016.384644\n",
      "2025-07-08 19:43: Train Epoch 371: 0/37 Loss: 391.461792\n",
      "2025-07-08 19:43: Train Epoch 371: 20/37 Loss: 284.997864\n",
      "2025-07-08 19:43: **********Train Epoch 371: averaged Loss: 355.033574\n",
      "2025-07-08 19:43: **********Val Epoch 371: average Loss: 2531.291219\n",
      "2025-07-08 19:43: Train Epoch 372: 0/37 Loss: 2599.551270\n",
      "2025-07-08 19:43: Train Epoch 372: 20/37 Loss: 311.549988\n",
      "2025-07-08 19:43: **********Train Epoch 372: averaged Loss: 682.704466\n",
      "2025-07-08 19:43: **********Val Epoch 372: average Loss: 1300.578695\n",
      "2025-07-08 19:43: Train Epoch 373: 0/37 Loss: 510.216614\n",
      "2025-07-08 19:43: Train Epoch 373: 20/37 Loss: 166.417084\n",
      "2025-07-08 19:43: **********Train Epoch 373: averaged Loss: 321.591283\n",
      "2025-07-08 19:43: **********Val Epoch 373: average Loss: 2568.698161\n",
      "2025-07-08 19:43: Train Epoch 374: 0/37 Loss: 1574.917480\n",
      "2025-07-08 19:43: Train Epoch 374: 20/37 Loss: 121.211395\n",
      "2025-07-08 19:43: **********Train Epoch 374: averaged Loss: 425.297317\n",
      "2025-07-08 19:43: **********Val Epoch 374: average Loss: 927.841054\n",
      "2025-07-08 19:43: Train Epoch 375: 0/37 Loss: 1141.356201\n",
      "2025-07-08 19:43: Train Epoch 375: 20/37 Loss: 235.550278\n",
      "2025-07-08 19:43: **********Train Epoch 375: averaged Loss: 430.929180\n",
      "2025-07-08 19:43: **********Val Epoch 375: average Loss: 2252.237956\n",
      "2025-07-08 19:43: Train Epoch 376: 0/37 Loss: 1992.728394\n",
      "2025-07-08 19:44: Train Epoch 376: 20/37 Loss: 303.022736\n",
      "2025-07-08 19:44: **********Train Epoch 376: averaged Loss: 627.736662\n",
      "2025-07-08 19:44: **********Val Epoch 376: average Loss: 1258.296549\n",
      "2025-07-08 19:44: Train Epoch 377: 0/37 Loss: 2000.682129\n",
      "2025-07-08 19:44: Train Epoch 377: 20/37 Loss: 219.330368\n",
      "2025-07-08 19:44: **********Train Epoch 377: averaged Loss: 643.340083\n",
      "2025-07-08 19:44: **********Val Epoch 377: average Loss: 2313.354451\n",
      "2025-07-08 19:44: Train Epoch 378: 0/37 Loss: 2441.213867\n",
      "2025-07-08 19:44: Train Epoch 378: 20/37 Loss: 439.876404\n",
      "2025-07-08 19:44: **********Train Epoch 378: averaged Loss: 622.240949\n",
      "2025-07-08 19:44: **********Val Epoch 378: average Loss: 1413.382100\n",
      "2025-07-08 19:44: Train Epoch 379: 0/37 Loss: 1146.674316\n",
      "2025-07-08 19:44: Train Epoch 379: 20/37 Loss: 370.910889\n",
      "2025-07-08 19:44: **********Train Epoch 379: averaged Loss: 444.240950\n",
      "2025-07-08 19:44: **********Val Epoch 379: average Loss: 1990.342814\n",
      "2025-07-08 19:44: Train Epoch 380: 0/37 Loss: 1892.531982\n",
      "2025-07-08 19:44: Train Epoch 380: 20/37 Loss: 358.597534\n",
      "2025-07-08 19:44: **********Train Epoch 380: averaged Loss: 496.842015\n",
      "2025-07-08 19:44: **********Val Epoch 380: average Loss: 1792.670736\n",
      "2025-07-08 19:44: Train Epoch 381: 0/37 Loss: 248.813080\n",
      "2025-07-08 19:44: Train Epoch 381: 20/37 Loss: 200.103821\n",
      "2025-07-08 19:44: **********Train Epoch 381: averaged Loss: 231.747228\n",
      "2025-07-08 19:44: **********Val Epoch 381: average Loss: 1438.001587\n",
      "2025-07-08 19:44: Train Epoch 382: 0/37 Loss: 268.648132\n",
      "2025-07-08 19:44: Train Epoch 382: 20/37 Loss: 337.490723\n",
      "2025-07-08 19:44: **********Train Epoch 382: averaged Loss: 267.238836\n",
      "2025-07-08 19:44: **********Val Epoch 382: average Loss: 2080.081462\n",
      "2025-07-08 19:44: Train Epoch 383: 0/37 Loss: 1117.352539\n",
      "2025-07-08 19:44: Train Epoch 383: 20/37 Loss: 140.192123\n",
      "2025-07-08 19:44: **********Train Epoch 383: averaged Loss: 318.345366\n",
      "2025-07-08 19:44: **********Val Epoch 383: average Loss: 1263.025370\n",
      "2025-07-08 19:44: Train Epoch 384: 0/37 Loss: 281.667419\n",
      "2025-07-08 19:44: Train Epoch 384: 20/37 Loss: 210.562393\n",
      "2025-07-08 19:44: **********Train Epoch 384: averaged Loss: 291.079081\n",
      "2025-07-08 19:44: **********Val Epoch 384: average Loss: 2144.885417\n",
      "2025-07-08 19:44: Train Epoch 385: 0/37 Loss: 1157.301514\n",
      "2025-07-08 19:44: Train Epoch 385: 20/37 Loss: 144.811050\n",
      "2025-07-08 19:44: **********Train Epoch 385: averaged Loss: 333.235540\n",
      "2025-07-08 19:44: **********Val Epoch 385: average Loss: 1070.086812\n",
      "2025-07-08 19:44: Train Epoch 386: 0/37 Loss: 395.625305\n",
      "2025-07-08 19:44: Train Epoch 386: 20/37 Loss: 165.418213\n",
      "2025-07-08 19:44: **********Train Epoch 386: averaged Loss: 278.638907\n",
      "2025-07-08 19:44: **********Val Epoch 386: average Loss: 1219.850993\n",
      "2025-07-08 19:44: Train Epoch 387: 0/37 Loss: 171.953064\n",
      "2025-07-08 19:44: Train Epoch 387: 20/37 Loss: 175.153046\n",
      "2025-07-08 19:44: **********Train Epoch 387: averaged Loss: 228.008595\n",
      "2025-07-08 19:44: **********Val Epoch 387: average Loss: 1347.343282\n",
      "2025-07-08 19:44: Train Epoch 388: 0/37 Loss: 298.649353\n",
      "2025-07-08 19:44: Train Epoch 388: 20/37 Loss: 266.464417\n",
      "2025-07-08 19:44: **********Train Epoch 388: averaged Loss: 287.018748\n",
      "2025-07-08 19:44: **********Val Epoch 388: average Loss: 939.775706\n",
      "2025-07-08 19:44: Train Epoch 389: 0/37 Loss: 217.265686\n",
      "2025-07-08 19:44: Train Epoch 389: 20/37 Loss: 158.262634\n",
      "2025-07-08 19:44: **********Train Epoch 389: averaged Loss: 277.707146\n",
      "2025-07-08 19:44: **********Val Epoch 389: average Loss: 1815.041178\n",
      "2025-07-08 19:44: Train Epoch 390: 0/37 Loss: 829.180359\n",
      "2025-07-08 19:44: Train Epoch 390: 20/37 Loss: 325.636230\n",
      "2025-07-08 19:44: **********Train Epoch 390: averaged Loss: 346.210004\n",
      "2025-07-08 19:44: **********Val Epoch 390: average Loss: 938.884257\n",
      "2025-07-08 19:44: Train Epoch 391: 0/37 Loss: 1233.573486\n",
      "2025-07-08 19:44: Train Epoch 391: 20/37 Loss: 316.906647\n",
      "2025-07-08 19:44: **********Train Epoch 391: averaged Loss: 462.570594\n",
      "2025-07-08 19:44: **********Val Epoch 391: average Loss: 2083.379964\n",
      "2025-07-08 19:44: Train Epoch 392: 0/37 Loss: 1024.367188\n",
      "2025-07-08 19:44: Train Epoch 392: 20/37 Loss: 377.166382\n",
      "2025-07-08 19:44: **********Train Epoch 392: averaged Loss: 392.136101\n",
      "2025-07-08 19:44: **********Val Epoch 392: average Loss: 659.046885\n",
      "2025-07-08 19:44: Train Epoch 393: 0/37 Loss: 1713.752319\n",
      "2025-07-08 19:44: Train Epoch 393: 20/37 Loss: 227.961227\n",
      "2025-07-08 19:44: **********Train Epoch 393: averaged Loss: 695.063603\n",
      "2025-07-08 19:44: **********Val Epoch 393: average Loss: 1399.378499\n",
      "2025-07-08 19:44: Train Epoch 394: 0/37 Loss: 1750.661133\n",
      "2025-07-08 19:44: Train Epoch 394: 20/37 Loss: 657.026489\n",
      "2025-07-08 19:44: **********Train Epoch 394: averaged Loss: 674.658492\n",
      "2025-07-08 19:44: **********Val Epoch 394: average Loss: 1657.489746\n",
      "2025-07-08 19:44: Train Epoch 395: 0/37 Loss: 2758.531250\n",
      "2025-07-08 19:44: Train Epoch 395: 20/37 Loss: 188.874359\n",
      "2025-07-08 19:44: **********Train Epoch 395: averaged Loss: 852.991569\n",
      "2025-07-08 19:44: **********Val Epoch 395: average Loss: 2577.827962\n",
      "2025-07-08 19:44: Train Epoch 396: 0/37 Loss: 2508.429443\n",
      "2025-07-08 19:44: Train Epoch 396: 20/37 Loss: 518.136841\n",
      "2025-07-08 19:44: **********Train Epoch 396: averaged Loss: 639.874811\n",
      "2025-07-08 19:44: **********Val Epoch 396: average Loss: 1366.670125\n",
      "2025-07-08 19:44: Train Epoch 397: 0/37 Loss: 502.068909\n",
      "2025-07-08 19:44: Train Epoch 397: 20/37 Loss: 337.163696\n",
      "2025-07-08 19:44: **********Train Epoch 397: averaged Loss: 486.548003\n",
      "2025-07-08 19:44: **********Val Epoch 397: average Loss: 1749.675252\n",
      "2025-07-08 19:44: Train Epoch 398: 0/37 Loss: 2191.125732\n",
      "2025-07-08 19:44: Train Epoch 398: 20/37 Loss: 206.579224\n",
      "2025-07-08 19:44: **********Train Epoch 398: averaged Loss: 694.208718\n",
      "2025-07-08 19:44: **********Val Epoch 398: average Loss: 1090.073161\n",
      "2025-07-08 19:44: Train Epoch 399: 0/37 Loss: 2061.045410\n",
      "2025-07-08 19:44: Train Epoch 399: 20/37 Loss: 454.595398\n",
      "2025-07-08 19:44: **********Train Epoch 399: averaged Loss: 755.923581\n",
      "2025-07-08 19:44: **********Val Epoch 399: average Loss: 3085.492269\n",
      "2025-07-08 19:44: Train Epoch 400: 0/37 Loss: 2715.144531\n",
      "2025-07-08 19:44: Train Epoch 400: 20/37 Loss: 275.730042\n",
      "2025-07-08 19:44: **********Train Epoch 400: averaged Loss: 771.984723\n",
      "2025-07-08 19:44: **********Val Epoch 400: average Loss: 1787.384115\n",
      "2025-07-08 19:44: Train Epoch 401: 0/37 Loss: 2455.013672\n",
      "2025-07-08 19:44: Train Epoch 401: 20/37 Loss: 399.794006\n",
      "2025-07-08 19:44: **********Train Epoch 401: averaged Loss: 695.222021\n",
      "2025-07-08 19:44: **********Val Epoch 401: average Loss: 1665.676310\n",
      "2025-07-08 19:44: Train Epoch 402: 0/37 Loss: 1976.626953\n",
      "2025-07-08 19:44: Train Epoch 402: 20/37 Loss: 433.786011\n",
      "2025-07-08 19:44: **********Train Epoch 402: averaged Loss: 575.469990\n",
      "2025-07-08 19:44: **********Val Epoch 402: average Loss: 2148.071859\n",
      "2025-07-08 19:44: Train Epoch 403: 0/37 Loss: 1961.249756\n",
      "2025-07-08 19:44: Train Epoch 403: 20/37 Loss: 495.977753\n",
      "2025-07-08 19:44: **********Train Epoch 403: averaged Loss: 524.471305\n",
      "2025-07-08 19:44: **********Val Epoch 403: average Loss: 1368.033386\n",
      "2025-07-08 19:44: Train Epoch 404: 0/37 Loss: 588.552185\n",
      "2025-07-08 19:44: Train Epoch 404: 20/37 Loss: 347.253662\n",
      "2025-07-08 19:44: **********Train Epoch 404: averaged Loss: 387.208052\n",
      "2025-07-08 19:44: **********Val Epoch 404: average Loss: 2280.188517\n",
      "2025-07-08 19:44: Train Epoch 405: 0/37 Loss: 2488.822998\n",
      "2025-07-08 19:44: Train Epoch 405: 20/37 Loss: 182.177841\n",
      "2025-07-08 19:44: **********Train Epoch 405: averaged Loss: 676.356715\n",
      "2025-07-08 19:44: **********Val Epoch 405: average Loss: 1231.364115\n",
      "2025-07-08 19:44: Train Epoch 406: 0/37 Loss: 573.169678\n",
      "2025-07-08 19:44: Train Epoch 406: 20/37 Loss: 164.906326\n",
      "2025-07-08 19:44: **********Train Epoch 406: averaged Loss: 356.270474\n",
      "2025-07-08 19:44: **********Val Epoch 406: average Loss: 2655.090169\n",
      "2025-07-08 19:44: Train Epoch 407: 0/37 Loss: 1272.133545\n",
      "2025-07-08 19:44: Train Epoch 407: 20/37 Loss: 392.635590\n",
      "2025-07-08 19:44: **********Train Epoch 407: averaged Loss: 429.530520\n",
      "2025-07-08 19:44: **********Val Epoch 407: average Loss: 910.523478\n",
      "2025-07-08 19:44: Train Epoch 408: 0/37 Loss: 1944.597168\n",
      "2025-07-08 19:44: Train Epoch 408: 20/37 Loss: 174.763412\n",
      "2025-07-08 19:44: **********Train Epoch 408: averaged Loss: 659.942110\n",
      "2025-07-08 19:44: **********Val Epoch 408: average Loss: 2434.202799\n",
      "2025-07-08 19:44: Train Epoch 409: 0/37 Loss: 1629.362549\n",
      "2025-07-08 19:44: Train Epoch 409: 20/37 Loss: 152.903915\n",
      "2025-07-08 19:44: **********Train Epoch 409: averaged Loss: 407.471122\n",
      "2025-07-08 19:44: **********Val Epoch 409: average Loss: 1548.041138\n",
      "2025-07-08 19:44: Train Epoch 410: 0/37 Loss: 877.771484\n",
      "2025-07-08 19:44: Train Epoch 410: 20/37 Loss: 164.253906\n",
      "2025-07-08 19:44: **********Train Epoch 410: averaged Loss: 369.107713\n",
      "2025-07-08 19:44: **********Val Epoch 410: average Loss: 2868.641357\n",
      "2025-07-08 19:44: Train Epoch 411: 0/37 Loss: 2107.858887\n",
      "2025-07-08 19:44: Train Epoch 411: 20/37 Loss: 205.014572\n",
      "2025-07-08 19:44: **********Train Epoch 411: averaged Loss: 511.404003\n",
      "2025-07-08 19:44: **********Val Epoch 411: average Loss: 786.700307\n",
      "2025-07-08 19:44: Train Epoch 412: 0/37 Loss: 1206.886963\n",
      "2025-07-08 19:45: Train Epoch 412: 20/37 Loss: 283.487793\n",
      "2025-07-08 19:45: **********Train Epoch 412: averaged Loss: 659.060112\n",
      "2025-07-08 19:45: **********Val Epoch 412: average Loss: 2309.373657\n",
      "2025-07-08 19:45: Train Epoch 413: 0/37 Loss: 3301.151855\n",
      "2025-07-08 19:45: Train Epoch 413: 20/37 Loss: 288.674683\n",
      "2025-07-08 19:45: **********Train Epoch 413: averaged Loss: 1525.545385\n",
      "2025-07-08 19:45: **********Val Epoch 413: average Loss: 3415.811198\n",
      "2025-07-08 19:45: Train Epoch 414: 0/37 Loss: 1595.290283\n",
      "2025-07-08 19:45: Train Epoch 414: 20/37 Loss: 597.345093\n",
      "2025-07-08 19:45: **********Train Epoch 414: averaged Loss: 1144.260525\n",
      "2025-07-08 19:45: **********Val Epoch 414: average Loss: 1996.787069\n",
      "2025-07-08 19:45: Train Epoch 415: 0/37 Loss: 7578.542969\n",
      "2025-07-08 19:45: Train Epoch 415: 20/37 Loss: 389.704651\n",
      "2025-07-08 19:45: **********Train Epoch 415: averaged Loss: 1882.770844\n",
      "2025-07-08 19:45: **********Val Epoch 415: average Loss: 932.107137\n",
      "2025-07-08 19:45: Train Epoch 416: 0/37 Loss: 528.937744\n",
      "2025-07-08 19:45: Train Epoch 416: 20/37 Loss: 263.508484\n",
      "2025-07-08 19:45: **********Train Epoch 416: averaged Loss: 314.582250\n",
      "2025-07-08 19:45: **********Val Epoch 416: average Loss: 1436.293132\n",
      "2025-07-08 19:45: Train Epoch 417: 0/37 Loss: 989.296082\n",
      "2025-07-08 19:45: Train Epoch 417: 20/37 Loss: 503.833160\n",
      "2025-07-08 19:45: **********Train Epoch 417: averaged Loss: 678.426731\n",
      "2025-07-08 19:45: **********Val Epoch 417: average Loss: 1839.129089\n",
      "2025-07-08 19:45: Train Epoch 418: 0/37 Loss: 2640.870850\n",
      "2025-07-08 19:45: Train Epoch 418: 20/37 Loss: 179.353424\n",
      "2025-07-08 19:45: **********Train Epoch 418: averaged Loss: 676.069547\n",
      "2025-07-08 19:45: **********Val Epoch 418: average Loss: 1667.915466\n",
      "2025-07-08 19:45: Train Epoch 419: 0/37 Loss: 1035.313477\n",
      "2025-07-08 19:45: Train Epoch 419: 20/37 Loss: 425.198914\n",
      "2025-07-08 19:45: **********Train Epoch 419: averaged Loss: 396.934744\n",
      "2025-07-08 19:45: **********Val Epoch 419: average Loss: 3311.436768\n",
      "2025-07-08 19:45: Train Epoch 420: 0/37 Loss: 2437.281982\n",
      "2025-07-08 19:45: Train Epoch 420: 20/37 Loss: 354.544922\n",
      "2025-07-08 19:45: **********Train Epoch 420: averaged Loss: 740.502080\n",
      "2025-07-08 19:45: **********Val Epoch 420: average Loss: 1309.559245\n",
      "2025-07-08 19:45: Train Epoch 421: 0/37 Loss: 318.902863\n",
      "2025-07-08 19:45: Train Epoch 421: 20/37 Loss: 414.300964\n",
      "2025-07-08 19:45: **********Train Epoch 421: averaged Loss: 317.087315\n",
      "2025-07-08 19:45: **********Val Epoch 421: average Loss: 3171.048665\n",
      "2025-07-08 19:45: Train Epoch 422: 0/37 Loss: 3150.992188\n",
      "2025-07-08 19:45: Train Epoch 422: 20/37 Loss: 142.833023\n",
      "2025-07-08 19:45: **********Train Epoch 422: averaged Loss: 939.538221\n",
      "2025-07-08 19:45: **********Val Epoch 422: average Loss: 1734.985840\n",
      "2025-07-08 19:45: Train Epoch 423: 0/37 Loss: 1494.103760\n",
      "2025-07-08 19:45: Train Epoch 423: 20/37 Loss: 332.452209\n",
      "2025-07-08 19:45: **********Train Epoch 423: averaged Loss: 471.383311\n",
      "2025-07-08 19:45: **********Val Epoch 423: average Loss: 595.110769\n",
      "2025-07-08 19:45: Train Epoch 424: 0/37 Loss: 205.222000\n",
      "2025-07-08 19:45: Train Epoch 424: 20/37 Loss: 462.067963\n",
      "2025-07-08 19:45: **********Train Epoch 424: averaged Loss: 501.044983\n",
      "2025-07-08 19:45: **********Val Epoch 424: average Loss: 2049.097412\n",
      "2025-07-08 19:45: Train Epoch 425: 0/37 Loss: 3694.509277\n",
      "2025-07-08 19:45: Train Epoch 425: 20/37 Loss: 505.380341\n",
      "2025-07-08 19:45: **********Train Epoch 425: averaged Loss: 1146.149466\n",
      "2025-07-08 19:45: **********Val Epoch 425: average Loss: 1970.447591\n",
      "2025-07-08 19:45: Train Epoch 426: 0/37 Loss: 296.658691\n",
      "2025-07-08 19:45: Train Epoch 426: 20/37 Loss: 208.363007\n",
      "2025-07-08 19:45: **********Train Epoch 426: averaged Loss: 369.493790\n",
      "2025-07-08 19:45: **********Val Epoch 426: average Loss: 1126.207967\n",
      "2025-07-08 19:45: Train Epoch 427: 0/37 Loss: 418.378113\n",
      "2025-07-08 19:45: Train Epoch 427: 20/37 Loss: 289.157043\n",
      "2025-07-08 19:45: **********Train Epoch 427: averaged Loss: 351.208355\n",
      "2025-07-08 19:45: **********Val Epoch 427: average Loss: 2706.038411\n",
      "2025-07-08 19:45: Train Epoch 428: 0/37 Loss: 2638.985840\n",
      "2025-07-08 19:45: Train Epoch 428: 20/37 Loss: 156.827820\n",
      "2025-07-08 19:45: **********Train Epoch 428: averaged Loss: 714.150511\n",
      "2025-07-08 19:45: **********Val Epoch 428: average Loss: 1244.291280\n",
      "2025-07-08 19:45: Train Epoch 429: 0/37 Loss: 192.898544\n",
      "2025-07-08 19:45: Train Epoch 429: 20/37 Loss: 171.936050\n",
      "2025-07-08 19:45: **********Train Epoch 429: averaged Loss: 268.891548\n",
      "2025-07-08 19:45: **********Val Epoch 429: average Loss: 2354.686849\n",
      "2025-07-08 19:45: Train Epoch 430: 0/37 Loss: 823.817749\n",
      "2025-07-08 19:45: Train Epoch 430: 20/37 Loss: 292.638977\n",
      "2025-07-08 19:45: **********Train Epoch 430: averaged Loss: 338.310899\n",
      "2025-07-08 19:45: **********Val Epoch 430: average Loss: 1197.324809\n",
      "2025-07-08 19:45: Train Epoch 431: 0/37 Loss: 716.067261\n",
      "2025-07-08 19:45: Train Epoch 431: 20/37 Loss: 323.326019\n",
      "2025-07-08 19:45: **********Train Epoch 431: averaged Loss: 373.205072\n",
      "2025-07-08 19:45: **********Val Epoch 431: average Loss: 1167.346293\n",
      "2025-07-08 19:45: Train Epoch 432: 0/37 Loss: 213.850586\n",
      "2025-07-08 19:45: Train Epoch 432: 20/37 Loss: 146.989914\n",
      "2025-07-08 19:45: **********Train Epoch 432: averaged Loss: 243.637611\n",
      "2025-07-08 19:45: **********Val Epoch 432: average Loss: 1590.059489\n",
      "2025-07-08 19:45: Train Epoch 433: 0/37 Loss: 445.702209\n",
      "2025-07-08 19:45: Train Epoch 433: 20/37 Loss: 268.825928\n",
      "2025-07-08 19:45: **********Train Epoch 433: averaged Loss: 321.822472\n",
      "2025-07-08 19:45: **********Val Epoch 433: average Loss: 958.807821\n",
      "2025-07-08 19:45: Train Epoch 434: 0/37 Loss: 196.341370\n",
      "2025-07-08 19:45: Train Epoch 434: 20/37 Loss: 130.673752\n",
      "2025-07-08 19:45: **********Train Epoch 434: averaged Loss: 218.804595\n",
      "2025-07-08 19:45: **********Val Epoch 434: average Loss: 867.009888\n",
      "2025-07-08 19:45: Train Epoch 435: 0/37 Loss: 260.414429\n",
      "2025-07-08 19:45: Train Epoch 435: 20/37 Loss: 195.036880\n",
      "2025-07-08 19:45: **********Train Epoch 435: averaged Loss: 356.076464\n",
      "2025-07-08 19:45: **********Val Epoch 435: average Loss: 1697.023234\n",
      "2025-07-08 19:45: Train Epoch 436: 0/37 Loss: 1301.224854\n",
      "2025-07-08 19:45: Train Epoch 436: 20/37 Loss: 281.777924\n",
      "2025-07-08 19:45: **********Train Epoch 436: averaged Loss: 381.490060\n",
      "2025-07-08 19:45: **********Val Epoch 436: average Loss: 1813.131104\n",
      "2025-07-08 19:45: Train Epoch 437: 0/37 Loss: 1565.973511\n",
      "2025-07-08 19:45: Train Epoch 437: 20/37 Loss: 379.259338\n",
      "2025-07-08 19:45: **********Train Epoch 437: averaged Loss: 555.501988\n",
      "2025-07-08 19:45: **********Val Epoch 437: average Loss: 2120.681681\n",
      "2025-07-08 19:45: Train Epoch 438: 0/37 Loss: 220.141724\n",
      "2025-07-08 19:45: Train Epoch 438: 20/37 Loss: 327.950165\n",
      "2025-07-08 19:45: **********Train Epoch 438: averaged Loss: 310.315047\n",
      "2025-07-08 19:45: **********Val Epoch 438: average Loss: 1604.507955\n",
      "2025-07-08 19:45: Train Epoch 439: 0/37 Loss: 227.763229\n",
      "2025-07-08 19:45: Train Epoch 439: 20/37 Loss: 172.895615\n",
      "2025-07-08 19:45: **********Train Epoch 439: averaged Loss: 332.175768\n",
      "2025-07-08 19:45: **********Val Epoch 439: average Loss: 1102.646729\n",
      "2025-07-08 19:45: Train Epoch 440: 0/37 Loss: 423.757385\n",
      "2025-07-08 19:45: Train Epoch 440: 20/37 Loss: 195.152725\n",
      "2025-07-08 19:45: **********Train Epoch 440: averaged Loss: 464.214152\n",
      "2025-07-08 19:45: **********Val Epoch 440: average Loss: 1966.988281\n",
      "2025-07-08 19:45: Train Epoch 441: 0/37 Loss: 1957.771240\n",
      "2025-07-08 19:45: Train Epoch 441: 20/37 Loss: 415.423096\n",
      "2025-07-08 19:45: **********Train Epoch 441: averaged Loss: 559.150619\n",
      "2025-07-08 19:45: **********Val Epoch 441: average Loss: 1969.945638\n",
      "2025-07-08 19:45: Train Epoch 442: 0/37 Loss: 344.105377\n",
      "2025-07-08 19:45: Train Epoch 442: 20/37 Loss: 250.759979\n",
      "2025-07-08 19:45: **********Train Epoch 442: averaged Loss: 324.269589\n",
      "2025-07-08 19:45: **********Val Epoch 442: average Loss: 1576.629883\n",
      "2025-07-08 19:45: Train Epoch 443: 0/37 Loss: 339.496765\n",
      "2025-07-08 19:45: Train Epoch 443: 20/37 Loss: 126.154907\n",
      "2025-07-08 19:45: **********Train Epoch 443: averaged Loss: 255.720474\n",
      "2025-07-08 19:45: **********Val Epoch 443: average Loss: 1560.613322\n",
      "2025-07-08 19:45: Train Epoch 444: 0/37 Loss: 475.409729\n",
      "2025-07-08 19:45: Train Epoch 444: 20/37 Loss: 248.463120\n",
      "2025-07-08 19:45: **********Train Epoch 444: averaged Loss: 387.370388\n",
      "2025-07-08 19:45: **********Val Epoch 444: average Loss: 2729.367513\n",
      "2025-07-08 19:45: Train Epoch 445: 0/37 Loss: 2333.153076\n",
      "2025-07-08 19:45: Train Epoch 445: 20/37 Loss: 287.532257\n",
      "2025-07-08 19:45: **********Train Epoch 445: averaged Loss: 625.821280\n",
      "2025-07-08 19:45: **********Val Epoch 445: average Loss: 1119.054443\n",
      "2025-07-08 19:45: Train Epoch 446: 0/37 Loss: 571.826111\n",
      "2025-07-08 19:45: Train Epoch 446: 20/37 Loss: 216.452133\n",
      "2025-07-08 19:45: **********Train Epoch 446: averaged Loss: 273.050371\n",
      "2025-07-08 19:45: **********Val Epoch 446: average Loss: 1935.054016\n",
      "2025-07-08 19:45: Train Epoch 447: 0/37 Loss: 1725.059326\n",
      "2025-07-08 19:45: Train Epoch 447: 20/37 Loss: 483.263702\n",
      "2025-07-08 19:45: **********Train Epoch 447: averaged Loss: 536.550527\n",
      "2025-07-08 19:45: **********Val Epoch 447: average Loss: 1387.195435\n",
      "2025-07-08 19:45: Train Epoch 448: 0/37 Loss: 1112.643188\n",
      "2025-07-08 19:46: Train Epoch 448: 20/37 Loss: 536.854248\n",
      "2025-07-08 19:46: **********Train Epoch 448: averaged Loss: 491.609826\n",
      "2025-07-08 19:46: **********Val Epoch 448: average Loss: 2816.707723\n",
      "2025-07-08 19:46: Train Epoch 449: 0/37 Loss: 3012.184082\n",
      "2025-07-08 19:46: Train Epoch 449: 20/37 Loss: 113.001816\n",
      "2025-07-08 19:46: **********Train Epoch 449: averaged Loss: 928.466377\n",
      "2025-07-08 19:46: **********Val Epoch 449: average Loss: 1453.001790\n",
      "2025-07-08 19:46: Train Epoch 450: 0/37 Loss: 332.239807\n",
      "2025-07-08 19:46: Train Epoch 450: 20/37 Loss: 132.698502\n",
      "2025-07-08 19:46: **********Train Epoch 450: averaged Loss: 370.151361\n",
      "2025-07-08 19:46: **********Val Epoch 450: average Loss: 966.551951\n",
      "2025-07-08 19:46: Train Epoch 451: 0/37 Loss: 324.831116\n",
      "2025-07-08 19:46: Train Epoch 451: 20/37 Loss: 383.148071\n",
      "2025-07-08 19:46: **********Train Epoch 451: averaged Loss: 406.793182\n",
      "2025-07-08 19:46: **********Val Epoch 451: average Loss: 2586.747640\n",
      "2025-07-08 19:46: Train Epoch 452: 0/37 Loss: 2773.979980\n",
      "2025-07-08 19:46: Train Epoch 452: 20/37 Loss: 192.226044\n",
      "2025-07-08 19:46: **********Train Epoch 452: averaged Loss: 835.815339\n",
      "2025-07-08 19:46: **********Val Epoch 452: average Loss: 1498.273722\n",
      "2025-07-08 19:46: Train Epoch 453: 0/37 Loss: 408.451111\n",
      "2025-07-08 19:46: Train Epoch 453: 20/37 Loss: 265.588135\n",
      "2025-07-08 19:46: **********Train Epoch 453: averaged Loss: 407.744757\n",
      "2025-07-08 19:46: **********Val Epoch 453: average Loss: 661.616618\n",
      "2025-07-08 19:46: Train Epoch 454: 0/37 Loss: 402.737061\n",
      "2025-07-08 19:46: Train Epoch 454: 20/37 Loss: 284.336426\n",
      "2025-07-08 19:46: **********Train Epoch 454: averaged Loss: 331.418829\n",
      "2025-07-08 19:46: **********Val Epoch 454: average Loss: 2442.457804\n",
      "2025-07-08 19:46: Train Epoch 455: 0/37 Loss: 2609.588379\n",
      "2025-07-08 19:46: Train Epoch 455: 20/37 Loss: 194.574524\n",
      "2025-07-08 19:46: **********Train Epoch 455: averaged Loss: 695.719091\n",
      "2025-07-08 19:46: **********Val Epoch 455: average Loss: 796.285126\n",
      "2025-07-08 19:46: Train Epoch 456: 0/37 Loss: 585.614441\n",
      "2025-07-08 19:46: Train Epoch 456: 20/37 Loss: 178.698288\n",
      "2025-07-08 19:46: **********Train Epoch 456: averaged Loss: 369.943820\n",
      "2025-07-08 19:46: **********Val Epoch 456: average Loss: 2145.024658\n",
      "2025-07-08 19:46: Train Epoch 457: 0/37 Loss: 1636.985352\n",
      "2025-07-08 19:46: Train Epoch 457: 20/37 Loss: 326.548706\n",
      "2025-07-08 19:46: **********Train Epoch 457: averaged Loss: 460.400434\n",
      "2025-07-08 19:46: **********Val Epoch 457: average Loss: 819.299388\n",
      "2025-07-08 19:46: Train Epoch 458: 0/37 Loss: 2111.082764\n",
      "2025-07-08 19:46: Train Epoch 458: 20/37 Loss: 160.616989\n",
      "2025-07-08 19:46: **********Train Epoch 458: averaged Loss: 711.078253\n",
      "2025-07-08 19:46: **********Val Epoch 458: average Loss: 2150.993978\n",
      "2025-07-08 19:46: Train Epoch 459: 0/37 Loss: 1240.538086\n",
      "2025-07-08 19:46: Train Epoch 459: 20/37 Loss: 123.155701\n",
      "2025-07-08 19:46: **********Train Epoch 459: averaged Loss: 360.594223\n",
      "2025-07-08 19:46: **********Val Epoch 459: average Loss: 2228.375732\n",
      "2025-07-08 19:46: Train Epoch 460: 0/37 Loss: 342.167480\n",
      "2025-07-08 19:46: Train Epoch 460: 20/37 Loss: 135.942459\n",
      "2025-07-08 19:46: **********Train Epoch 460: averaged Loss: 326.743483\n",
      "2025-07-08 19:46: **********Val Epoch 460: average Loss: 1198.754679\n",
      "2025-07-08 19:46: Train Epoch 461: 0/37 Loss: 347.825500\n",
      "2025-07-08 19:46: Train Epoch 461: 20/37 Loss: 131.472916\n",
      "2025-07-08 19:46: **********Train Epoch 461: averaged Loss: 276.895707\n",
      "2025-07-08 19:46: **********Val Epoch 461: average Loss: 1548.943563\n",
      "2025-07-08 19:46: Train Epoch 462: 0/37 Loss: 1206.314087\n",
      "2025-07-08 19:46: Train Epoch 462: 20/37 Loss: 473.361298\n",
      "2025-07-08 19:46: **********Train Epoch 462: averaged Loss: 444.151409\n",
      "2025-07-08 19:46: **********Val Epoch 462: average Loss: 2691.705851\n",
      "2025-07-08 19:46: Train Epoch 463: 0/37 Loss: 2107.875000\n",
      "2025-07-08 19:46: Train Epoch 463: 20/37 Loss: 370.489990\n",
      "2025-07-08 19:46: **********Train Epoch 463: averaged Loss: 592.923442\n",
      "2025-07-08 19:46: **********Val Epoch 463: average Loss: 1117.923950\n",
      "2025-07-08 19:46: Train Epoch 464: 0/37 Loss: 184.213364\n",
      "2025-07-08 19:46: Train Epoch 464: 20/37 Loss: 227.003250\n",
      "2025-07-08 19:46: **********Train Epoch 464: averaged Loss: 284.060269\n",
      "2025-07-08 19:46: **********Val Epoch 464: average Loss: 2844.282633\n",
      "2025-07-08 19:46: Train Epoch 465: 0/37 Loss: 1901.163818\n",
      "2025-07-08 19:46: Train Epoch 465: 20/37 Loss: 201.126068\n",
      "2025-07-08 19:46: **********Train Epoch 465: averaged Loss: 452.765591\n",
      "2025-07-08 19:46: **********Val Epoch 465: average Loss: 1147.775574\n",
      "2025-07-08 19:46: Train Epoch 466: 0/37 Loss: 1030.463135\n",
      "2025-07-08 19:46: Train Epoch 466: 20/37 Loss: 228.351791\n",
      "2025-07-08 19:46: **********Train Epoch 466: averaged Loss: 470.858531\n",
      "2025-07-08 19:46: **********Val Epoch 466: average Loss: 2586.391846\n",
      "2025-07-08 19:46: Train Epoch 467: 0/37 Loss: 2719.995117\n",
      "2025-07-08 19:46: Train Epoch 467: 20/37 Loss: 160.475861\n",
      "2025-07-08 19:46: **********Train Epoch 467: averaged Loss: 914.463298\n",
      "2025-07-08 19:46: **********Val Epoch 467: average Loss: 1629.427775\n",
      "2025-07-08 19:46: Train Epoch 468: 0/37 Loss: 1864.170776\n",
      "2025-07-08 19:46: Train Epoch 468: 20/37 Loss: 170.011383\n",
      "2025-07-08 19:46: **********Train Epoch 468: averaged Loss: 597.560105\n",
      "2025-07-08 19:46: **********Val Epoch 468: average Loss: 2821.421631\n",
      "2025-07-08 19:46: Train Epoch 469: 0/37 Loss: 2402.159668\n",
      "2025-07-08 19:46: Train Epoch 469: 20/37 Loss: 353.559937\n",
      "2025-07-08 19:46: **********Train Epoch 469: averaged Loss: 682.030305\n",
      "2025-07-08 19:46: **********Val Epoch 469: average Loss: 616.899872\n",
      "2025-07-08 19:46: Train Epoch 470: 0/37 Loss: 255.113525\n",
      "2025-07-08 19:46: Train Epoch 470: 20/37 Loss: 314.632446\n",
      "2025-07-08 19:46: **********Train Epoch 470: averaged Loss: 323.547683\n",
      "2025-07-08 19:46: **********Val Epoch 470: average Loss: 2657.874593\n",
      "2025-07-08 19:46: Train Epoch 471: 0/37 Loss: 2713.479492\n",
      "2025-07-08 19:46: Train Epoch 471: 20/37 Loss: 139.654572\n",
      "2025-07-08 19:46: **********Train Epoch 471: averaged Loss: 748.536397\n",
      "2025-07-08 19:46: **********Val Epoch 471: average Loss: 995.898936\n",
      "2025-07-08 19:46: Train Epoch 472: 0/37 Loss: 188.719193\n",
      "2025-07-08 19:46: Train Epoch 472: 20/37 Loss: 149.354568\n",
      "2025-07-08 19:46: **********Train Epoch 472: averaged Loss: 249.324566\n",
      "2025-07-08 19:46: **********Val Epoch 472: average Loss: 1433.851440\n",
      "2025-07-08 19:46: Train Epoch 473: 0/37 Loss: 917.846863\n",
      "2025-07-08 19:46: Train Epoch 473: 20/37 Loss: 130.753967\n",
      "2025-07-08 19:46: **********Train Epoch 473: averaged Loss: 303.545459\n",
      "2025-07-08 19:46: **********Val Epoch 473: average Loss: 1194.047811\n",
      "2025-07-08 19:46: Train Epoch 474: 0/37 Loss: 498.490662\n",
      "2025-07-08 19:46: Train Epoch 474: 20/37 Loss: 149.261093\n",
      "2025-07-08 19:46: **********Train Epoch 474: averaged Loss: 279.764330\n",
      "2025-07-08 19:46: **********Val Epoch 474: average Loss: 959.063446\n",
      "2025-07-08 19:46: Train Epoch 475: 0/37 Loss: 509.349243\n",
      "2025-07-08 19:46: Train Epoch 475: 20/37 Loss: 159.093628\n",
      "2025-07-08 19:46: **********Train Epoch 475: averaged Loss: 292.405871\n",
      "2025-07-08 19:46: **********Val Epoch 475: average Loss: 1979.681803\n",
      "2025-07-08 19:46: Train Epoch 476: 0/37 Loss: 1291.540771\n",
      "2025-07-08 19:46: Train Epoch 476: 20/37 Loss: 123.555969\n",
      "2025-07-08 19:46: **********Train Epoch 476: averaged Loss: 349.383015\n",
      "2025-07-08 19:46: **********Val Epoch 476: average Loss: 674.130768\n",
      "2025-07-08 19:46: Train Epoch 477: 0/37 Loss: 756.839722\n",
      "2025-07-08 19:46: Train Epoch 477: 20/37 Loss: 175.170319\n",
      "2025-07-08 19:46: **********Train Epoch 477: averaged Loss: 388.586957\n",
      "2025-07-08 19:46: **********Val Epoch 477: average Loss: 2261.472819\n",
      "2025-07-08 19:46: Train Epoch 478: 0/37 Loss: 1529.245483\n",
      "2025-07-08 19:46: Train Epoch 478: 20/37 Loss: 247.127045\n",
      "2025-07-08 19:46: **********Train Epoch 478: averaged Loss: 402.112444\n",
      "2025-07-08 19:46: **********Val Epoch 478: average Loss: 577.289968\n",
      "2025-07-08 19:46: Train Epoch 479: 0/37 Loss: 1789.589844\n",
      "2025-07-08 19:46: Train Epoch 479: 20/37 Loss: 192.930267\n",
      "2025-07-08 19:46: **********Train Epoch 479: averaged Loss: 700.588060\n",
      "2025-07-08 19:46: **********Val Epoch 479: average Loss: 1440.685669\n",
      "2025-07-08 19:46: Train Epoch 480: 0/37 Loss: 1281.917480\n",
      "2025-07-08 19:46: Train Epoch 480: 20/37 Loss: 309.060425\n",
      "2025-07-08 19:46: **********Train Epoch 480: averaged Loss: 446.669217\n",
      "2025-07-08 19:46: **********Val Epoch 480: average Loss: 1227.908468\n",
      "2025-07-08 19:46: Train Epoch 481: 0/37 Loss: 2174.629883\n",
      "2025-07-08 19:46: Train Epoch 481: 20/37 Loss: 176.873840\n",
      "2025-07-08 19:46: **********Train Epoch 481: averaged Loss: 688.223080\n",
      "2025-07-08 19:46: **********Val Epoch 481: average Loss: 2549.195312\n",
      "2025-07-08 19:46: Train Epoch 482: 0/37 Loss: 1747.501953\n",
      "2025-07-08 19:46: Train Epoch 482: 20/37 Loss: 223.909454\n",
      "2025-07-08 19:46: **********Train Epoch 482: averaged Loss: 617.260235\n",
      "2025-07-08 19:46: **********Val Epoch 482: average Loss: 1094.416911\n",
      "2025-07-08 19:46: Train Epoch 483: 0/37 Loss: 1792.791992\n",
      "2025-07-08 19:46: Train Epoch 483: 20/37 Loss: 161.758194\n",
      "2025-07-08 19:46: **********Train Epoch 483: averaged Loss: 516.986203\n",
      "2025-07-08 19:46: **********Val Epoch 483: average Loss: 2544.571086\n",
      "2025-07-08 19:46: Train Epoch 484: 0/37 Loss: 2532.085449\n",
      "2025-07-08 19:47: Train Epoch 484: 20/37 Loss: 391.846130\n",
      "2025-07-08 19:47: **********Train Epoch 484: averaged Loss: 807.666872\n",
      "2025-07-08 19:47: **********Val Epoch 484: average Loss: 680.172628\n",
      "2025-07-08 19:47: Train Epoch 485: 0/37 Loss: 225.043564\n",
      "2025-07-08 19:47: Train Epoch 485: 20/37 Loss: 479.293854\n",
      "2025-07-08 19:47: **********Train Epoch 485: averaged Loss: 355.051381\n",
      "2025-07-08 19:47: **********Val Epoch 485: average Loss: 2777.930257\n",
      "2025-07-08 19:47: Train Epoch 486: 0/37 Loss: 3105.390137\n",
      "2025-07-08 19:47: Train Epoch 486: 20/37 Loss: 131.626602\n",
      "2025-07-08 19:47: **********Train Epoch 486: averaged Loss: 882.012068\n",
      "2025-07-08 19:47: **********Val Epoch 486: average Loss: 1169.374715\n",
      "2025-07-08 19:47: Train Epoch 487: 0/37 Loss: 684.892273\n",
      "2025-07-08 19:47: Train Epoch 487: 20/37 Loss: 143.757629\n",
      "2025-07-08 19:47: **********Train Epoch 487: averaged Loss: 365.827519\n",
      "2025-07-08 19:47: **********Val Epoch 487: average Loss: 774.147064\n",
      "2025-07-08 19:47: Train Epoch 488: 0/37 Loss: 516.921997\n",
      "2025-07-08 19:47: Train Epoch 488: 20/37 Loss: 471.702515\n",
      "2025-07-08 19:47: **********Train Epoch 488: averaged Loss: 416.339212\n",
      "2025-07-08 19:47: **********Val Epoch 488: average Loss: 2246.264445\n",
      "2025-07-08 19:47: Train Epoch 489: 0/37 Loss: 2866.792725\n",
      "2025-07-08 19:47: Train Epoch 489: 20/37 Loss: 273.354919\n",
      "2025-07-08 19:47: **********Train Epoch 489: averaged Loss: 807.061881\n",
      "2025-07-08 19:47: **********Val Epoch 489: average Loss: 1507.958862\n",
      "2025-07-08 19:47: Train Epoch 490: 0/37 Loss: 773.373596\n",
      "2025-07-08 19:47: Train Epoch 490: 20/37 Loss: 224.486008\n",
      "2025-07-08 19:47: **********Train Epoch 490: averaged Loss: 358.246725\n",
      "2025-07-08 19:47: **********Val Epoch 490: average Loss: 715.158193\n",
      "2025-07-08 19:47: Train Epoch 491: 0/37 Loss: 434.668945\n",
      "2025-07-08 19:47: Train Epoch 491: 20/37 Loss: 212.199997\n",
      "2025-07-08 19:47: **********Train Epoch 491: averaged Loss: 288.942516\n",
      "2025-07-08 19:47: **********Val Epoch 491: average Loss: 2438.823690\n",
      "2025-07-08 19:47: Train Epoch 492: 0/37 Loss: 2005.820312\n",
      "2025-07-08 19:47: Train Epoch 492: 20/37 Loss: 250.378006\n",
      "2025-07-08 19:47: **********Train Epoch 492: averaged Loss: 491.371968\n",
      "2025-07-08 19:47: **********Val Epoch 492: average Loss: 644.319489\n",
      "2025-07-08 19:47: Train Epoch 493: 0/37 Loss: 1013.990112\n",
      "2025-07-08 19:47: Train Epoch 493: 20/37 Loss: 227.404984\n",
      "2025-07-08 19:47: **********Train Epoch 493: averaged Loss: 556.032093\n",
      "2025-07-08 19:47: **********Val Epoch 493: average Loss: 1833.248332\n",
      "2025-07-08 19:47: Train Epoch 494: 0/37 Loss: 2585.830811\n",
      "2025-07-08 19:47: Train Epoch 494: 20/37 Loss: 214.890152\n",
      "2025-07-08 19:47: **********Train Epoch 494: averaged Loss: 1083.612889\n",
      "2025-07-08 19:47: **********Val Epoch 494: average Loss: 2873.118652\n",
      "2025-07-08 19:47: Train Epoch 495: 0/37 Loss: 1900.702881\n",
      "2025-07-08 19:47: Train Epoch 495: 20/37 Loss: 203.557114\n",
      "2025-07-08 19:47: **********Train Epoch 495: averaged Loss: 687.328145\n",
      "2025-07-08 19:47: **********Val Epoch 495: average Loss: 2475.298991\n",
      "2025-07-08 19:47: Train Epoch 496: 0/37 Loss: 2664.426758\n",
      "2025-07-08 19:47: Train Epoch 496: 20/37 Loss: 138.949020\n",
      "2025-07-08 19:47: **********Train Epoch 496: averaged Loss: 695.834485\n",
      "2025-07-08 19:47: **********Val Epoch 496: average Loss: 1187.338786\n",
      "2025-07-08 19:47: Train Epoch 497: 0/37 Loss: 1658.366333\n",
      "2025-07-08 19:47: Train Epoch 497: 20/37 Loss: 155.692230\n",
      "2025-07-08 19:47: **********Train Epoch 497: averaged Loss: 491.341235\n",
      "2025-07-08 19:47: **********Val Epoch 497: average Loss: 3127.212484\n",
      "2025-07-08 19:47: Train Epoch 498: 0/37 Loss: 2491.003906\n",
      "2025-07-08 19:47: Train Epoch 498: 20/37 Loss: 328.402191\n",
      "2025-07-08 19:47: **********Train Epoch 498: averaged Loss: 722.535700\n",
      "2025-07-08 19:47: **********Val Epoch 498: average Loss: 1023.993113\n",
      "2025-07-08 19:47: Train Epoch 499: 0/37 Loss: 224.847961\n",
      "2025-07-08 19:47: Train Epoch 499: 20/37 Loss: 238.111984\n",
      "2025-07-08 19:47: **********Train Epoch 499: averaged Loss: 308.716945\n",
      "2025-07-08 19:47: **********Val Epoch 499: average Loss: 2279.015055\n",
      "2025-07-08 19:47: Train Epoch 500: 0/37 Loss: 2481.973145\n",
      "2025-07-08 19:47: Train Epoch 500: 20/37 Loss: 226.841553\n",
      "2025-07-08 19:47: **********Train Epoch 500: averaged Loss: 815.473022\n",
      "2025-07-08 19:47: **********Val Epoch 500: average Loss: 898.886169\n",
      "2025-07-08 19:47: Train Epoch 501: 0/37 Loss: 987.936523\n",
      "2025-07-08 19:47: Train Epoch 501: 20/37 Loss: 243.486801\n",
      "2025-07-08 19:47: **********Train Epoch 501: averaged Loss: 380.557585\n",
      "2025-07-08 19:47: **********Val Epoch 501: average Loss: 812.855845\n",
      "2025-07-08 19:47: Train Epoch 502: 0/37 Loss: 1515.114258\n",
      "2025-07-08 19:47: Train Epoch 502: 20/37 Loss: 284.600952\n",
      "2025-07-08 19:47: **********Train Epoch 502: averaged Loss: 498.653017\n",
      "2025-07-08 19:47: **********Val Epoch 502: average Loss: 1473.367391\n",
      "2025-07-08 19:47: Train Epoch 503: 0/37 Loss: 2570.436279\n",
      "2025-07-08 19:47: Train Epoch 503: 20/37 Loss: 118.214630\n",
      "2025-07-08 19:47: **********Train Epoch 503: averaged Loss: 803.133435\n",
      "2025-07-08 19:47: **********Val Epoch 503: average Loss: 453.926951\n",
      "2025-07-08 19:47: Train Epoch 504: 0/37 Loss: 255.269943\n",
      "2025-07-08 19:47: Train Epoch 504: 20/37 Loss: 178.984543\n",
      "2025-07-08 19:47: **********Train Epoch 504: averaged Loss: 288.897900\n",
      "2025-07-08 19:47: **********Val Epoch 504: average Loss: 1160.597026\n",
      "2025-07-08 19:47: Train Epoch 505: 0/37 Loss: 1246.835449\n",
      "2025-07-08 19:47: Train Epoch 505: 20/37 Loss: 152.639221\n",
      "2025-07-08 19:47: **********Train Epoch 505: averaged Loss: 366.761144\n",
      "2025-07-08 19:47: **********Val Epoch 505: average Loss: 474.529897\n",
      "2025-07-08 19:47: Train Epoch 506: 0/37 Loss: 625.525391\n",
      "2025-07-08 19:47: Train Epoch 506: 20/37 Loss: 393.666016\n",
      "2025-07-08 19:47: **********Train Epoch 506: averaged Loss: 408.785887\n",
      "2025-07-08 19:47: **********Val Epoch 506: average Loss: 662.666321\n",
      "2025-07-08 19:47: Train Epoch 507: 0/37 Loss: 198.082352\n",
      "2025-07-08 19:47: Train Epoch 507: 20/37 Loss: 330.351379\n",
      "2025-07-08 19:47: **********Train Epoch 507: averaged Loss: 264.213205\n",
      "2025-07-08 19:47: **********Val Epoch 507: average Loss: 311.828644\n",
      "2025-07-08 19:47: *********************************Current best model saved!\n",
      "2025-07-08 19:47: Train Epoch 508: 0/37 Loss: 245.217819\n",
      "2025-07-08 19:47: Train Epoch 508: 20/37 Loss: 314.175507\n",
      "2025-07-08 19:47: **********Train Epoch 508: averaged Loss: 279.031057\n",
      "2025-07-08 19:47: **********Val Epoch 508: average Loss: 459.999023\n",
      "2025-07-08 19:47: Train Epoch 509: 0/37 Loss: 206.355362\n",
      "2025-07-08 19:47: Train Epoch 509: 20/37 Loss: 311.581787\n",
      "2025-07-08 19:47: **********Train Epoch 509: averaged Loss: 357.395077\n",
      "2025-07-08 19:47: **********Val Epoch 509: average Loss: 376.567434\n",
      "2025-07-08 19:47: Train Epoch 510: 0/37 Loss: 230.877563\n",
      "2025-07-08 19:47: Train Epoch 510: 20/37 Loss: 231.075775\n",
      "2025-07-08 19:47: **********Train Epoch 510: averaged Loss: 428.155930\n",
      "2025-07-08 19:47: **********Val Epoch 510: average Loss: 476.761424\n",
      "2025-07-08 19:47: Train Epoch 511: 0/37 Loss: 1053.563721\n",
      "2025-07-08 19:47: Train Epoch 511: 20/37 Loss: 222.444855\n",
      "2025-07-08 19:47: **********Train Epoch 511: averaged Loss: 544.051126\n",
      "2025-07-08 19:47: **********Val Epoch 511: average Loss: 829.269582\n",
      "2025-07-08 19:47: Train Epoch 512: 0/37 Loss: 512.116699\n",
      "2025-07-08 19:47: Train Epoch 512: 20/37 Loss: 806.995728\n",
      "2025-07-08 19:47: **********Train Epoch 512: averaged Loss: 460.415380\n",
      "2025-07-08 19:47: **********Val Epoch 512: average Loss: 680.342712\n",
      "2025-07-08 19:47: Train Epoch 513: 0/37 Loss: 1687.678955\n",
      "2025-07-08 19:47: Train Epoch 513: 20/37 Loss: 150.338547\n",
      "2025-07-08 19:47: **********Train Epoch 513: averaged Loss: 539.578034\n",
      "2025-07-08 19:47: **********Val Epoch 513: average Loss: 1588.403076\n",
      "2025-07-08 19:47: Train Epoch 514: 0/37 Loss: 4061.223877\n",
      "2025-07-08 19:47: Train Epoch 514: 20/37 Loss: 193.688354\n",
      "2025-07-08 19:47: **********Train Epoch 514: averaged Loss: 877.196978\n",
      "2025-07-08 19:47: **********Val Epoch 514: average Loss: 404.182027\n",
      "2025-07-08 19:47: Train Epoch 515: 0/37 Loss: 2085.249268\n",
      "2025-07-08 19:47: Train Epoch 515: 20/37 Loss: 639.348328\n",
      "2025-07-08 19:47: **********Train Epoch 515: averaged Loss: 1089.186271\n",
      "2025-07-08 19:47: **********Val Epoch 515: average Loss: 2926.426514\n",
      "2025-07-08 19:47: Train Epoch 516: 0/37 Loss: 4941.404785\n",
      "2025-07-08 19:47: Train Epoch 516: 20/37 Loss: 300.661682\n",
      "2025-07-08 19:47: **********Train Epoch 516: averaged Loss: 1477.177127\n",
      "2025-07-08 19:47: **********Val Epoch 516: average Loss: 703.161184\n",
      "2025-07-08 19:47: Train Epoch 517: 0/37 Loss: 2523.353027\n",
      "2025-07-08 19:47: Train Epoch 517: 20/37 Loss: 318.037842\n",
      "2025-07-08 19:47: **********Train Epoch 517: averaged Loss: 613.258830\n",
      "2025-07-08 19:47: **********Val Epoch 517: average Loss: 2768.992961\n",
      "2025-07-08 19:47: Train Epoch 518: 0/37 Loss: 5059.402832\n",
      "2025-07-08 19:47: Train Epoch 518: 20/37 Loss: 444.249695\n",
      "2025-07-08 19:47: **********Train Epoch 518: averaged Loss: 1227.469128\n",
      "2025-07-08 19:47: **********Val Epoch 518: average Loss: 1265.198140\n",
      "2025-07-08 19:47: Train Epoch 519: 0/37 Loss: 1399.834473\n",
      "2025-07-08 19:47: Train Epoch 519: 20/37 Loss: 266.817566\n",
      "2025-07-08 19:47: **********Train Epoch 519: averaged Loss: 487.598924\n",
      "2025-07-08 19:47: **********Val Epoch 519: average Loss: 2277.110677\n",
      "2025-07-08 19:47: Train Epoch 520: 0/37 Loss: 2073.803467\n",
      "2025-07-08 19:48: Train Epoch 520: 20/37 Loss: 428.337769\n",
      "2025-07-08 19:48: **********Train Epoch 520: averaged Loss: 535.581898\n",
      "2025-07-08 19:48: **********Val Epoch 520: average Loss: 1519.107869\n",
      "2025-07-08 19:48: Train Epoch 521: 0/37 Loss: 781.650635\n",
      "2025-07-08 19:48: Train Epoch 521: 20/37 Loss: 408.246826\n",
      "2025-07-08 19:48: **********Train Epoch 521: averaged Loss: 446.522156\n",
      "2025-07-08 19:48: **********Val Epoch 521: average Loss: 2264.423299\n",
      "2025-07-08 19:48: Train Epoch 522: 0/37 Loss: 2478.538330\n",
      "2025-07-08 19:48: Train Epoch 522: 20/37 Loss: 181.172943\n",
      "2025-07-08 19:48: **********Train Epoch 522: averaged Loss: 772.382320\n",
      "2025-07-08 19:48: **********Val Epoch 522: average Loss: 1719.084554\n",
      "2025-07-08 19:48: Train Epoch 523: 0/37 Loss: 477.091888\n",
      "2025-07-08 19:48: Train Epoch 523: 20/37 Loss: 296.753296\n",
      "2025-07-08 19:48: **********Train Epoch 523: averaged Loss: 380.793844\n",
      "2025-07-08 19:48: **********Val Epoch 523: average Loss: 1079.241791\n",
      "2025-07-08 19:48: Train Epoch 524: 0/37 Loss: 776.175049\n",
      "2025-07-08 19:48: Train Epoch 524: 20/37 Loss: 492.218994\n",
      "2025-07-08 19:48: **********Train Epoch 524: averaged Loss: 429.559378\n",
      "2025-07-08 19:48: **********Val Epoch 524: average Loss: 2749.280192\n",
      "2025-07-08 19:48: Train Epoch 525: 0/37 Loss: 3516.896973\n",
      "2025-07-08 19:48: Train Epoch 525: 20/37 Loss: 321.775574\n",
      "2025-07-08 19:48: **********Train Epoch 525: averaged Loss: 1129.834823\n",
      "2025-07-08 19:48: **********Val Epoch 525: average Loss: 2085.207316\n",
      "2025-07-08 19:48: Train Epoch 526: 0/37 Loss: 911.695801\n",
      "2025-07-08 19:48: Train Epoch 526: 20/37 Loss: 270.044678\n",
      "2025-07-08 19:48: **********Train Epoch 526: averaged Loss: 339.385281\n",
      "2025-07-08 19:48: **********Val Epoch 526: average Loss: 1658.916260\n",
      "2025-07-08 19:48: Train Epoch 527: 0/37 Loss: 1910.126953\n",
      "2025-07-08 19:48: Train Epoch 527: 20/37 Loss: 345.136414\n",
      "2025-07-08 19:48: **********Train Epoch 527: averaged Loss: 551.735373\n",
      "2025-07-08 19:48: **********Val Epoch 527: average Loss: 607.535797\n",
      "2025-07-08 19:48: Train Epoch 528: 0/37 Loss: 827.834473\n",
      "2025-07-08 19:48: Train Epoch 528: 20/37 Loss: 442.871033\n",
      "2025-07-08 19:48: **********Train Epoch 528: averaged Loss: 356.639054\n",
      "2025-07-08 19:48: **********Val Epoch 528: average Loss: 1743.856242\n",
      "2025-07-08 19:48: Train Epoch 529: 0/37 Loss: 1906.537964\n",
      "2025-07-08 19:48: Train Epoch 529: 20/37 Loss: 279.208679\n",
      "2025-07-08 19:48: **********Train Epoch 529: averaged Loss: 548.241679\n",
      "2025-07-08 19:48: **********Val Epoch 529: average Loss: 710.469248\n",
      "2025-07-08 19:48: Train Epoch 530: 0/37 Loss: 178.512314\n",
      "2025-07-08 19:48: Train Epoch 530: 20/37 Loss: 218.818527\n",
      "2025-07-08 19:48: **********Train Epoch 530: averaged Loss: 269.513608\n",
      "2025-07-08 19:48: **********Val Epoch 530: average Loss: 1843.056152\n",
      "2025-07-08 19:48: Train Epoch 531: 0/37 Loss: 1071.389893\n",
      "2025-07-08 19:48: Train Epoch 531: 20/37 Loss: 135.461258\n",
      "2025-07-08 19:48: **********Train Epoch 531: averaged Loss: 322.068905\n",
      "2025-07-08 19:48: **********Val Epoch 531: average Loss: 975.955933\n",
      "2025-07-08 19:48: Train Epoch 532: 0/37 Loss: 765.094910\n",
      "2025-07-08 19:48: Train Epoch 532: 20/37 Loss: 310.476013\n",
      "2025-07-08 19:48: **********Train Epoch 532: averaged Loss: 355.311869\n",
      "2025-07-08 19:48: **********Val Epoch 532: average Loss: 949.567525\n",
      "2025-07-08 19:48: Train Epoch 533: 0/37 Loss: 195.808838\n",
      "2025-07-08 19:48: Train Epoch 533: 20/37 Loss: 144.721008\n",
      "2025-07-08 19:48: **********Train Epoch 533: averaged Loss: 233.284705\n",
      "2025-07-08 19:48: **********Val Epoch 533: average Loss: 1031.464722\n",
      "2025-07-08 19:48: Train Epoch 534: 0/37 Loss: 159.518097\n",
      "2025-07-08 19:48: Train Epoch 534: 20/37 Loss: 151.645508\n",
      "2025-07-08 19:48: **********Train Epoch 534: averaged Loss: 265.783885\n",
      "2025-07-08 19:48: **********Val Epoch 534: average Loss: 452.075918\n",
      "2025-07-08 19:48: Train Epoch 535: 0/37 Loss: 733.316833\n",
      "2025-07-08 19:48: Train Epoch 535: 20/37 Loss: 143.947845\n",
      "2025-07-08 19:48: **********Train Epoch 535: averaged Loss: 317.255538\n",
      "2025-07-08 19:48: **********Val Epoch 535: average Loss: 1824.946248\n",
      "2025-07-08 19:48: Train Epoch 536: 0/37 Loss: 1276.968750\n",
      "2025-07-08 19:48: Train Epoch 536: 20/37 Loss: 129.866760\n",
      "2025-07-08 19:48: **********Train Epoch 536: averaged Loss: 376.674989\n",
      "2025-07-08 19:48: **********Val Epoch 536: average Loss: 377.105387\n",
      "2025-07-08 19:48: Train Epoch 537: 0/37 Loss: 1047.744873\n",
      "2025-07-08 19:48: Train Epoch 537: 20/37 Loss: 187.267731\n",
      "2025-07-08 19:48: **********Train Epoch 537: averaged Loss: 457.653515\n",
      "2025-07-08 19:48: **********Val Epoch 537: average Loss: 1713.633667\n",
      "2025-07-08 19:48: Train Epoch 538: 0/37 Loss: 2324.364746\n",
      "2025-07-08 19:48: Train Epoch 538: 20/37 Loss: 201.635803\n",
      "2025-07-08 19:48: **********Train Epoch 538: averaged Loss: 640.206359\n",
      "2025-07-08 19:48: **********Val Epoch 538: average Loss: 681.162170\n",
      "2025-07-08 19:48: Train Epoch 539: 0/37 Loss: 1789.063232\n",
      "2025-07-08 19:48: Train Epoch 539: 20/37 Loss: 170.813217\n",
      "2025-07-08 19:48: **********Train Epoch 539: averaged Loss: 639.925731\n",
      "2025-07-08 19:48: **********Val Epoch 539: average Loss: 2166.997355\n",
      "2025-07-08 19:48: Train Epoch 540: 0/37 Loss: 1546.250366\n",
      "2025-07-08 19:48: Train Epoch 540: 20/37 Loss: 231.965271\n",
      "2025-07-08 19:48: **********Train Epoch 540: averaged Loss: 397.324058\n",
      "2025-07-08 19:48: **********Val Epoch 540: average Loss: 1406.236450\n",
      "2025-07-08 19:48: Train Epoch 541: 0/37 Loss: 233.707535\n",
      "2025-07-08 19:48: Train Epoch 541: 20/37 Loss: 226.592178\n",
      "2025-07-08 19:48: **********Train Epoch 541: averaged Loss: 313.035254\n",
      "2025-07-08 19:48: **********Val Epoch 541: average Loss: 2370.038411\n",
      "2025-07-08 19:48: Train Epoch 542: 0/37 Loss: 1910.726807\n",
      "2025-07-08 19:48: Train Epoch 542: 20/37 Loss: 356.616455\n",
      "2025-07-08 19:48: **********Train Epoch 542: averaged Loss: 520.088045\n",
      "2025-07-08 19:48: **********Val Epoch 542: average Loss: 770.254323\n",
      "2025-07-08 19:48: Train Epoch 543: 0/37 Loss: 226.548859\n",
      "2025-07-08 19:48: Train Epoch 543: 20/37 Loss: 371.103149\n",
      "2025-07-08 19:48: **********Train Epoch 543: averaged Loss: 426.102710\n",
      "2025-07-08 19:48: **********Val Epoch 543: average Loss: 1806.407633\n",
      "2025-07-08 19:48: Train Epoch 544: 0/37 Loss: 2489.358398\n",
      "2025-07-08 19:48: Train Epoch 544: 20/37 Loss: 153.999100\n",
      "2025-07-08 19:48: **********Train Epoch 544: averaged Loss: 827.014952\n",
      "2025-07-08 19:48: **********Val Epoch 544: average Loss: 1828.008504\n",
      "2025-07-08 19:48: Train Epoch 545: 0/37 Loss: 1103.141846\n",
      "2025-07-08 19:48: Train Epoch 545: 20/37 Loss: 178.687927\n",
      "2025-07-08 19:48: **********Train Epoch 545: averaged Loss: 426.090755\n",
      "2025-07-08 19:48: **********Val Epoch 545: average Loss: 1674.478353\n",
      "2025-07-08 19:48: Train Epoch 546: 0/37 Loss: 1439.547607\n",
      "2025-07-08 19:48: Train Epoch 546: 20/37 Loss: 382.957703\n",
      "2025-07-08 19:48: **********Train Epoch 546: averaged Loss: 465.433473\n",
      "2025-07-08 19:48: **********Val Epoch 546: average Loss: 1091.376678\n",
      "2025-07-08 19:48: Train Epoch 547: 0/37 Loss: 715.690796\n",
      "2025-07-08 19:48: Train Epoch 547: 20/37 Loss: 474.089203\n",
      "2025-07-08 19:48: **********Train Epoch 547: averaged Loss: 456.198012\n",
      "2025-07-08 19:48: **********Val Epoch 547: average Loss: 2563.979248\n",
      "2025-07-08 19:48: Train Epoch 548: 0/37 Loss: 3196.747070\n",
      "2025-07-08 19:48: Train Epoch 548: 20/37 Loss: 279.257446\n",
      "2025-07-08 19:48: **********Train Epoch 548: averaged Loss: 1024.636188\n",
      "2025-07-08 19:48: **********Val Epoch 548: average Loss: 1839.410034\n",
      "2025-07-08 19:48: Train Epoch 549: 0/37 Loss: 227.862915\n",
      "2025-07-08 19:48: Train Epoch 549: 20/37 Loss: 134.233658\n",
      "2025-07-08 19:48: **********Train Epoch 549: averaged Loss: 274.319194\n",
      "2025-07-08 19:48: **********Val Epoch 549: average Loss: 1033.246887\n",
      "2025-07-08 19:48: Train Epoch 550: 0/37 Loss: 864.497925\n",
      "2025-07-08 19:48: Train Epoch 550: 20/37 Loss: 421.460205\n",
      "2025-07-08 19:48: **********Train Epoch 550: averaged Loss: 371.121080\n",
      "2025-07-08 19:48: **********Val Epoch 550: average Loss: 2201.196045\n",
      "2025-07-08 19:48: Train Epoch 551: 0/37 Loss: 2035.598022\n",
      "2025-07-08 19:48: Train Epoch 551: 20/37 Loss: 385.863464\n",
      "2025-07-08 19:48: **********Train Epoch 551: averaged Loss: 693.453309\n",
      "2025-07-08 19:48: **********Val Epoch 551: average Loss: 1669.589152\n",
      "2025-07-08 19:48: Train Epoch 552: 0/37 Loss: 1379.508057\n",
      "2025-07-08 19:48: Train Epoch 552: 20/37 Loss: 146.117630\n",
      "2025-07-08 19:48: **********Train Epoch 552: averaged Loss: 435.667895\n",
      "2025-07-08 19:48: **********Val Epoch 552: average Loss: 1404.911987\n",
      "2025-07-08 19:48: Train Epoch 553: 0/37 Loss: 852.241455\n",
      "2025-07-08 19:48: Train Epoch 553: 20/37 Loss: 223.851639\n",
      "2025-07-08 19:48: **********Train Epoch 553: averaged Loss: 330.296498\n",
      "2025-07-08 19:48: **********Val Epoch 553: average Loss: 1445.396912\n",
      "2025-07-08 19:48: Train Epoch 554: 0/37 Loss: 774.827148\n",
      "2025-07-08 19:48: Train Epoch 554: 20/37 Loss: 152.951111\n",
      "2025-07-08 19:48: **********Train Epoch 554: averaged Loss: 292.352591\n",
      "2025-07-08 19:48: **********Val Epoch 554: average Loss: 1437.810343\n",
      "2025-07-08 19:48: Train Epoch 555: 0/37 Loss: 622.176636\n",
      "2025-07-08 19:48: Train Epoch 555: 20/37 Loss: 168.859619\n",
      "2025-07-08 19:48: **********Train Epoch 555: averaged Loss: 252.853742\n",
      "2025-07-08 19:48: **********Val Epoch 555: average Loss: 1521.012451\n",
      "2025-07-08 19:48: Train Epoch 556: 0/37 Loss: 561.805359\n",
      "2025-07-08 19:48: Train Epoch 556: 20/37 Loss: 131.229599\n",
      "2025-07-08 19:49: **********Train Epoch 556: averaged Loss: 239.681037\n",
      "2025-07-08 19:49: **********Val Epoch 556: average Loss: 1560.652120\n",
      "2025-07-08 19:49: Train Epoch 557: 0/37 Loss: 466.124207\n",
      "2025-07-08 19:49: Train Epoch 557: 20/37 Loss: 136.502670\n",
      "2025-07-08 19:49: **********Train Epoch 557: averaged Loss: 218.085940\n",
      "2025-07-08 19:49: **********Val Epoch 557: average Loss: 1550.276937\n",
      "2025-07-08 19:49: Train Epoch 558: 0/37 Loss: 384.760834\n",
      "2025-07-08 19:49: Train Epoch 558: 20/37 Loss: 134.986328\n",
      "2025-07-08 19:49: **********Train Epoch 558: averaged Loss: 213.180669\n",
      "2025-07-08 19:49: **********Val Epoch 558: average Loss: 1560.672831\n",
      "2025-07-08 19:49: Train Epoch 559: 0/37 Loss: 305.719727\n",
      "2025-07-08 19:49: Train Epoch 559: 20/37 Loss: 130.579559\n",
      "2025-07-08 19:49: **********Train Epoch 559: averaged Loss: 203.799268\n",
      "2025-07-08 19:49: **********Val Epoch 559: average Loss: 1521.228739\n",
      "2025-07-08 19:49: Train Epoch 560: 0/37 Loss: 383.580902\n",
      "2025-07-08 19:49: Train Epoch 560: 20/37 Loss: 139.288895\n",
      "2025-07-08 19:49: **********Train Epoch 560: averaged Loss: 204.374080\n",
      "2025-07-08 19:49: **********Val Epoch 560: average Loss: 1473.355469\n",
      "2025-07-08 19:49: Train Epoch 561: 0/37 Loss: 238.940033\n",
      "2025-07-08 19:49: Train Epoch 561: 20/37 Loss: 137.946259\n",
      "2025-07-08 19:49: **********Train Epoch 561: averaged Loss: 199.961060\n",
      "2025-07-08 19:49: **********Val Epoch 561: average Loss: 1484.821859\n",
      "2025-07-08 19:49: Train Epoch 562: 0/37 Loss: 345.282898\n",
      "2025-07-08 19:49: Train Epoch 562: 20/37 Loss: 131.155807\n",
      "2025-07-08 19:49: **********Train Epoch 562: averaged Loss: 199.472651\n",
      "2025-07-08 19:49: **********Val Epoch 562: average Loss: 1387.645671\n",
      "2025-07-08 19:49: Train Epoch 563: 0/37 Loss: 171.234665\n",
      "2025-07-08 19:49: Train Epoch 563: 20/37 Loss: 146.301132\n",
      "2025-07-08 19:49: **********Train Epoch 563: averaged Loss: 196.969381\n",
      "2025-07-08 19:49: **********Val Epoch 563: average Loss: 1449.727661\n",
      "2025-07-08 19:49: Train Epoch 564: 0/37 Loss: 280.557373\n",
      "2025-07-08 19:49: Train Epoch 564: 20/37 Loss: 148.161224\n",
      "2025-07-08 19:49: **********Train Epoch 564: averaged Loss: 193.482015\n",
      "2025-07-08 19:49: **********Val Epoch 564: average Loss: 1319.533427\n",
      "2025-07-08 19:49: Train Epoch 565: 0/37 Loss: 151.152664\n",
      "2025-07-08 19:49: Train Epoch 565: 20/37 Loss: 153.482727\n",
      "2025-07-08 19:49: **********Train Epoch 565: averaged Loss: 193.461008\n",
      "2025-07-08 19:49: **********Val Epoch 565: average Loss: 1371.243978\n",
      "2025-07-08 19:49: Train Epoch 566: 0/37 Loss: 264.286194\n",
      "2025-07-08 19:49: Train Epoch 566: 20/37 Loss: 149.050949\n",
      "2025-07-08 19:49: **********Train Epoch 566: averaged Loss: 194.959483\n",
      "2025-07-08 19:49: **********Val Epoch 566: average Loss: 1327.206645\n",
      "2025-07-08 19:49: Train Epoch 567: 0/37 Loss: 183.292282\n",
      "2025-07-08 19:49: Train Epoch 567: 20/37 Loss: 162.638489\n",
      "2025-07-08 19:49: **********Train Epoch 567: averaged Loss: 191.981508\n",
      "2025-07-08 19:49: **********Val Epoch 567: average Loss: 1316.773966\n",
      "2025-07-08 19:49: Train Epoch 568: 0/37 Loss: 322.489746\n",
      "2025-07-08 19:49: Train Epoch 568: 20/37 Loss: 133.439346\n",
      "2025-07-08 19:49: **********Train Epoch 568: averaged Loss: 197.058069\n",
      "2025-07-08 19:49: **********Val Epoch 568: average Loss: 1267.091024\n",
      "2025-07-08 19:49: Train Epoch 569: 0/37 Loss: 156.307541\n",
      "2025-07-08 19:49: Train Epoch 569: 20/37 Loss: 142.092133\n",
      "2025-07-08 19:49: **********Train Epoch 569: averaged Loss: 199.477722\n",
      "2025-07-08 19:49: **********Val Epoch 569: average Loss: 1297.494324\n",
      "2025-07-08 19:49: Train Epoch 570: 0/37 Loss: 341.766144\n",
      "2025-07-08 19:49: Train Epoch 570: 20/37 Loss: 136.217056\n",
      "2025-07-08 19:49: **********Train Epoch 570: averaged Loss: 203.222312\n",
      "2025-07-08 19:49: **********Val Epoch 570: average Loss: 1230.049479\n",
      "2025-07-08 19:49: Train Epoch 571: 0/37 Loss: 159.186646\n",
      "2025-07-08 19:49: Train Epoch 571: 20/37 Loss: 163.105881\n",
      "2025-07-08 19:49: **********Train Epoch 571: averaged Loss: 209.877886\n",
      "2025-07-08 19:49: **********Val Epoch 571: average Loss: 1257.483175\n",
      "2025-07-08 19:49: Train Epoch 572: 0/37 Loss: 340.674988\n",
      "2025-07-08 19:49: Train Epoch 572: 20/37 Loss: 124.318436\n",
      "2025-07-08 19:49: **********Train Epoch 572: averaged Loss: 220.776201\n",
      "2025-07-08 19:49: **********Val Epoch 572: average Loss: 1111.367493\n",
      "2025-07-08 19:49: Train Epoch 573: 0/37 Loss: 207.216385\n",
      "2025-07-08 19:49: Train Epoch 573: 20/37 Loss: 151.666428\n",
      "2025-07-08 19:49: **********Train Epoch 573: averaged Loss: 242.806861\n",
      "2025-07-08 19:49: **********Val Epoch 573: average Loss: 1329.110514\n",
      "2025-07-08 19:49: Train Epoch 574: 0/37 Loss: 471.467285\n",
      "2025-07-08 19:49: Train Epoch 574: 20/37 Loss: 125.763962\n",
      "2025-07-08 19:49: **********Train Epoch 574: averaged Loss: 257.787396\n",
      "2025-07-08 19:49: **********Val Epoch 574: average Loss: 999.287730\n",
      "2025-07-08 19:49: Train Epoch 575: 0/37 Loss: 321.069244\n",
      "2025-07-08 19:49: Train Epoch 575: 20/37 Loss: 150.874329\n",
      "2025-07-08 19:49: **********Train Epoch 575: averaged Loss: 284.963983\n",
      "2025-07-08 19:49: **********Val Epoch 575: average Loss: 1389.077311\n",
      "2025-07-08 19:49: Train Epoch 576: 0/37 Loss: 550.863159\n",
      "2025-07-08 19:49: Train Epoch 576: 20/37 Loss: 150.285889\n",
      "2025-07-08 19:49: **********Train Epoch 576: averaged Loss: 284.355580\n",
      "2025-07-08 19:49: **********Val Epoch 576: average Loss: 968.198812\n",
      "2025-07-08 19:49: Train Epoch 577: 0/37 Loss: 337.230713\n",
      "2025-07-08 19:49: Train Epoch 577: 20/37 Loss: 141.599442\n",
      "2025-07-08 19:49: **********Train Epoch 577: averaged Loss: 311.229140\n",
      "2025-07-08 19:49: **********Val Epoch 577: average Loss: 1381.751709\n",
      "2025-07-08 19:49: Train Epoch 578: 0/37 Loss: 567.318787\n",
      "2025-07-08 19:49: Train Epoch 578: 20/37 Loss: 136.514099\n",
      "2025-07-08 19:49: **********Train Epoch 578: averaged Loss: 293.068850\n",
      "2025-07-08 19:49: **********Val Epoch 578: average Loss: 946.327382\n",
      "2025-07-08 19:49: Train Epoch 579: 0/37 Loss: 308.774475\n",
      "2025-07-08 19:49: Train Epoch 579: 20/37 Loss: 142.358749\n",
      "2025-07-08 19:49: **********Train Epoch 579: averaged Loss: 317.113576\n",
      "2025-07-08 19:49: **********Val Epoch 579: average Loss: 1416.312215\n",
      "2025-07-08 19:49: Train Epoch 580: 0/37 Loss: 610.775879\n",
      "2025-07-08 19:49: Train Epoch 580: 20/37 Loss: 136.230164\n",
      "2025-07-08 19:49: **********Train Epoch 580: averaged Loss: 299.944216\n",
      "2025-07-08 19:49: **********Val Epoch 580: average Loss: 951.366262\n",
      "2025-07-08 19:49: Train Epoch 581: 0/37 Loss: 266.478149\n",
      "2025-07-08 19:49: Train Epoch 581: 20/37 Loss: 146.806946\n",
      "2025-07-08 19:49: **********Train Epoch 581: averaged Loss: 308.563953\n",
      "2025-07-08 19:49: **********Val Epoch 581: average Loss: 1413.613770\n",
      "2025-07-08 19:49: Train Epoch 582: 0/37 Loss: 580.479614\n",
      "2025-07-08 19:49: Train Epoch 582: 20/37 Loss: 138.596207\n",
      "2025-07-08 19:49: **********Train Epoch 582: averaged Loss: 289.816052\n",
      "2025-07-08 19:49: **********Val Epoch 582: average Loss: 941.561228\n",
      "2025-07-08 19:49: Train Epoch 583: 0/37 Loss: 255.209991\n",
      "2025-07-08 19:49: Train Epoch 583: 20/37 Loss: 144.463333\n",
      "2025-07-08 19:49: **********Train Epoch 583: averaged Loss: 304.639360\n",
      "2025-07-08 19:49: **********Val Epoch 583: average Loss: 1413.955587\n",
      "2025-07-08 19:49: Train Epoch 584: 0/37 Loss: 581.696411\n",
      "2025-07-08 19:49: Train Epoch 584: 20/37 Loss: 141.860016\n",
      "2025-07-08 19:49: **********Train Epoch 584: averaged Loss: 290.672160\n",
      "2025-07-08 19:49: **********Val Epoch 584: average Loss: 937.871582\n",
      "2025-07-08 19:49: Train Epoch 585: 0/37 Loss: 228.124329\n",
      "2025-07-08 19:49: Train Epoch 585: 20/37 Loss: 140.650894\n",
      "2025-07-08 19:49: **********Train Epoch 585: averaged Loss: 294.848592\n",
      "2025-07-08 19:49: **********Val Epoch 585: average Loss: 1396.611613\n",
      "2025-07-08 19:49: Train Epoch 586: 0/37 Loss: 613.999878\n",
      "2025-07-08 19:49: Train Epoch 586: 20/37 Loss: 128.611816\n",
      "2025-07-08 19:49: **********Train Epoch 586: averaged Loss: 282.321854\n",
      "2025-07-08 19:49: **********Val Epoch 586: average Loss: 974.856283\n",
      "2025-07-08 19:49: Train Epoch 587: 0/37 Loss: 253.281128\n",
      "2025-07-08 19:49: Train Epoch 587: 20/37 Loss: 141.285767\n",
      "2025-07-08 19:49: **********Train Epoch 587: averaged Loss: 286.952821\n",
      "2025-07-08 19:49: **********Val Epoch 587: average Loss: 1343.525675\n",
      "2025-07-08 19:49: Train Epoch 588: 0/37 Loss: 539.572510\n",
      "2025-07-08 19:49: Train Epoch 588: 20/37 Loss: 143.652786\n",
      "2025-07-08 19:49: **********Train Epoch 588: averaged Loss: 280.489867\n",
      "2025-07-08 19:49: **********Val Epoch 588: average Loss: 951.988454\n",
      "2025-07-08 19:49: Train Epoch 589: 0/37 Loss: 256.469727\n",
      "2025-07-08 19:49: Train Epoch 589: 20/37 Loss: 142.566284\n",
      "2025-07-08 19:49: **********Train Epoch 589: averaged Loss: 291.169699\n",
      "2025-07-08 19:49: **********Val Epoch 589: average Loss: 1358.589437\n",
      "2025-07-08 19:49: Train Epoch 590: 0/37 Loss: 570.227661\n",
      "2025-07-08 19:49: Train Epoch 590: 20/37 Loss: 131.483643\n",
      "2025-07-08 19:49: **********Train Epoch 590: averaged Loss: 283.975226\n",
      "2025-07-08 19:49: **********Val Epoch 590: average Loss: 959.507273\n",
      "2025-07-08 19:49: Train Epoch 591: 0/37 Loss: 223.748505\n",
      "2025-07-08 19:49: Train Epoch 591: 20/37 Loss: 140.662750\n",
      "2025-07-08 19:49: **********Train Epoch 591: averaged Loss: 291.679863\n",
      "2025-07-08 19:49: **********Val Epoch 591: average Loss: 1399.504028\n",
      "2025-07-08 19:49: Train Epoch 592: 0/37 Loss: 593.066284\n",
      "2025-07-08 19:49: Train Epoch 592: 20/37 Loss: 132.985336\n",
      "2025-07-08 19:49: **********Train Epoch 592: averaged Loss: 282.683195\n",
      "2025-07-08 19:49: **********Val Epoch 592: average Loss: 927.860372\n",
      "2025-07-08 19:49: Train Epoch 593: 0/37 Loss: 236.319458\n",
      "2025-07-08 19:50: Train Epoch 593: 20/37 Loss: 140.848312\n",
      "2025-07-08 19:50: **********Train Epoch 593: averaged Loss: 287.392973\n",
      "2025-07-08 19:50: **********Val Epoch 593: average Loss: 1312.291361\n",
      "2025-07-08 19:50: Train Epoch 594: 0/37 Loss: 548.486633\n",
      "2025-07-08 19:50: Train Epoch 594: 20/37 Loss: 131.478592\n",
      "2025-07-08 19:50: **********Train Epoch 594: averaged Loss: 272.547152\n",
      "2025-07-08 19:50: **********Val Epoch 594: average Loss: 946.648305\n",
      "2025-07-08 19:50: Train Epoch 595: 0/37 Loss: 242.704147\n",
      "2025-07-08 19:50: Train Epoch 595: 20/37 Loss: 141.049728\n",
      "2025-07-08 19:50: **********Train Epoch 595: averaged Loss: 275.996103\n",
      "2025-07-08 19:50: **********Val Epoch 595: average Loss: 1308.320150\n",
      "2025-07-08 19:50: Train Epoch 596: 0/37 Loss: 560.829102\n",
      "2025-07-08 19:50: Train Epoch 596: 20/37 Loss: 141.667740\n",
      "2025-07-08 19:50: **********Train Epoch 596: averaged Loss: 273.555786\n",
      "2025-07-08 19:50: **********Val Epoch 596: average Loss: 886.009064\n",
      "2025-07-08 19:50: Train Epoch 597: 0/37 Loss: 241.676743\n",
      "2025-07-08 19:50: Train Epoch 597: 20/37 Loss: 139.533859\n",
      "2025-07-08 19:50: **********Train Epoch 597: averaged Loss: 295.454211\n",
      "2025-07-08 19:50: **********Val Epoch 597: average Loss: 1330.681132\n",
      "2025-07-08 19:50: Train Epoch 598: 0/37 Loss: 602.572815\n",
      "2025-07-08 19:50: Train Epoch 598: 20/37 Loss: 123.828522\n",
      "2025-07-08 19:50: **********Train Epoch 598: averaged Loss: 280.291605\n",
      "2025-07-08 19:50: **********Val Epoch 598: average Loss: 892.024485\n",
      "2025-07-08 19:50: Train Epoch 599: 0/37 Loss: 239.768234\n",
      "2025-07-08 19:50: Train Epoch 599: 20/37 Loss: 141.395035\n",
      "2025-07-08 19:50: **********Train Epoch 599: averaged Loss: 294.059510\n",
      "2025-07-08 19:50: **********Val Epoch 599: average Loss: 1343.571289\n",
      "2025-07-08 19:50: Train Epoch 600: 0/37 Loss: 551.753784\n",
      "2025-07-08 19:50: Train Epoch 600: 20/37 Loss: 124.603348\n",
      "2025-07-08 19:50: **********Train Epoch 600: averaged Loss: 275.700486\n",
      "2025-07-08 19:50: **********Val Epoch 600: average Loss: 918.344564\n",
      "2025-07-08 19:50: Train Epoch 601: 0/37 Loss: 250.352829\n",
      "2025-07-08 19:50: Train Epoch 601: 20/37 Loss: 137.795074\n",
      "2025-07-08 19:50: **********Train Epoch 601: averaged Loss: 288.143119\n",
      "2025-07-08 19:50: **********Val Epoch 601: average Loss: 1332.228475\n",
      "2025-07-08 19:50: Train Epoch 602: 0/37 Loss: 593.764526\n",
      "2025-07-08 19:50: Train Epoch 602: 20/37 Loss: 139.448349\n",
      "2025-07-08 19:50: **********Train Epoch 602: averaged Loss: 271.101486\n",
      "2025-07-08 19:50: **********Val Epoch 602: average Loss: 928.099599\n",
      "2025-07-08 19:50: Train Epoch 603: 0/37 Loss: 203.200195\n",
      "2025-07-08 19:50: Train Epoch 603: 20/37 Loss: 137.683792\n",
      "2025-07-08 19:50: **********Train Epoch 603: averaged Loss: 274.359069\n",
      "2025-07-08 19:50: **********Val Epoch 603: average Loss: 1297.766439\n",
      "2025-07-08 19:50: Train Epoch 604: 0/37 Loss: 516.026123\n",
      "2025-07-08 19:50: Train Epoch 604: 20/37 Loss: 138.132736\n",
      "2025-07-08 19:50: **********Train Epoch 604: averaged Loss: 262.819696\n",
      "2025-07-08 19:50: **********Val Epoch 604: average Loss: 911.571289\n",
      "2025-07-08 19:50: Train Epoch 605: 0/37 Loss: 250.370163\n",
      "2025-07-08 19:50: Train Epoch 605: 20/37 Loss: 140.800980\n",
      "2025-07-08 19:50: **********Train Epoch 605: averaged Loss: 276.659685\n",
      "2025-07-08 19:50: **********Val Epoch 605: average Loss: 1296.857910\n",
      "2025-07-08 19:50: Train Epoch 606: 0/37 Loss: 557.443176\n",
      "2025-07-08 19:50: Train Epoch 606: 20/37 Loss: 149.485901\n",
      "2025-07-08 19:50: **********Train Epoch 606: averaged Loss: 268.293463\n",
      "2025-07-08 19:50: **********Val Epoch 606: average Loss: 873.030640\n",
      "2025-07-08 19:50: Train Epoch 607: 0/37 Loss: 247.891922\n",
      "2025-07-08 19:50: Train Epoch 607: 20/37 Loss: 137.789429\n",
      "2025-07-08 19:50: **********Train Epoch 607: averaged Loss: 281.532196\n",
      "2025-07-08 19:50: **********Val Epoch 607: average Loss: 1307.857096\n",
      "2025-07-08 19:50: Train Epoch 608: 0/37 Loss: 592.604126\n",
      "2025-07-08 19:50: Train Epoch 608: 20/37 Loss: 138.174484\n",
      "2025-07-08 19:50: **********Train Epoch 608: averaged Loss: 272.934174\n",
      "2025-07-08 19:50: **********Val Epoch 608: average Loss: 897.764862\n",
      "2025-07-08 19:50: Train Epoch 609: 0/37 Loss: 250.463882\n",
      "2025-07-08 19:50: Train Epoch 609: 20/37 Loss: 138.862518\n",
      "2025-07-08 19:50: **********Train Epoch 609: averaged Loss: 283.518207\n",
      "2025-07-08 19:50: **********Val Epoch 609: average Loss: 1301.405233\n",
      "2025-07-08 19:50: Train Epoch 610: 0/37 Loss: 576.472107\n",
      "2025-07-08 19:50: Train Epoch 610: 20/37 Loss: 123.442780\n",
      "2025-07-08 19:50: **********Train Epoch 610: averaged Loss: 270.463262\n",
      "2025-07-08 19:50: **********Val Epoch 610: average Loss: 880.979156\n",
      "2025-07-08 19:50: Train Epoch 611: 0/37 Loss: 257.152863\n",
      "2025-07-08 19:50: Train Epoch 611: 20/37 Loss: 136.783112\n",
      "2025-07-08 19:50: **********Train Epoch 611: averaged Loss: 281.841380\n",
      "2025-07-08 19:50: **********Val Epoch 611: average Loss: 1286.178731\n",
      "2025-07-08 19:50: Train Epoch 612: 0/37 Loss: 557.085693\n",
      "2025-07-08 19:50: Train Epoch 612: 20/37 Loss: 126.857773\n",
      "2025-07-08 19:50: **********Train Epoch 612: averaged Loss: 265.001750\n",
      "2025-07-08 19:50: **********Val Epoch 612: average Loss: 887.787231\n",
      "2025-07-08 19:50: Train Epoch 613: 0/37 Loss: 246.627838\n",
      "2025-07-08 19:50: Train Epoch 613: 20/37 Loss: 139.069534\n",
      "2025-07-08 19:50: **********Train Epoch 613: averaged Loss: 271.203798\n",
      "2025-07-08 19:50: **********Val Epoch 613: average Loss: 1235.291748\n",
      "2025-07-08 19:50: Train Epoch 614: 0/37 Loss: 543.707947\n",
      "2025-07-08 19:50: Train Epoch 614: 20/37 Loss: 137.557800\n",
      "2025-07-08 19:50: **********Train Epoch 614: averaged Loss: 263.855238\n",
      "2025-07-08 19:50: **********Val Epoch 614: average Loss: 887.420502\n",
      "2025-07-08 19:50: Train Epoch 615: 0/37 Loss: 234.536713\n",
      "2025-07-08 19:50: Train Epoch 615: 20/37 Loss: 155.269531\n",
      "2025-07-08 19:50: **********Train Epoch 615: averaged Loss: 271.047916\n",
      "2025-07-08 19:50: **********Val Epoch 615: average Loss: 1203.623291\n",
      "2025-07-08 19:50: Train Epoch 616: 0/37 Loss: 576.242676\n",
      "2025-07-08 19:50: Train Epoch 616: 20/37 Loss: 120.611267\n",
      "2025-07-08 19:50: **********Train Epoch 616: averaged Loss: 265.003411\n",
      "2025-07-08 19:50: **********Val Epoch 616: average Loss: 878.559316\n",
      "2025-07-08 19:50: Train Epoch 617: 0/37 Loss: 197.808533\n",
      "2025-07-08 19:50: Train Epoch 617: 20/37 Loss: 139.790741\n",
      "2025-07-08 19:50: **********Train Epoch 617: averaged Loss: 272.210005\n",
      "2025-07-08 19:50: **********Val Epoch 617: average Loss: 1240.458008\n",
      "2025-07-08 19:50: Train Epoch 618: 0/37 Loss: 582.437683\n",
      "2025-07-08 19:50: Train Epoch 618: 20/37 Loss: 134.696838\n",
      "2025-07-08 19:50: **********Train Epoch 618: averaged Loss: 266.662025\n",
      "2025-07-08 19:50: **********Val Epoch 618: average Loss: 881.935293\n",
      "2025-07-08 19:50: Train Epoch 619: 0/37 Loss: 223.456161\n",
      "2025-07-08 19:50: Train Epoch 619: 20/37 Loss: 143.985611\n",
      "2025-07-08 19:50: **********Train Epoch 619: averaged Loss: 281.886201\n",
      "2025-07-08 19:50: **********Val Epoch 619: average Loss: 1261.173218\n",
      "2025-07-08 19:50: Train Epoch 620: 0/37 Loss: 547.638489\n",
      "2025-07-08 19:50: Train Epoch 620: 20/37 Loss: 143.374649\n",
      "2025-07-08 19:50: **********Train Epoch 620: averaged Loss: 267.471991\n",
      "2025-07-08 19:50: **********Val Epoch 620: average Loss: 843.894948\n",
      "2025-07-08 19:50: Train Epoch 621: 0/37 Loss: 267.224579\n",
      "2025-07-08 19:50: Train Epoch 621: 20/37 Loss: 138.302277\n",
      "2025-07-08 19:50: **********Train Epoch 621: averaged Loss: 288.010842\n",
      "2025-07-08 19:50: **********Val Epoch 621: average Loss: 1214.585938\n",
      "2025-07-08 19:50: Train Epoch 622: 0/37 Loss: 575.830627\n",
      "2025-07-08 19:50: Train Epoch 622: 20/37 Loss: 129.927856\n",
      "2025-07-08 19:50: **********Train Epoch 622: averaged Loss: 270.333473\n",
      "2025-07-08 19:50: **********Val Epoch 622: average Loss: 823.451294\n",
      "2025-07-08 19:50: Train Epoch 623: 0/37 Loss: 242.394836\n",
      "2025-07-08 19:50: Train Epoch 623: 20/37 Loss: 138.418228\n",
      "2025-07-08 19:50: **********Train Epoch 623: averaged Loss: 282.060364\n",
      "2025-07-08 19:50: **********Val Epoch 623: average Loss: 1246.902873\n",
      "2025-07-08 19:50: Train Epoch 624: 0/37 Loss: 573.476318\n",
      "2025-07-08 19:50: Train Epoch 624: 20/37 Loss: 134.735962\n",
      "2025-07-08 19:50: **********Train Epoch 624: averaged Loss: 275.113641\n",
      "2025-07-08 19:50: **********Val Epoch 624: average Loss: 823.676320\n",
      "2025-07-08 19:50: Train Epoch 625: 0/37 Loss: 230.287811\n",
      "2025-07-08 19:50: Train Epoch 625: 20/37 Loss: 141.667191\n",
      "2025-07-08 19:50: **********Train Epoch 625: averaged Loss: 282.592468\n",
      "2025-07-08 19:50: **********Val Epoch 625: average Loss: 1243.281128\n",
      "2025-07-08 19:50: Train Epoch 626: 0/37 Loss: 522.216797\n",
      "2025-07-08 19:50: Train Epoch 626: 20/37 Loss: 144.895538\n",
      "2025-07-08 19:50: **********Train Epoch 626: averaged Loss: 269.421636\n",
      "2025-07-08 19:50: **********Val Epoch 626: average Loss: 847.967295\n",
      "2025-07-08 19:50: Train Epoch 627: 0/37 Loss: 253.085892\n",
      "2025-07-08 19:50: Train Epoch 627: 20/37 Loss: 139.654755\n",
      "2025-07-08 19:50: **********Train Epoch 627: averaged Loss: 276.864351\n",
      "2025-07-08 19:50: **********Val Epoch 627: average Loss: 1235.600057\n",
      "2025-07-08 19:50: Train Epoch 628: 0/37 Loss: 550.578613\n",
      "2025-07-08 19:50: Train Epoch 628: 20/37 Loss: 132.997864\n",
      "2025-07-08 19:50: **********Train Epoch 628: averaged Loss: 267.450824\n",
      "2025-07-08 19:50: **********Val Epoch 628: average Loss: 862.928925\n",
      "2025-07-08 19:50: Train Epoch 629: 0/37 Loss: 245.429916\n",
      "2025-07-08 19:50: Train Epoch 629: 20/37 Loss: 140.776154\n",
      "2025-07-08 19:51: **********Train Epoch 629: averaged Loss: 277.381951\n",
      "2025-07-08 19:51: **********Val Epoch 629: average Loss: 1238.739543\n",
      "2025-07-08 19:51: Train Epoch 630: 0/37 Loss: 575.640381\n",
      "2025-07-08 19:51: Train Epoch 630: 20/37 Loss: 134.003784\n",
      "2025-07-08 19:51: **********Train Epoch 630: averaged Loss: 271.279143\n",
      "2025-07-08 19:51: **********Val Epoch 630: average Loss: 810.669495\n",
      "2025-07-08 19:51: Train Epoch 631: 0/37 Loss: 245.407471\n",
      "2025-07-08 19:51: Train Epoch 631: 20/37 Loss: 140.657471\n",
      "2025-07-08 19:51: **********Train Epoch 631: averaged Loss: 280.999046\n",
      "2025-07-08 19:51: **********Val Epoch 631: average Loss: 1244.860779\n",
      "2025-07-08 19:51: Train Epoch 632: 0/37 Loss: 538.462769\n",
      "2025-07-08 19:51: Train Epoch 632: 20/37 Loss: 129.519470\n",
      "2025-07-08 19:51: **********Train Epoch 632: averaged Loss: 265.113784\n",
      "2025-07-08 19:51: **********Val Epoch 632: average Loss: 839.426412\n",
      "2025-07-08 19:51: Train Epoch 633: 0/37 Loss: 232.632980\n",
      "2025-07-08 19:51: Train Epoch 633: 20/37 Loss: 139.752701\n",
      "2025-07-08 19:51: **********Train Epoch 633: averaged Loss: 282.693622\n",
      "2025-07-08 19:51: **********Val Epoch 633: average Loss: 1254.306844\n",
      "2025-07-08 19:51: Train Epoch 634: 0/37 Loss: 591.419983\n",
      "2025-07-08 19:51: Train Epoch 634: 20/37 Loss: 137.732452\n",
      "2025-07-08 19:51: **********Train Epoch 634: averaged Loss: 274.170584\n",
      "2025-07-08 19:51: **********Val Epoch 634: average Loss: 826.173157\n",
      "2025-07-08 19:51: Train Epoch 635: 0/37 Loss: 247.219055\n",
      "2025-07-08 19:51: Train Epoch 635: 20/37 Loss: 139.714539\n",
      "2025-07-08 19:51: **********Train Epoch 635: averaged Loss: 281.029811\n",
      "2025-07-08 19:51: **********Val Epoch 635: average Loss: 1241.917908\n",
      "2025-07-08 19:51: Train Epoch 636: 0/37 Loss: 592.174316\n",
      "2025-07-08 19:51: Train Epoch 636: 20/37 Loss: 125.865982\n",
      "2025-07-08 19:51: **********Train Epoch 636: averaged Loss: 267.017326\n",
      "2025-07-08 19:51: **********Val Epoch 636: average Loss: 821.382233\n",
      "2025-07-08 19:51: Train Epoch 637: 0/37 Loss: 234.264984\n",
      "2025-07-08 19:51: Train Epoch 637: 20/37 Loss: 138.156189\n",
      "2025-07-08 19:51: **********Train Epoch 637: averaged Loss: 272.085384\n",
      "2025-07-08 19:51: **********Val Epoch 637: average Loss: 1178.693604\n",
      "2025-07-08 19:51: Train Epoch 638: 0/37 Loss: 545.333740\n",
      "2025-07-08 19:51: Train Epoch 638: 20/37 Loss: 135.497391\n",
      "2025-07-08 19:51: **********Train Epoch 638: averaged Loss: 264.049584\n",
      "2025-07-08 19:51: **********Val Epoch 638: average Loss: 804.635793\n",
      "2025-07-08 19:51: Train Epoch 639: 0/37 Loss: 214.700546\n",
      "2025-07-08 19:51: Train Epoch 639: 20/37 Loss: 145.324554\n",
      "2025-07-08 19:51: **********Train Epoch 639: averaged Loss: 279.103368\n",
      "2025-07-08 19:51: **********Val Epoch 639: average Loss: 1212.201843\n",
      "2025-07-08 19:51: Train Epoch 640: 0/37 Loss: 567.102051\n",
      "2025-07-08 19:51: Train Epoch 640: 20/37 Loss: 124.268066\n",
      "2025-07-08 19:51: **********Train Epoch 640: averaged Loss: 266.689842\n",
      "2025-07-08 19:51: **********Val Epoch 640: average Loss: 828.872793\n",
      "2025-07-08 19:51: Train Epoch 641: 0/37 Loss: 227.739136\n",
      "2025-07-08 19:51: Train Epoch 641: 20/37 Loss: 137.603943\n",
      "2025-07-08 19:51: **********Train Epoch 641: averaged Loss: 274.312848\n",
      "2025-07-08 19:51: **********Val Epoch 641: average Loss: 1203.536479\n",
      "2025-07-08 19:51: Train Epoch 642: 0/37 Loss: 567.454590\n",
      "2025-07-08 19:51: Train Epoch 642: 20/37 Loss: 134.758286\n",
      "2025-07-08 19:51: **********Train Epoch 642: averaged Loss: 265.428819\n",
      "2025-07-08 19:51: **********Val Epoch 642: average Loss: 829.272044\n",
      "2025-07-08 19:51: Train Epoch 643: 0/37 Loss: 234.742065\n",
      "2025-07-08 19:51: Train Epoch 643: 20/37 Loss: 137.674408\n",
      "2025-07-08 19:51: **********Train Epoch 643: averaged Loss: 278.294269\n",
      "2025-07-08 19:51: **********Val Epoch 643: average Loss: 1178.791423\n",
      "2025-07-08 19:51: Train Epoch 644: 0/37 Loss: 566.558838\n",
      "2025-07-08 19:51: Train Epoch 644: 20/37 Loss: 125.941940\n",
      "2025-07-08 19:51: **********Train Epoch 644: averaged Loss: 270.185596\n",
      "2025-07-08 19:51: **********Val Epoch 644: average Loss: 803.551595\n",
      "2025-07-08 19:51: Train Epoch 645: 0/37 Loss: 226.162125\n",
      "2025-07-08 19:51: Train Epoch 645: 20/37 Loss: 138.976868\n",
      "2025-07-08 19:51: **********Train Epoch 645: averaged Loss: 282.948531\n",
      "2025-07-08 19:51: **********Val Epoch 645: average Loss: 1242.994141\n",
      "2025-07-08 19:51: Train Epoch 646: 0/37 Loss: 615.428772\n",
      "2025-07-08 19:51: Train Epoch 646: 20/37 Loss: 144.228348\n",
      "2025-07-08 19:51: **********Train Epoch 646: averaged Loss: 275.323262\n",
      "2025-07-08 19:51: **********Val Epoch 646: average Loss: 791.865438\n",
      "2025-07-08 19:51: Train Epoch 647: 0/37 Loss: 243.361816\n",
      "2025-07-08 19:51: Train Epoch 647: 20/37 Loss: 137.128799\n",
      "2025-07-08 19:51: **********Train Epoch 647: averaged Loss: 285.058397\n",
      "2025-07-08 19:51: **********Val Epoch 647: average Loss: 1215.031514\n",
      "2025-07-08 19:51: Train Epoch 648: 0/37 Loss: 593.094482\n",
      "2025-07-08 19:51: Train Epoch 648: 20/37 Loss: 126.843018\n",
      "2025-07-08 19:51: **********Train Epoch 648: averaged Loss: 273.042809\n",
      "2025-07-08 19:51: **********Val Epoch 648: average Loss: 807.055990\n",
      "2025-07-08 19:51: Train Epoch 649: 0/37 Loss: 259.182892\n",
      "2025-07-08 19:51: Train Epoch 649: 20/37 Loss: 143.671005\n",
      "2025-07-08 19:51: **********Train Epoch 649: averaged Loss: 280.177102\n",
      "2025-07-08 19:51: **********Val Epoch 649: average Loss: 1210.051432\n",
      "2025-07-08 19:51: Train Epoch 650: 0/37 Loss: 619.363098\n",
      "2025-07-08 19:51: Train Epoch 650: 20/37 Loss: 141.387970\n",
      "2025-07-08 19:51: **********Train Epoch 650: averaged Loss: 271.281621\n",
      "2025-07-08 19:51: **********Val Epoch 650: average Loss: 810.529307\n",
      "2025-07-08 19:51: Train Epoch 651: 0/37 Loss: 220.520294\n",
      "2025-07-08 19:51: Train Epoch 651: 20/37 Loss: 138.987747\n",
      "2025-07-08 19:51: **********Train Epoch 651: averaged Loss: 276.291205\n",
      "2025-07-08 19:51: **********Val Epoch 651: average Loss: 1188.676432\n",
      "2025-07-08 19:51: Train Epoch 652: 0/37 Loss: 565.882690\n",
      "2025-07-08 19:51: Train Epoch 652: 20/37 Loss: 134.986023\n",
      "2025-07-08 19:51: **********Train Epoch 652: averaged Loss: 271.646249\n",
      "2025-07-08 19:51: **********Val Epoch 652: average Loss: 819.150360\n",
      "2025-07-08 19:51: Train Epoch 653: 0/37 Loss: 229.294205\n",
      "2025-07-08 19:51: Train Epoch 653: 20/37 Loss: 139.361145\n",
      "2025-07-08 19:51: **********Train Epoch 653: averaged Loss: 281.102193\n",
      "2025-07-08 19:51: **********Val Epoch 653: average Loss: 1245.697774\n",
      "2025-07-08 19:51: Train Epoch 654: 0/37 Loss: 544.962708\n",
      "2025-07-08 19:51: Train Epoch 654: 20/37 Loss: 128.576172\n",
      "2025-07-08 19:51: **********Train Epoch 654: averaged Loss: 270.095016\n",
      "2025-07-08 19:51: **********Val Epoch 654: average Loss: 807.234121\n",
      "2025-07-08 19:51: Train Epoch 655: 0/37 Loss: 216.412140\n",
      "2025-07-08 19:51: Train Epoch 655: 20/37 Loss: 137.890457\n",
      "2025-07-08 19:51: **********Train Epoch 655: averaged Loss: 277.565650\n",
      "2025-07-08 19:51: **********Val Epoch 655: average Loss: 1196.049561\n",
      "2025-07-08 19:51: Train Epoch 656: 0/37 Loss: 551.676147\n",
      "2025-07-08 19:51: Train Epoch 656: 20/37 Loss: 137.312729\n",
      "2025-07-08 19:51: **********Train Epoch 656: averaged Loss: 266.803657\n",
      "2025-07-08 19:51: **********Val Epoch 656: average Loss: 805.733968\n",
      "2025-07-08 19:51: Train Epoch 657: 0/37 Loss: 214.296448\n",
      "2025-07-08 19:51: Train Epoch 657: 20/37 Loss: 139.699707\n",
      "2025-07-08 19:51: **********Train Epoch 657: averaged Loss: 281.224614\n",
      "2025-07-08 19:51: **********Val Epoch 657: average Loss: 1190.457031\n",
      "2025-07-08 19:51: Train Epoch 658: 0/37 Loss: 596.548035\n",
      "2025-07-08 19:51: Train Epoch 658: 20/37 Loss: 135.849579\n",
      "2025-07-08 19:51: **********Train Epoch 658: averaged Loss: 265.506063\n",
      "2025-07-08 19:51: **********Val Epoch 658: average Loss: 785.449636\n",
      "2025-07-08 19:51: Train Epoch 659: 0/37 Loss: 226.196167\n",
      "2025-07-08 19:51: Train Epoch 659: 20/37 Loss: 138.168732\n",
      "2025-07-08 19:51: **********Train Epoch 659: averaged Loss: 280.240924\n",
      "2025-07-08 19:51: **********Val Epoch 659: average Loss: 1183.271790\n",
      "2025-07-08 19:51: Train Epoch 660: 0/37 Loss: 570.232727\n",
      "2025-07-08 19:51: Train Epoch 660: 20/37 Loss: 142.332565\n",
      "2025-07-08 19:51: **********Train Epoch 660: averaged Loss: 267.489031\n",
      "2025-07-08 19:51: **********Val Epoch 660: average Loss: 767.085459\n",
      "2025-07-08 19:51: Train Epoch 661: 0/37 Loss: 216.834991\n",
      "2025-07-08 19:51: Train Epoch 661: 20/37 Loss: 140.373062\n",
      "2025-07-08 19:51: **********Train Epoch 661: averaged Loss: 268.463315\n",
      "2025-07-08 19:51: **********Val Epoch 661: average Loss: 1124.079549\n",
      "2025-07-08 19:51: Train Epoch 662: 0/37 Loss: 539.591980\n",
      "2025-07-08 19:51: Train Epoch 662: 20/37 Loss: 133.237122\n",
      "2025-07-08 19:51: **********Train Epoch 662: averaged Loss: 262.555088\n",
      "2025-07-08 19:51: **********Val Epoch 662: average Loss: 773.066091\n",
      "2025-07-08 19:51: Train Epoch 663: 0/37 Loss: 228.067764\n",
      "2025-07-08 19:51: Train Epoch 663: 20/37 Loss: 143.408127\n",
      "2025-07-08 19:51: **********Train Epoch 663: averaged Loss: 275.421100\n",
      "2025-07-08 19:51: **********Val Epoch 663: average Loss: 1187.354919\n",
      "2025-07-08 19:51: Train Epoch 664: 0/37 Loss: 563.016418\n",
      "2025-07-08 19:51: Train Epoch 664: 20/37 Loss: 130.383041\n",
      "2025-07-08 19:51: **********Train Epoch 664: averaged Loss: 271.693678\n",
      "2025-07-08 19:51: **********Val Epoch 664: average Loss: 747.589610\n",
      "2025-07-08 19:51: Train Epoch 665: 0/37 Loss: 253.381088\n",
      "2025-07-08 19:51: Train Epoch 665: 20/37 Loss: 144.497025\n",
      "2025-07-08 19:51: **********Train Epoch 665: averaged Loss: 286.226431\n",
      "2025-07-08 19:51: **********Val Epoch 665: average Loss: 1187.631409\n",
      "2025-07-08 19:51: Train Epoch 666: 0/37 Loss: 557.019287\n",
      "2025-07-08 19:51: Train Epoch 666: 20/37 Loss: 125.992233\n",
      "2025-07-08 19:52: **********Train Epoch 666: averaged Loss: 269.061253\n",
      "2025-07-08 19:52: **********Val Epoch 666: average Loss: 776.617859\n",
      "2025-07-08 19:52: Train Epoch 667: 0/37 Loss: 248.228836\n",
      "2025-07-08 19:52: Train Epoch 667: 20/37 Loss: 140.114120\n",
      "2025-07-08 19:52: **********Train Epoch 667: averaged Loss: 280.258189\n",
      "2025-07-08 19:52: **********Val Epoch 667: average Loss: 1144.259237\n",
      "2025-07-08 19:52: Train Epoch 668: 0/37 Loss: 571.838318\n",
      "2025-07-08 19:52: Train Epoch 668: 20/37 Loss: 135.182388\n",
      "2025-07-08 19:52: **********Train Epoch 668: averaged Loss: 265.417279\n",
      "2025-07-08 19:52: **********Val Epoch 668: average Loss: 725.402883\n",
      "2025-07-08 19:52: Train Epoch 669: 0/37 Loss: 235.450882\n",
      "2025-07-08 19:52: Train Epoch 669: 20/37 Loss: 143.137589\n",
      "2025-07-08 19:52: **********Train Epoch 669: averaged Loss: 276.794743\n",
      "2025-07-08 19:52: **********Val Epoch 669: average Loss: 1136.430339\n",
      "2025-07-08 19:52: Train Epoch 670: 0/37 Loss: 563.715576\n",
      "2025-07-08 19:52: Train Epoch 670: 20/37 Loss: 129.257919\n",
      "2025-07-08 19:52: **********Train Epoch 670: averaged Loss: 262.575129\n",
      "2025-07-08 19:52: **********Val Epoch 670: average Loss: 763.209208\n",
      "2025-07-08 19:52: Train Epoch 671: 0/37 Loss: 237.126816\n",
      "2025-07-08 19:52: Train Epoch 671: 20/37 Loss: 145.915451\n",
      "2025-07-08 19:52: **********Train Epoch 671: averaged Loss: 268.020305\n",
      "2025-07-08 19:52: **********Val Epoch 671: average Loss: 1139.211466\n",
      "2025-07-08 19:52: Train Epoch 672: 0/37 Loss: 588.071777\n",
      "2025-07-08 19:52: Train Epoch 672: 20/37 Loss: 142.088852\n",
      "2025-07-08 19:52: **********Train Epoch 672: averaged Loss: 265.932476\n",
      "2025-07-08 19:52: **********Val Epoch 672: average Loss: 790.308167\n",
      "2025-07-08 19:52: Train Epoch 673: 0/37 Loss: 231.225266\n",
      "2025-07-08 19:52: Train Epoch 673: 20/37 Loss: 142.098022\n",
      "2025-07-08 19:52: **********Train Epoch 673: averaged Loss: 272.485402\n",
      "2025-07-08 19:52: **********Val Epoch 673: average Loss: 1154.163798\n",
      "2025-07-08 19:52: Train Epoch 674: 0/37 Loss: 563.332581\n",
      "2025-07-08 19:52: Train Epoch 674: 20/37 Loss: 131.238403\n",
      "2025-07-08 19:52: **********Train Epoch 674: averaged Loss: 265.805780\n",
      "2025-07-08 19:52: **********Val Epoch 674: average Loss: 766.213674\n",
      "2025-07-08 19:52: Train Epoch 675: 0/37 Loss: 221.351013\n",
      "2025-07-08 19:52: Train Epoch 675: 20/37 Loss: 143.733170\n",
      "2025-07-08 19:52: **********Train Epoch 675: averaged Loss: 277.386443\n",
      "2025-07-08 19:52: **********Val Epoch 675: average Loss: 1159.590007\n",
      "2025-07-08 19:52: Train Epoch 676: 0/37 Loss: 606.534485\n",
      "2025-07-08 19:52: Train Epoch 676: 20/37 Loss: 144.870987\n",
      "2025-07-08 19:52: **********Train Epoch 676: averaged Loss: 268.802636\n",
      "2025-07-08 19:52: **********Val Epoch 676: average Loss: 744.964834\n",
      "2025-07-08 19:52: Train Epoch 677: 0/37 Loss: 238.593430\n",
      "2025-07-08 19:52: Train Epoch 677: 20/37 Loss: 139.321655\n",
      "2025-07-08 19:52: **********Train Epoch 677: averaged Loss: 276.577990\n",
      "2025-07-08 19:52: **********Val Epoch 677: average Loss: 1144.169149\n",
      "2025-07-08 19:52: Train Epoch 678: 0/37 Loss: 578.617432\n",
      "2025-07-08 19:52: Train Epoch 678: 20/37 Loss: 139.248856\n",
      "2025-07-08 19:52: **********Train Epoch 678: averaged Loss: 263.583103\n",
      "2025-07-08 19:52: **********Val Epoch 678: average Loss: 733.096710\n",
      "2025-07-08 19:52: Train Epoch 679: 0/37 Loss: 225.730606\n",
      "2025-07-08 19:52: Train Epoch 679: 20/37 Loss: 140.731949\n",
      "2025-07-08 19:52: **********Train Epoch 679: averaged Loss: 271.442119\n",
      "2025-07-08 19:52: **********Val Epoch 679: average Loss: 1163.984212\n",
      "2025-07-08 19:52: Train Epoch 680: 0/37 Loss: 594.208618\n",
      "2025-07-08 19:52: Train Epoch 680: 20/37 Loss: 138.459625\n",
      "2025-07-08 19:52: **********Train Epoch 680: averaged Loss: 267.246516\n",
      "2025-07-08 19:52: **********Val Epoch 680: average Loss: 740.328410\n",
      "2025-07-08 19:52: Train Epoch 681: 0/37 Loss: 228.750565\n",
      "2025-07-08 19:52: Train Epoch 681: 20/37 Loss: 137.340958\n",
      "2025-07-08 19:52: **********Train Epoch 681: averaged Loss: 274.891400\n",
      "2025-07-08 19:52: **********Val Epoch 681: average Loss: 1123.042236\n",
      "2025-07-08 19:52: Train Epoch 682: 0/37 Loss: 588.139465\n",
      "2025-07-08 19:52: Train Epoch 682: 20/37 Loss: 136.284637\n",
      "2025-07-08 19:52: **********Train Epoch 682: averaged Loss: 271.945186\n",
      "2025-07-08 19:52: **********Val Epoch 682: average Loss: 775.135508\n",
      "2025-07-08 19:52: Train Epoch 683: 0/37 Loss: 260.951965\n",
      "2025-07-08 19:52: Train Epoch 683: 20/37 Loss: 140.261871\n",
      "2025-07-08 19:52: **********Train Epoch 683: averaged Loss: 275.577040\n",
      "2025-07-08 19:52: **********Val Epoch 683: average Loss: 1142.971517\n",
      "2025-07-08 19:52: Train Epoch 684: 0/37 Loss: 555.760315\n",
      "2025-07-08 19:52: Train Epoch 684: 20/37 Loss: 131.847061\n",
      "2025-07-08 19:52: **********Train Epoch 684: averaged Loss: 269.721971\n",
      "2025-07-08 19:52: **********Val Epoch 684: average Loss: 770.463491\n",
      "2025-07-08 19:52: Train Epoch 685: 0/37 Loss: 221.473328\n",
      "2025-07-08 19:52: Train Epoch 685: 20/37 Loss: 138.488678\n",
      "2025-07-08 19:52: **********Train Epoch 685: averaged Loss: 271.264685\n",
      "2025-07-08 19:52: **********Val Epoch 685: average Loss: 1147.169128\n",
      "2025-07-08 19:52: Train Epoch 686: 0/37 Loss: 577.497925\n",
      "2025-07-08 19:52: Train Epoch 686: 20/37 Loss: 131.223282\n",
      "2025-07-08 19:52: **********Train Epoch 686: averaged Loss: 266.920790\n",
      "2025-07-08 19:52: **********Val Epoch 686: average Loss: 781.416860\n",
      "2025-07-08 19:52: Train Epoch 687: 0/37 Loss: 264.952209\n",
      "2025-07-08 19:52: Train Epoch 687: 20/37 Loss: 137.136169\n",
      "2025-07-08 19:52: **********Train Epoch 687: averaged Loss: 286.292194\n",
      "2025-07-08 19:52: **********Val Epoch 687: average Loss: 1169.398641\n",
      "2025-07-08 19:52: Train Epoch 688: 0/37 Loss: 596.285461\n",
      "2025-07-08 19:52: Train Epoch 688: 20/37 Loss: 135.090637\n",
      "2025-07-08 19:52: **********Train Epoch 688: averaged Loss: 269.508941\n",
      "2025-07-08 19:52: **********Val Epoch 688: average Loss: 758.947133\n",
      "2025-07-08 19:52: Train Epoch 689: 0/37 Loss: 255.528793\n",
      "2025-07-08 19:52: Train Epoch 689: 20/37 Loss: 138.575760\n",
      "2025-07-08 19:52: **********Train Epoch 689: averaged Loss: 280.444980\n",
      "2025-07-08 19:52: **********Val Epoch 689: average Loss: 1168.282389\n",
      "2025-07-08 19:52: Train Epoch 690: 0/37 Loss: 581.240845\n",
      "2025-07-08 19:52: Train Epoch 690: 20/37 Loss: 142.432465\n",
      "2025-07-08 19:52: **********Train Epoch 690: averaged Loss: 266.562039\n",
      "2025-07-08 19:52: **********Val Epoch 690: average Loss: 744.989105\n",
      "2025-07-08 19:52: Train Epoch 691: 0/37 Loss: 241.101181\n",
      "2025-07-08 19:52: Train Epoch 691: 20/37 Loss: 138.526733\n",
      "2025-07-08 19:52: **********Train Epoch 691: averaged Loss: 275.072639\n",
      "2025-07-08 19:52: **********Val Epoch 691: average Loss: 1127.060059\n",
      "2025-07-08 19:52: Train Epoch 692: 0/37 Loss: 579.640747\n",
      "2025-07-08 19:52: Train Epoch 692: 20/37 Loss: 129.592804\n",
      "2025-07-08 19:52: **********Train Epoch 692: averaged Loss: 269.621431\n",
      "2025-07-08 19:52: **********Val Epoch 692: average Loss: 755.520996\n",
      "2025-07-08 19:52: Train Epoch 693: 0/37 Loss: 260.226166\n",
      "2025-07-08 19:52: Train Epoch 693: 20/37 Loss: 137.629547\n",
      "2025-07-08 19:52: **********Train Epoch 693: averaged Loss: 276.047635\n",
      "2025-07-08 19:52: **********Val Epoch 693: average Loss: 1090.312907\n",
      "2025-07-08 19:52: Train Epoch 694: 0/37 Loss: 585.663330\n",
      "2025-07-08 19:52: Train Epoch 694: 20/37 Loss: 132.033981\n",
      "2025-07-08 19:52: **********Train Epoch 694: averaged Loss: 275.825201\n",
      "2025-07-08 19:52: **********Val Epoch 694: average Loss: 734.875092\n",
      "2025-07-08 19:52: Train Epoch 695: 0/37 Loss: 220.857315\n",
      "2025-07-08 19:52: Train Epoch 695: 20/37 Loss: 137.105042\n",
      "2025-07-08 19:52: **********Train Epoch 695: averaged Loss: 280.562202\n",
      "2025-07-08 19:52: **********Val Epoch 695: average Loss: 1134.623088\n",
      "2025-07-08 19:52: Train Epoch 696: 0/37 Loss: 576.806580\n",
      "2025-07-08 19:52: Train Epoch 696: 20/37 Loss: 136.892517\n",
      "2025-07-08 19:52: **********Train Epoch 696: averaged Loss: 271.110066\n",
      "2025-07-08 19:52: **********Val Epoch 696: average Loss: 739.581624\n",
      "2025-07-08 19:52: Train Epoch 697: 0/37 Loss: 233.356430\n",
      "2025-07-08 19:52: Train Epoch 697: 20/37 Loss: 141.471085\n",
      "2025-07-08 19:52: **********Train Epoch 697: averaged Loss: 272.496600\n",
      "2025-07-08 19:52: **********Val Epoch 697: average Loss: 1146.163961\n",
      "2025-07-08 19:52: Train Epoch 698: 0/37 Loss: 581.632263\n",
      "2025-07-08 19:52: Train Epoch 698: 20/37 Loss: 125.983475\n",
      "2025-07-08 19:52: **********Train Epoch 698: averaged Loss: 268.664652\n",
      "2025-07-08 19:52: **********Val Epoch 698: average Loss: 704.211731\n",
      "2025-07-08 19:52: Train Epoch 699: 0/37 Loss: 237.930328\n",
      "2025-07-08 19:52: Train Epoch 699: 20/37 Loss: 139.145111\n",
      "2025-07-08 19:52: **********Train Epoch 699: averaged Loss: 270.217998\n",
      "2025-07-08 19:52: **********Val Epoch 699: average Loss: 1090.656453\n",
      "2025-07-08 19:52: Train Epoch 700: 0/37 Loss: 544.432373\n",
      "2025-07-08 19:52: Train Epoch 700: 20/37 Loss: 134.717834\n",
      "2025-07-08 19:52: **********Train Epoch 700: averaged Loss: 261.835197\n",
      "2025-07-08 19:52: **********Val Epoch 700: average Loss: 735.829142\n",
      "2025-07-08 19:52: Train Epoch 701: 0/37 Loss: 207.012497\n",
      "2025-07-08 19:52: Train Epoch 701: 20/37 Loss: 139.513199\n",
      "2025-07-08 19:52: **********Train Epoch 701: averaged Loss: 266.700973\n",
      "2025-07-08 19:52: **********Val Epoch 701: average Loss: 1089.484456\n",
      "2025-07-08 19:52: Train Epoch 702: 0/37 Loss: 531.144043\n",
      "2025-07-08 19:52: Train Epoch 702: 20/37 Loss: 129.956619\n",
      "2025-07-08 19:52: **********Train Epoch 702: averaged Loss: 258.806551\n",
      "2025-07-08 19:52: **********Val Epoch 702: average Loss: 708.606049\n",
      "2025-07-08 19:52: Train Epoch 703: 0/37 Loss: 196.002716\n",
      "2025-07-08 19:53: Train Epoch 703: 20/37 Loss: 135.670715\n",
      "2025-07-08 19:53: **********Train Epoch 703: averaged Loss: 271.960523\n",
      "2025-07-08 19:53: **********Val Epoch 703: average Loss: 1116.763428\n",
      "2025-07-08 19:53: Train Epoch 704: 0/37 Loss: 568.438110\n",
      "2025-07-08 19:53: Train Epoch 704: 20/37 Loss: 134.733658\n",
      "2025-07-08 19:53: **********Train Epoch 704: averaged Loss: 269.031160\n",
      "2025-07-08 19:53: **********Val Epoch 704: average Loss: 711.272227\n",
      "2025-07-08 19:53: Train Epoch 705: 0/37 Loss: 259.463806\n",
      "2025-07-08 19:53: Train Epoch 705: 20/37 Loss: 142.188110\n",
      "2025-07-08 19:53: **********Train Epoch 705: averaged Loss: 284.535739\n",
      "2025-07-08 19:53: **********Val Epoch 705: average Loss: 1136.077128\n",
      "2025-07-08 19:53: Train Epoch 706: 0/37 Loss: 618.969543\n",
      "2025-07-08 19:53: Train Epoch 706: 20/37 Loss: 133.063904\n",
      "2025-07-08 19:53: **********Train Epoch 706: averaged Loss: 275.476858\n",
      "2025-07-08 19:53: **********Val Epoch 706: average Loss: 709.820638\n",
      "2025-07-08 19:53: Train Epoch 707: 0/37 Loss: 241.816940\n",
      "2025-07-08 19:53: Train Epoch 707: 20/37 Loss: 138.792465\n",
      "2025-07-08 19:53: **********Train Epoch 707: averaged Loss: 280.708436\n",
      "2025-07-08 19:53: **********Val Epoch 707: average Loss: 1074.482300\n",
      "2025-07-08 19:53: Validation performance didn't improve for 200 epochs. Training stops.\n",
      "2025-07-08 19:53: Total training time: 20.1555min, best loss: 311.828644\n",
      "2025-07-08 19:53: Average Horizon, MAE: 0.0102, MSE: 0.0128\n",
      "2025-07-08 19:53: Average Horizon, MAE: 0.0101, MSE: 0.0127\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'masked_mae_loss.<locals>.loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 121\u001b[0m\n\u001b[1;32m    114\u001b[0m RIC\u001b[38;5;241m=\u001b[39mrank_information_coefficient(return_t[:,\u001b[38;5;241m0\u001b[39m],return_p[:,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    117\u001b[0m result_train_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAGCRN_Model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult_train_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamba_IXIC_staticG.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    124\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(IC)))\n",
      "Cell \u001b[0;32mIn[3], line 192\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, model_dir, epoch)\u001b[0m\n\u001b[1;32m    190\u001b[0m file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_stemgnn.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/samba/lib/python3.10/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/samba/lib/python3.10/site-packages/torch/serialization.py:1088\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1086\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m   1087\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m-> 1088\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m   1090\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'masked_mae_loss.<locals>.loss'"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "\n",
    "    Mode = 'train'\n",
    "    DEBUG = 'True'\n",
    "    DATASET = 'PEMSD8'      #PEMSD4 or PEMSD8\n",
    "    DEVICE = 'cuda:0'\n",
    "    MODEL = 'AGCRN'\n",
    "\n",
    "#get configuration\n",
    "    config_file = './{}_{}.conf'.format(DATASET, MODEL)\n",
    "#print('Read configuration file: %s' % (config_file))\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "#parser\n",
    "##### 原程式碼 ##########\n",
    "    # args={\"dataset\":DATASET,\"mode\":Mode,\"device\":DEVICE,\"debug\":DEBUG,\"model\":MODEL,\"cuda\":True,\"val_ratio\":0.15,\"test_ratio\":0.15,\n",
    "    #   \"lag\":window,\"horizon\":predict,\"num_nodes\":XX.shape[2],\"tod\":False,\"normalizer\":'std',\"column_wise\":False,\"default_graph\":True,\n",
    "    #  \"input_dim\":1,\"output_dim\":1,\"embed_dim\":10,\"rnn_units\":128,\"num_layers\":3,\"cheb_k\":3,\"loss_func\":'mae',\"seed\":1,\n",
    "    #  \"batch_size\":32,\"epochs\":1100,\"lr_init\":0.001,\"lr_decay\":True,\"lr_decay_rate\":0.5,\"lr_decay_step\":[40,70,100],\n",
    "    #   \"early_stop\":True,\"early_stop_patience\":200,\"grad_norm\":False,\"max_grad_norm\":5,\"real_value\":False,\"mae_thresh\":None,\n",
    "    #   \"mape_thresh\":0,\"log_dir\":'./',\"log_step\":20,\"plot\":False,\"teacher_forcing\":False,\"d_in\":32,\"hid\":32}\n",
    "    ####################\n",
    "    ##### loss function 用 \"loss_func\":'mask_mae' => 絕對值(價格-預測價格)\n",
    "    args={\"dataset\":DATASET,\"mode\":Mode,\"device\":DEVICE,\"debug\":DEBUG,\"model\":MODEL,\"cuda\":True,\"val_ratio\":0.15,\"test_ratio\":0.15,\n",
    "      \"lag\":window,\"horizon\":predict,\"num_nodes\":XX.shape[2],\"tod\":False,\"normalizer\":'std',\"column_wise\":False,\"default_graph\":True,\n",
    "     \"input_dim\":1,\"output_dim\":1,\"embed_dim\":10,\"rnn_units\":128,\"num_layers\":3,\"cheb_k\":3,\"loss_func\":'mask_mae',\"seed\":1,\n",
    "     \"batch_size\":32,\"epochs\":1100,\"lr_init\":0.001,\"lr_decay\":True,\"lr_decay_rate\":0.5,\"lr_decay_step\":[40,70,100],\n",
    "      \"early_stop\":True,\"early_stop_patience\":200,\"grad_norm\":False,\"max_grad_norm\":5,\"real_value\":False,\"mae_thresh\":None,\n",
    "      \"mape_thresh\":0,\"log_dir\":'./',\"log_step\":20,\"plot\":False,\"teacher_forcing\":False,\"d_in\":32,\"hid\":32}\n",
    "    ####################\n",
    "\n",
    "\n",
    "\n",
    "#init model\n",
    "    ##### 原程式碼 ##########\n",
    "    # model = SAMBA(ModelArgs(args.get(\"d_in\"),args.get(\"num_layers\"),args.get(\"num_nodes\"),args.get('lag'),args.get('horizon')),args.get('hid'),args.get('lag'),args.get('horizon'),args.get('embed_dim'),args.get(\"cheb_k\"))\n",
    "    # model = model.cuda()\n",
    "    # for p in model.parameters():\n",
    "    #     if p.dim() > 1:\n",
    "    #         nn.init.xavier_uniform_(p)\n",
    "    #     else:\n",
    "    #         nn.init.uniform_(p)\n",
    "    # print_model_parameters(model, only_num=False)\n",
    "\n",
    "    # if args.get('loss_func') == 'mask_mae':\n",
    "    #     loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "    # elif args.get('loss_func') == 'mae':\n",
    "    #     loss = torch.nn.L1Loss().to(args.get('device'))\n",
    "    # elif args.get('loss_func') == 'mse':\n",
    "    #     loss = torch.nn.MSELoss().to(args.get('device'))\n",
    "    # else:\n",
    "    #     raise ValueError\n",
    "    ######################################################################################################\n",
    "    ###### loss = masked_mae_loss(scaler, mask_value=0.0) => masked_mae_loss(mmn, mask_value=0.0)\n",
    "    ######################################################################################################\n",
    "    model = SAMBA(ModelArgs(args.get(\"d_in\"),args.get(\"num_layers\"),args.get(\"num_nodes\"),args.get('lag'),args.get('horizon')),args.get('hid'),args.get('lag'),args.get('horizon'),args.get('embed_dim'),args.get(\"cheb_k\"))\n",
    "    model = model.cuda()\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        else:\n",
    "            nn.init.uniform_(p)\n",
    "    print_model_parameters(model, only_num=False)\n",
    "\n",
    "    if args.get('loss_func') == 'mask_mae':\n",
    "        loss = masked_mae_loss(mmn, mask_value=0.0)\n",
    "    elif args.get('loss_func') == 'mae':\n",
    "        loss = torch.nn.L1Loss().to(args.get('device'))\n",
    "    elif args.get('loss_func') == 'mse':\n",
    "        loss = torch.nn.MSELoss().to(args.get('device'))\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=args.get('lr_init'), eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "#learning rate decay\n",
    "    lr_scheduler = None\n",
    "    if args.get('lr_decay'):\n",
    "        print('Applying learning rate decay.')\n",
    "        lr_decay_steps = [int(i) for i in args.get('lr_decay_step')]\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "        milestones=[0.5 * args.get('epochs'),0.7 * args.get('epochs'), 0.9 * args.get('epochs')],gamma=0.1)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "\n",
    "\n",
    "#start training\n",
    "    trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, args=args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    y1,y2=trainer.test(trainer.model, trainer.args, test_loader, trainer.logger)\n",
    "    \n",
    "    y_p=np.array(y1[:,0,:].cpu())\n",
    "\n",
    "    y_t=np.array(y2[:,0,:].cpu())\n",
    "\n",
    "    y_p = mmn.inverse_transform(y_p)\n",
    "\n",
    "    y_t = mmn.inverse_transform(y_t)\n",
    "    \n",
    "    ##### 畫圖\n",
    "    #-------------------------------------------------------------------------------\n",
    "    y_p_np = y_p.detach().cpu().numpy() if isinstance(y_p, torch.Tensor) else y_p\n",
    "    y_t_np = y_t.detach().cpu().numpy() if isinstance(y_t, torch.Tensor) else y_t\n",
    "\n",
    "    # 選擇一個節點來畫（如果 y_p 維度是 [時間, 節點]）\n",
    "    node_index = 0  # or any index within y_p.shape[1]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_t_np[:, node_index], label='Ground Truth', color='black', linestyle='--')\n",
    "    plt.plot(y_p_np[:, node_index], label='Prediction', color='blue')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Prediction vs. Ground Truth (Node {node_index})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #--------------------------------------------------------------------------------\n",
    "#y_p=(y_p-mean)/std\n",
    "#y_t=(y_t-mean)/std\n",
    "\n",
    "    y_p=torch.tensor(y_p)\n",
    "    y_t=torch.tensor(y_t)\n",
    "\n",
    "    diff = y_p[1:] - y_p[:-1]\n",
    "    return_p = diff / y_p[:-1]\n",
    "\n",
    "    diff = y_t[1:] - y_t[:-1]\n",
    "    return_t = diff / y_t[:-1]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    mae, rmse, _=All_Metrics(return_p,return_t, None,None )\n",
    "\n",
    "    IC=pearson_correlation(return_t,return_p)\n",
    "    \n",
    "    RIC=rank_information_coefficient(return_t[:,0],return_p[:,0])\n",
    "\n",
    "\n",
    "    result_train_file = os.path.join(\"AGCRN_Model\", \"milan\",\"call\")\n",
    "\n",
    "\n",
    "\n",
    "    save_model(trainer,result_train_file,i+1)\n",
    "\n",
    "    with open('samba_IXIC_staticG.txt', 'a') as f:\n",
    "        f.write(str(np.array(IC)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(RIC)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(mae)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(rmse)))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e88ac",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a106fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 選擇 device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. 把訓練完存的 Trainer 物件 load 回來\n",
    "trainer: Trainer = torch.load(\n",
    "    \"AGCRN_Model/milan/call/1_stemgnn.pt\",\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "# 3. 讓 model 跑到正確的 device，並設為 eval\n",
    "trainer.model.to(device)\n",
    "trainer.model.eval()\n",
    "\n",
    "# 4. 重建 scaler\n",
    "#    由於你沒有把 min/max 放到 trainer 裡，這裡我們重新從原始 CSV 再 fit 一次\n",
    "df = pd.read_csv('combined_dataframe_DJI.csv', index_col=\"Date\", parse_dates=True)\n",
    "name = df[\"Name\"].iloc[0]\n",
    "del df[\"Name\"]\n",
    "df[\"Target\"] = (df[\"Price\"].pct_change().shift(-1) > 0).astype(int)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "raw = df.to_numpy()        # 這就是訓練時 a = X.to_numpy()\n",
    "mmn = MinMaxNorm01()\n",
    "mmn.fit(raw)               # 重新算出 min, max\n",
    "\n",
    "# 5. 做推論\n",
    "y_pred_t, y_true_t = Trainer.test(\n",
    "    model       = trainer.model,\n",
    "    args        = trainer.args,\n",
    "    data_loader = test_loader,\n",
    "    logger      = trainer.logger,\n",
    "    path        = None       # 已經直接用 trainer.model\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
